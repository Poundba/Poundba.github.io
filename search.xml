<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Java 数组转为 List 的三种方式及对比</title>
    <url>/10b6773/</url>
    <content><![CDATA[<h3 id="常见方式"><a href="#常见方式" class="headerlink" title="常见方式"></a>常见方式</h3><p>通过 <code>Arrays.asList(strArray)</code> 方式将数组转换为 List 后，只能查改，不能增删，否则抛异常。</p>
<ul>
<li><p>关键代码：<code>List list = Arrays.asList(strArray);</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">List</span> <span class="variable">list</span> <span class="operator">=</span> Arrays.asList(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>);</span><br><span class="line">    <span class="comment">//对转换后的list插入一条数据</span></span><br><span class="line">	list.add(<span class="number">1</span>);</span><br><span class="line">	System.out.println(list);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行结果</p>
<p>这个错误是由于UnsupportedOperationException异常引起的。在给定的代码中，您试图在一个不可修改的列表上执行添加操作。Arrays.asList()方法返回一个固定大小的列表，这意味着您不能向列表中添加或删除元素。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">java.lang.UnsupportedOperationException</span><br><span class="line">	at java.base/java.util.AbstractList.add(AbstractList.java:155)</span><br><span class="line">	at java.base/java.util.AbstractList.add(AbstractList.java:113)</span><br><span class="line">	at com.example.SpringTeachApplicationTests.contextLoads(SpringTeachApplicationTests.java:15)</span><br><span class="line">	at java.base/java.lang.reflect.Method.invoke(Method.java:580)</span><br><span class="line">	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)</span><br><span class="line">	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="数组转为-List-后，支持增删改查的方式"><a href="#数组转为-List-后，支持增删改查的方式" class="headerlink" title="数组转为 List 后，支持增删改查的方式"></a>数组转为 List 后，支持增删改查的方式</h3><p>通过 ArrayList 的构造器，将 <code>Arrays.asList(strArray)</code>  的返回值由 <code>java.util.Arrays.ArrayList</code> 转为 <code>java.util.ArrayList</code>。</p>
<ul>
<li><p>关键代码：<code>ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(strArray));</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">    String[] strArray = <span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">2</span>];</span><br><span class="line">    ArrayList&lt;String&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;(Arrays.asList(strArray));</span><br><span class="line">    list.add(<span class="string">&quot;1&quot;</span>);</span><br><span class="line">    System.out.println(list);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行结果</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[null, null, 1]</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="通过集合工具类-Collections-addAll-方法（最高效）"><a href="#通过集合工具类-Collections-addAll-方法（最高效）" class="headerlink" title="通过集合工具类 Collections.addAll() 方法（最高效）"></a>通过集合工具类 Collections.addAll() 方法（最高效）</h3><p>通过 <code>Collections.addAll(arrayList, strArray)</code> 方式转换，根据数组的长度创建一个长度相同的 List，然后通过 <code>Collections.addAll()</code> 方法，将数组中的元素转为二进制，然后添加到 List 中，这是最高效的方法。</p>
<ul>
<li><p>关键代码</p>
<ol>
<li><code>ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(strArray.length);</code></li>
<li><code>Collections.addAll(list, strArray);</code></li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">    String[] strArray = <span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">2</span>];</span><br><span class="line">    ArrayList&lt;String&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;(strArray.length);</span><br><span class="line">    Collections.addAll(list, strArray);</span><br><span class="line">    list.add(<span class="string">&quot;1&quot;</span>);</span><br><span class="line">    System.out.println(list);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行结果</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[null, null, 1]</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="通过-Steam-流将-3-种基本类型数组转换为-List"><a href="#通过-Steam-流将-3-种基本类型数组转换为-List" class="headerlink" title="通过 Steam 流将 3 种基本类型数组转换为 List"></a>通过 Steam 流将 3 种基本类型数组转换为 List</h3><p>如果 JDK 版本 &gt;&#x3D; 1.8，可以使用 Steam流 将 int[]、long[]、double[] 转为 List。其他数据类型，比如 short[]、byte[]、char[]，暂不支持。</p>
<ul>
<li><p>转换代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Integer&gt; integerList = Arrays.stream(<span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;).boxed().collect(Collectors.toList());</span><br><span class="line">List&lt;Long&gt; longList = Arrays.stream(<span class="keyword">new</span> <span class="title class_">long</span>[]&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;).boxed().collect(Collectors.toList());</span><br><span class="line">List&lt;Double&gt; doubleList = Arrays.stream(<span class="keyword">new</span> <span class="title class_">double</span>[]&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;).boxed().collect(Collectors.toList());</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于 String 数组</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String[] arrays = &#123;<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>&#125;;</span><br><span class="line">List&lt;String&gt; stringList = Stream.of(arrays).collect(Collectors.toList());</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Java</tag>
        <tag>List</tag>
      </tags>
  </entry>
  <entry>
    <title>十大排序算法 ing</title>
    <url>/b7e144d1/</url>
    <content><![CDATA[<h2 id="十大排序算法"><a href="#十大排序算法" class="headerlink" title="十大排序算法"></a>十大排序算法</h2><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> <span class="title function_">bubble</span><span class="params">()</span> &#123;</span><br><span class="line">	<span class="type">int</span>[] arr = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">12</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">5</span>&#125;;</span><br><span class="line">	<span class="comment">// 外层循环代表比较了几圈：n-1 圈</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; arr.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; arr.length - <span class="number">1</span> - i; j++) &#123;</span><br><span class="line">			<span class="keyword">if</span> (arr[j] &gt; arr[j + <span class="number">1</span>]) &#123;</span><br><span class="line">				<span class="type">int</span> <span class="variable">temp</span> <span class="operator">=</span> arr[j];</span><br><span class="line">				arr[j] = arr[j + <span class="number">1</span>];</span><br><span class="line">				arr[j + <span class="number">1</span>] = temp;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  简单选择排序（升序）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ChoiceSort</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span>[] sortArray(<span class="type">int</span>[] numx) &#123;</span><br><span class="line">		<span class="keyword">if</span>(nums.length == <span class="number">0</span>) &#123;</span><br><span class="line">			<span class="keyword">return</span> nums;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i&lt; nums.length; i++) &#123;</span><br><span class="line">			<span class="type">int</span> <span class="variable">minIndex</span> <span class="operator">=</span> i;<span class="comment">/*最小数的下标，每个循环开始总是假设第一个数最小*/</span></span><br><span class="line">			<span class="keyword">for</span>(<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> i; j&lt; nums.length;j++) &#123;</span><br><span class="line">				<span class="keyword">if</span>(numb[j] &lt; nums[minIndex]) &#123;<span class="comment">/*找到最小的数*/</span></span><br><span class="line">					minIndex = j;<span class="comment">/*将最小数的索引保存*/</span></span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			System.out.println(<span class="string">&quot;最小数为：&quot;</span> + nums[minIndex]);</span><br><span class="line">			<span class="comment">/*交换最小数和i当前所指的元素*/</span></span><br><span class="line">			<span class="type">int</span> <span class="variable">temp</span> <span class="operator">=</span> nums[minIndex];</span><br><span class="line">			nums[minIndex] = nums[i];</span><br><span class="line">			PrintArray.print(nums);</span><br><span class="line">			System.out.println(<span class="string">&quot;-----&quot;</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> nums;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">    	PrintArray.print(PrintArray.SRC);</span><br><span class="line">    	System.out.println(<span class="string">&quot;==================================================&quot;</span>)</span><br><span class="line">    	<span class="type">int</span>[] dest=<span class="keyword">new</span> <span class="title class_">ChoiceSort</span>().sortArray(PrintArray.SRC);</span><br><span class="line">    	PrintArray.print(desc);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>主要国家和地区货币代码表</title>
    <url>/84c932d1/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th>国家名称</th>
<th>货币名称</th>
<th>货币代码</th>
</tr>
</thead>
<tbody><tr>
<td>中国</td>
<td>人民币元</td>
<td>156</td>
</tr>
<tr>
<td>中国香港</td>
<td>港元</td>
<td>344</td>
</tr>
<tr>
<td>美国</td>
<td>美元</td>
<td>840</td>
</tr>
<tr>
<td>英国</td>
<td>英镑</td>
<td>826</td>
</tr>
<tr>
<td>日本</td>
<td>日元</td>
<td>392</td>
</tr>
<tr>
<td>德国</td>
<td>德国马克</td>
<td>280</td>
</tr>
<tr>
<td>法国</td>
<td>法国法郎</td>
<td>250</td>
</tr>
<tr>
<td>澳大利亚</td>
<td>澳元</td>
<td>036</td>
</tr>
<tr>
<td>巴西</td>
<td>克鲁塞罗</td>
<td>076</td>
</tr>
<tr>
<td>俄罗斯</td>
<td>卢布</td>
<td>810</td>
</tr>
<tr>
<td>菲律宾</td>
<td>菲比索</td>
<td>608</td>
</tr>
<tr>
<td>芬兰</td>
<td>马克</td>
<td>246</td>
</tr>
<tr>
<td>荷兰</td>
<td>荷兰盾</td>
<td>528</td>
</tr>
<tr>
<td>加拿大</td>
<td>加元</td>
<td>124</td>
</tr>
<tr>
<td>科威尔科威特科威特</td>
<td>第纳尔</td>
<td>414</td>
</tr>
<tr>
<td>西班牙</td>
<td>西班牙比塞塔</td>
<td>724</td>
</tr>
<tr>
<td>葡萄牙</td>
<td>葡萄牙埃斯库多</td>
<td>620</td>
</tr>
<tr>
<td>南朝鲜</td>
<td>圆</td>
<td>410</td>
</tr>
<tr>
<td>南非</td>
<td>兰特</td>
<td>710</td>
</tr>
<tr>
<td>挪威</td>
<td>挪威克朗</td>
<td>578</td>
</tr>
<tr>
<td>瑞典</td>
<td>瑞典克朗</td>
<td>752</td>
</tr>
<tr>
<td>瑞士</td>
<td>瑞士法郎</td>
<td>756</td>
</tr>
<tr>
<td>泰国</td>
<td>铢</td>
<td>764</td>
</tr>
<tr>
<td>新加坡</td>
<td>新加坡元</td>
<td>702</td>
</tr>
<tr>
<td>新西兰</td>
<td>新西兰元</td>
<td>554</td>
</tr>
<tr>
<td>比利时</td>
<td>比利时法郎</td>
<td>056</td>
</tr>
<tr>
<td>意大利</td>
<td>意里拉</td>
<td>380</td>
</tr>
<tr>
<td>印度</td>
<td>印度卢比</td>
<td>356</td>
</tr>
<tr>
<td>印尼</td>
<td>卢比</td>
<td>360</td>
</tr>
<tr>
<td>希腊</td>
<td>德拉克马</td>
<td>300</td>
</tr>
<tr>
<td>中国澳门</td>
<td>澳门元</td>
<td>446</td>
</tr>
<tr>
<td>中国台湾</td>
<td>新台湾元</td>
<td>901</td>
</tr>
<tr>
<td>奥地利</td>
<td>先令</td>
<td>040</td>
</tr>
<tr>
<td>丹麦</td>
<td>丹麦克朗</td>
<td>208</td>
</tr>
<tr>
<td>埃及</td>
<td>埃及镑</td>
<td>818</td>
</tr>
<tr>
<td>伊拉克</td>
<td>伊拉克第纳尔</td>
<td>368</td>
</tr>
<tr>
<td>伊朗</td>
<td>伊朗里亚尔</td>
<td>364</td>
</tr>
<tr>
<td>沙特阿拉伯</td>
<td>沙特里亚尔</td>
<td>682</td>
</tr>
<tr>
<td>阿联酋</td>
<td>UAE迪拉姆</td>
<td>784</td>
</tr>
<tr>
<td>马来西亚</td>
<td>马来西亚林吉特</td>
<td>458</td>
</tr>
</tbody></table>
]]></content>
      <tags>
        <tag>货币</tag>
      </tags>
  </entry>
  <entry>
    <title>Java-Stream</title>
    <url>/1650c766/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>ElasticSearch</title>
    <url>/d48132d3/</url>
    <content><![CDATA[<h2 id="ElasticSearch-速成"><a href="#ElasticSearch-速成" class="headerlink" title="ElasticSearch 速成"></a>ElasticSearch 速成</h2><h3 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h3><h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4>]]></content>
  </entry>
  <entry>
    <title>Ubuntu 修改默认启动方式</title>
    <url>/756a7bd6/</url>
    <content><![CDATA[<h3 id="查看系统默认级别"><a href="#查看系统默认级别" class="headerlink" title="查看系统默认级别"></a>查看系统默认级别</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl get-default</span><br><span class="line">graphical.target</span><br></pre></td></tr></table></figure>

<p><img src="/756a7bd6/image-20230919011439511.png" alt="image-20230919011439511"></p>
<h3 id="修改启动方式为命令行"><a href="#修改启动方式为命令行" class="headerlink" title="修改启动方式为命令行"></a>修改启动方式为命令行</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo systemctl set-default multi-user.target</span><br></pre></td></tr></table></figure>

<p><img src="/756a7bd6/image-20230919011556238.png" alt="image-20230919011556238"></p>
<h3 id="修改启动方式为图形界面"><a href="#修改启动方式为图形界面" class="headerlink" title="修改启动方式为图形界面"></a>修改启动方式为图形界面</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl set-default graphical.target</span><br></pre></td></tr></table></figure>

<p><img src="/756a7bd6/image-20230919011708085.png" alt="image-20230919011708085"></p>
]]></content>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>如何用两个线程轮流输出 0 到 200 的值？</title>
    <url>/181384cc/</url>
    <content><![CDATA[<h2 id="如何用两个线程轮流输出-0-到-200-的值？"><a href="#如何用两个线程轮流输出-0-到-200-的值？" class="headerlink" title="如何用两个线程轮流输出 0 到 200 的值？"></a>如何用两个线程轮流输出 0 到 200 的值？</h2><h3 id="volatile-实现"><a href="#volatile-实现" class="headerlink" title="volatile 实现"></a>volatile 实现</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MainApplicationTests</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="type">AtomicInteger</span> <span class="variable">integer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">AtomicInteger</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">volatile</span> <span class="type">boolean</span> <span class="variable">flag</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">while</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (flag) &#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; print: &quot;</span> + integer.getAndIncrement());</span><br><span class="line">                    flag = <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">while</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (!flag) &#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; print: &quot;</span> + integer.getAndIncrement());</span><br><span class="line">                    flag = <span class="literal">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="synchronized-实现"><a href="#synchronized-实现" class="headerlink" title="synchronized 实现"></a>synchronized 实现</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MainApplicationTests</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="type">AtomicInteger</span> <span class="variable">integer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">AtomicInteger</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Object</span> <span class="variable">object</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Object</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">while</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                <span class="keyword">synchronized</span> (object) &#123;</span><br><span class="line">                    object.notify();</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; print: &quot;</span> + integer.getAndIncrement());</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        object.wait();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">while</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                <span class="keyword">synchronized</span> (object) &#123;</span><br><span class="line">                    object.notify();</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; print: &quot;</span> + integer.getAndIncrement());</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        object.wait();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ReentrantLock-和-Condition-实现"><a href="#ReentrantLock-和-Condition-实现" class="headerlink" title="ReentrantLock 和 Condition 实现"></a>ReentrantLock 和 Condition 实现</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MainApplicationTests</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="type">AtomicInteger</span> <span class="variable">integer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">AtomicInteger</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">ReentrantLock</span> <span class="variable">lock</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ReentrantLock</span>();</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Condition</span> <span class="variable">condition</span> <span class="operator">=</span> lock.newCondition();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">while</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                lock.lock();</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; print: &quot;</span> + integer.getAndIncrement());</span><br><span class="line">                    condition.signalAll();</span><br><span class="line">                    <span class="keyword">if</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                        condition.await();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    lock.unlock();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">while</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                lock.lock();</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; print: &quot;</span> + integer.getAndIncrement());</span><br><span class="line">                    condition.signalAll();</span><br><span class="line">                    <span class="keyword">if</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                        condition.await();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    lock.unlock();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="使用-Semphore-实现"><a href="#使用-Semphore-实现" class="headerlink" title="使用 Semphore 实现"></a>使用 Semphore 实现</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MainApplicationTests</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="type">AtomicInteger</span> <span class="variable">integer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">AtomicInteger</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Semaphore</span> <span class="variable">s1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Semaphore</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Semaphore</span> <span class="variable">s2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Semaphore</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">while</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    s1.acquire();</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; print: &quot;</span> + integer.getAndIncrement());</span><br><span class="line">                    s2.release();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">while</span> (integer.get() &lt;= <span class="number">200</span>) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    s2.acquire();</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; print: &quot;</span> + integer.getAndIncrement());</span><br><span class="line">                    s1.release();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Boot 读取配置文件的 6 种方式</title>
    <url>/12fb19f3/</url>
    <content><![CDATA[<h3 id="Value"><a href="#Value" class="headerlink" title="@Value"></a>@Value</h3><ul>
<li><p>StudyApplicationTests.Java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StudyApplicationTests</span> &#123;</span><br><span class="line">    <span class="comment">// @Value(&quot;$&#123;test.name&#125;&quot;)</span></span><br><span class="line">    <span class="comment">// 设置 【默认值】</span></span><br><span class="line">    <span class="meta">@Value(&quot;$&#123;test.name:default&#125;&quot;)</span></span><br><span class="line">    <span class="comment">// static 或者 final 都无法生效</span></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        log.info(<span class="string">&quot;Value 配置获取 &#123;&#125;&quot;</span>, name);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>application.yml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">test:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">testName</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="ConfigurationProperties"><a href="#ConfigurationProperties" class="headerlink" title="@ConfigurationProperties"></a>@ConfigurationProperties</h3><ul>
<li><p>application.yml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">user:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">userName</span></span><br><span class="line">  <span class="attr">age:</span> <span class="number">18</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>User.Java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Data</span><span class="comment">//getter, setter</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@ConfigurationProperties(prefix = &quot;user&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">User</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> Integer age;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>StudyApplicationTests.Java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StudyApplicationTests</span> &#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> User user;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;user = &quot;</span> + user);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Environment"><a href="#Environment" class="headerlink" title="@Environment"></a>@Environment</h3><ul>
<li><p>application.yml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">test:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">testName</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>StudyApplicationTests.Java</p>
<div class="tabs" id="t1"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="t1-1">自动装配的方式</button><button type="button" class="tab " data-href="t1-2">实现接口 EnvironmentAware 的方式</button></ul><div class="tab-contents"><div class="tab-item-content active" id="t1-1"><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StudyApplicationTests</span> &#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> Environment env;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> env.getProperty(<span class="string">&quot;test.name&quot;</span>);</span><br><span class="line">        log.info(<span class="string">&quot;Test name: &quot;</span> + name);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><div class="tab-item-content" id="t1-2"><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StudyApplicationTests</span> <span class="keyword">implements</span> <span class="title class_">EnvironmentAware</span> &#123;</span><br><span class="line">  <span class="meta">@Autowired</span></span><br><span class="line">  <span class="keyword">private</span> Environment env;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Test</span></span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> env.getProperty(<span class="string">&quot;test.name&quot;</span>);</span><br><span class="line">      log.info(<span class="string">&quot;Test name: &quot;</span> + name);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setEnvironment</span><span class="params">(Environment environment)</span> &#123;</span><br><span class="line">      <span class="built_in">this</span>.env = environment;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></li>
</ul>
<h3 id="PropertySources-【只能获取-properties-结尾的文件】"><a href="#PropertySources-【只能获取-properties-结尾的文件】" class="headerlink" title="@PropertySources - 【只能获取 properties 结尾的文件】"></a>@PropertySources - 【只能获取 properties 结尾的文件】</h3><ul>
<li><p>test.properties</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">test.name</span>: <span class="string">testName</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>StudyApplicationTests.Java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StudyApplicationTests</span>  &#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> PropertySourcesConf propertySourcesConf;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        log.info(<span class="string">&quot;Test name: &quot;</span> + propertySourcesConf.getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="PropertySourcesPlaceholderConfigurer"><a href="#PropertySourcesPlaceholderConfigurer" class="headerlink" title="PropertySourcesPlaceholderConfigurer"></a>PropertySourcesPlaceholderConfigurer</h3><ul>
<li><p>test.yml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">test:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">testName123</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>MyYamlConfig.Java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyYamlConfig</span> &#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> PropertySourcesPlaceholderConfigurer <span class="title function_">yamlConfigurer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">PropertySourcesPlaceholderConfigurer</span> <span class="variable">configurer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PropertySourcesPlaceholderConfigurer</span>();</span><br><span class="line">        <span class="type">YamlPropertiesFactoryBean</span> <span class="variable">yaml</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">YamlPropertiesFactoryBean</span>();</span><br><span class="line">        yaml.setResources(<span class="keyword">new</span> <span class="title class_">ClassPathResource</span>(<span class="string">&quot;test.yml&quot;</span>));</span><br><span class="line">        configurer.setProperties(Objects.requireNonNull(yaml.getObject()));</span><br><span class="line">        <span class="keyword">return</span> configurer;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>StudyApplicationTests.Java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StudyApplicationTests</span>  &#123;</span><br><span class="line">    <span class="meta">@Value(&quot;$&#123;test.type&#125;&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> String type;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        log.info(<span class="string">&quot;外部yaml配置获取: &#123;&#125;&quot;</span>, type);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Java-原生方式"><a href="#Java-原生方式" class="headerlink" title="Java 原生方式"></a>Java 原生方式</h3><ul>
<li><p>test.properties</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">test.type</span>: <span class="string">testType</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>StudyApplicationTests.Java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StudyApplicationTests</span>  &#123;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">contextLoads</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="type">InputStreamReader</span> <span class="variable">inputStreamReader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(</span><br><span class="line">                <span class="built_in">this</span>.getClass().getClassLoader().getResourceAsStream(<span class="string">&quot;test.properties&quot;</span>),</span><br><span class="line">                StandardCharsets.UTF_8);</span><br><span class="line">            props.load(inputStreamReader);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">        &#125;</span><br><span class="line">        log.info(<span class="string">&quot;Java 原生获取 properties: &#123;&#125;&quot;</span>, props.getProperty(<span class="string">&quot;test.type&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title>基于 Ubuntu 24.04 部署原生 K8S 1.30.0 集群</title>
    <url>/655061ae/</url>
    <content><![CDATA[<h2 id="如何基于-Ubuntu-24-04-部署原生-K8S-1-30-0-集群？"><a href="#如何基于-Ubuntu-24-04-部署原生-K8S-1-30-0-集群？" class="headerlink" title="如何基于 Ubuntu 24.04 部署原生 K8S 1.30.0 集群？"></a>如何基于 Ubuntu 24.04 部署原生 K8S 1.30.0 集群？</h2><h3 id="一、K8S-集群主机准备"><a href="#一、K8S-集群主机准备" class="headerlink" title="一、K8S 集群主机准备"></a>一、K8S 集群主机准备</h3><h4 id="1-1-主机操作系统说明"><a href="#1-1-主机操作系统说明" class="headerlink" title="1.1 主机操作系统说明"></a>1.1 主机操作系统说明</h4><table>
<thead>
<tr>
<th align="center">序号</th>
<th align="center">操作系统及版本</th>
<th align="center">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">Ubuntu 24.04</td>
<td align="center"></td>
</tr>
</tbody></table>
<h4 id="1-2-主机硬件配置说明"><a href="#1-2-主机硬件配置说明" class="headerlink" title="1.2 主机硬件配置说明"></a>1.2 主机硬件配置说明</h4><table>
<thead>
<tr>
<th align="center">需求</th>
<th align="center">CPU</th>
<th align="center">内存</th>
<th>硬盘</th>
<th align="center">角色</th>
<th align="center">主机名</th>
</tr>
</thead>
<tbody><tr>
<td align="center">值</td>
<td align="center">8C</td>
<td align="center">8G</td>
<td>1024G</td>
<td align="center">master</td>
<td align="center">k8s-master01</td>
</tr>
<tr>
<td align="center">值</td>
<td align="center">8C</td>
<td align="center">16G</td>
<td>1024G</td>
<td align="center">worker(node)</td>
<td align="center">k8s-worker01</td>
</tr>
<tr>
<td align="center">值</td>
<td align="center">8C</td>
<td align="center">16G</td>
<td>1024G</td>
<td align="center">worker(node)</td>
<td align="center">k8s-worker02</td>
</tr>
</tbody></table>
<h4 id="1-3-主机配置"><a href="#1-3-主机配置" class="headerlink" title="1.3 主机配置"></a>1.3 主机配置</h4><h5 id="1-3-1-主机名配置"><a href="#1-3-1-主机名配置" class="headerlink" title="1.3.1 主机名配置"></a>1.3.1 主机名配置</h5><p>由于本次使用3台主机完成kubernetes集群部署，其中1台为master节点，名称为k8s-master01；其中2台为worker节点，名称分别为：k8s-worker01和k8s-worker02</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">master节点</span><br><span class="line"><span class="comment"># hostnamectl set-hostname k8s-master01</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">worker01节点</span><br><span class="line"><span class="comment"># hostnamectl set-hostname k8s-worker01</span></span><br><span class="line"></span><br><span class="line">worker02节点</span><br><span class="line"><span class="comment"># hostnamectl set-hostname k8s-worker02</span></span><br></pre></td></tr></table></figure>

<h5 id="1-3-2-主机-IP-地址配置"><a href="#1-3-2-主机-IP-地址配置" class="headerlink" title="1.3.2 主机 IP 地址配置"></a>1.3.2 主机 IP 地址配置</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:/etc/netplan# ll</span><br><span class="line">总计 24</span><br><span class="line">drwxr-xr-x   2 root root  4096  5月  7 21:56 ./</span><br><span class="line">drwxr-xr-x 139 root root 12288  5月  7 20:57 ../</span><br><span class="line">-rw-r--r--   1 root root   104  4月 10 15:17 01-network-manager-all.yaml</span><br><span class="line">-rw-------   1 root root   624  5月  7 21:56 50-cloud-init.yaml</span><br><span class="line">root@k8s-master01:/etc/netplan# chmod 0600 01-network-manager-all.yaml</span><br><span class="line">root@k8s-master01:/etc/netplan# ll</span><br><span class="line">总计 24</span><br><span class="line">drwxr-xr-x   2 root root  4096  5月  7 21:56 ./</span><br><span class="line">drwxr-xr-x 139 root root 12288  5月  7 20:57 ../</span><br><span class="line">-rw-------   1 root root   104  4月 10 15:17 01-network-manager-all.yaml</span><br><span class="line">-rw-------   1 root root   624  5月  7 21:56 50-cloud-init.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k8s-master01节点IP地址为：192.168.216.140/24</span><br><span class="line">root@k8s-master01:~# vim /etc/netplan/01-network-manager-all.yaml</span><br><span class="line">root@k8s-master01:~# cat /etc/netplan/01-network-manager-all.yaml</span><br><span class="line"></span><br><span class="line">或者 ubuntu 24.04使用下面的命令</span><br><span class="line">root@k8s-master01:~# vim /etc/netplan/50-cloud-init.yaml</span><br><span class="line">root@k8s-master01:~# cat /etc/netplan/50-cloud-init.yaml</span><br><span class="line">network:</span><br><span class="line">  version: 2</span><br><span class="line">  renderer: networkd</span><br><span class="line">  ethernets:</span><br><span class="line">    ens33:</span><br><span class="line">      dhcp4: no</span><br><span class="line">      addresses:</span><br><span class="line">        - 192.168.216.140/24</span><br><span class="line">      routes:</span><br><span class="line">        - to: default</span><br><span class="line">          via: 192.168.216.2</span><br><span class="line">      nameservers:</span><br><span class="line">      	addresses: [119.29.29.29,114.114.114.114,8.8.8.8]</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># netplan apply</span><br><span class="line"># ip a s ens33</span><br></pre></td></tr></table></figure>

<h5 id=""><a href="#" class="headerlink" title=""></a></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@k8s-worker01:/etc/netplan# ll</span><br><span class="line">总计 24</span><br><span class="line">drwxr-xr-x   2 root root  4096  5月  7 21:56 ./</span><br><span class="line">drwxr-xr-x 139 root root 12288  5月  7 20:57 ../</span><br><span class="line">-rw-r--r--   1 root root   104  4月 10 15:17 01-network-manager-all.yaml</span><br><span class="line">-rw-------   1 root root   624  5月  7 21:56 50-cloud-init.yaml</span><br><span class="line">root@k8s-worker01:/etc/netplan# chmod 0600 01-network-manager-all.yaml</span><br><span class="line">root@k8s-worker01:/etc/netplan# ll</span><br><span class="line">总计 24</span><br><span class="line">drwxr-xr-x   2 root root  4096  5月  7 21:56 ./</span><br><span class="line">drwxr-xr-x 139 root root 12288  5月  7 20:57 ../</span><br><span class="line">-rw-------   1 root root   104  4月 10 15:17 01-network-manager-all.yaml</span><br><span class="line">-rw-------   1 root root   624  5月  7 21:56 50-cloud-init.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k8s-worker01节点IP地址为：192.168.216.141/24</span><br><span class="line">root@k8s-worker01:~# vim /etc/netplan/01-network-manager-all.yaml</span><br><span class="line">root@k8s-worker01:~# cat /etc/netplan/01-network-manager-all.yaml</span><br><span class="line"></span><br><span class="line">或者 ubuntu 24.04使用下面的命令</span><br><span class="line">root@k8s-worker01:~# vim /etc/netplan/50-cloud-init.yaml</span><br><span class="line">root@k8s-worker01:~# cat /etc/netplan/50-cloud-init.yaml</span><br><span class="line">network:</span><br><span class="line">  version: 2</span><br><span class="line">  renderer: networkd</span><br><span class="line">  ethernets:</span><br><span class="line">    ens33:</span><br><span class="line">      dhcp4: no</span><br><span class="line">      addresses:</span><br><span class="line">        - 192.168.216.141/24</span><br><span class="line">      routes:</span><br><span class="line">        - to: default</span><br><span class="line">          via: 192.168.216.2</span><br><span class="line">      nameservers:</span><br><span class="line">      	addresses: [119.29.29.29,114.114.114.114,8.8.8.8]</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># netplan apply</span><br><span class="line"># ip a s ens33</span><br></pre></td></tr></table></figure>

<h5 id="-1"><a href="#-1" class="headerlink" title=""></a></h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-worker02:/etc/netplan<span class="comment"># ll</span></span><br><span class="line">总计 24</span><br><span class="line">drwxr-xr-x   2 root root  4096  5月  7 21:56 ./</span><br><span class="line">drwxr-xr-x 139 root root 12288  5月  7 20:57 ../</span><br><span class="line">-rw-r--r--   1 root root   104  4月 10 15:17 01-network-manager-all.yaml</span><br><span class="line">-rw-------   1 root root   624  5月  7 21:56 50-cloud-init.yaml</span><br><span class="line">root@k8s-worker02:/etc/netplan<span class="comment"># chmod 0600 01-network-manager-all.yaml</span></span><br><span class="line">root@k8s-worker02:/etc/netplan<span class="comment"># ll</span></span><br><span class="line">总计 24</span><br><span class="line">drwxr-xr-x   2 root root  4096  5月  7 21:56 ./</span><br><span class="line">drwxr-xr-x 139 root root 12288  5月  7 20:57 ../</span><br><span class="line">-rw-------   1 root root   104  4月 10 15:17 01-network-manager-all.yaml</span><br><span class="line">-rw-------   1 root root   624  5月  7 21:56 50-cloud-init.yaml</span><br><span class="line"></span><br><span class="line">k8s-worker02节点IP地址为：192.168.216.142/24</span><br><span class="line">root@k8s-worker02:~<span class="comment"># vim /etc/netplan/01-network-manager-all.yaml</span></span><br><span class="line">root@k8s-worker02:~<span class="comment"># cat /etc/netplan/01-network-manager-all.yaml</span></span><br><span class="line"></span><br><span class="line">或者 ubuntu 24.04使用下面的命令</span><br><span class="line">root@k8s-worker02:~<span class="comment"># vim /etc/netplan/50-cloud-init.yaml</span></span><br><span class="line">root@k8s-worker02:~<span class="comment"># cat /etc/netplan/50-cloud-init.yaml</span></span><br><span class="line">network:</span><br><span class="line">  version: 2</span><br><span class="line">  renderer: networkd</span><br><span class="line">  ethernets:</span><br><span class="line">    ens33:</span><br><span class="line">      dhcp4: no</span><br><span class="line">      addresses:</span><br><span class="line">        - 192.168.216.142/24</span><br><span class="line">      routes:</span><br><span class="line">        - to: default</span><br><span class="line">          via: 192.168.216.2</span><br><span class="line">      nameservers:</span><br><span class="line">      	addresses: [119.29.29.29,114.114.114.114,8.8.8.8]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># netplan apply</span></span><br><span class="line"><span class="comment"># ip a s ens33</span></span><br></pre></td></tr></table></figure>



<h5 id="1-3-3-主机名与IP地址解析"><a href="#1-3-3-主机名与IP地址解析" class="headerlink" title="1.3.3 主机名与IP地址解析"></a>1.3.3 主机名与IP地址解析</h5><p>所有集群主机均需要进行配置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt;&gt; /etc/hosts &lt;&lt; EOF</span></span><br><span class="line">192.168.216.140 k8s-master01</span><br><span class="line">192.168.216.141 k8s-worker01</span><br><span class="line">192.168.216.142 k8s-worker02</span><br></pre></td></tr></table></figure>

<p><img src="/655061ae/image-20240508032426622.png" alt="image-20240508032426622"></p>
<h5 id="1-3-4-时间同步配置"><a href="#1-3-4-时间同步配置" class="headerlink" title="1.3.4 时间同步配置"></a>1.3.4 时间同步配置</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">查看时间</span><br><span class="line"><span class="comment"># date</span></span><br><span class="line">2024年 05月 08日 星期三 03:24:14 CST</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">更换时区</span><br><span class="line"><span class="comment"># timedatectl set-timezone Asia/Shanghai</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">再次查看时间</span><br><span class="line"><span class="comment"># date</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">安装 ntpdate 命令</span><br><span class="line"><span class="comment"># apt install ntpdate</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">使用 ntpdate 命令同步时间</span><br><span class="line"><span class="comment"># ntpdate time1.aliyun.com</span></span><br><span class="line">2024-05-08 03:28:58.838404 (+0800) -0.030498 +/- 0.032866 time1.aliyun.com 203.107.6.88 s2 no-leap</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">通过计划任务实现时间同步</span><br><span class="line"></span><br><span class="line"><span class="comment"># crontab -e</span></span><br><span class="line">no crontab <span class="keyword">for</span> root - using an empty one</span><br><span class="line"></span><br><span class="line">Select an editor.  To change later, run <span class="string">&#x27;select-editor&#x27;</span>.</span><br><span class="line">  1. /bin/nano        &lt;---- easiest</span><br><span class="line">  2. /usr/bin/vim.tiny</span><br><span class="line">  3. /bin/ed</span><br><span class="line">  </span><br><span class="line">Choose 1-3 [1]: 1</span><br><span class="line">crontab: installing new crontab</span><br><span class="line"></span><br><span class="line">在文件末尾加上</span><br><span class="line">0 0 * * * ntpdate time1.aliyun.com</span><br></pre></td></tr></table></figure>

<p><img src="/655061ae/image-20240508033256857.png" alt="image-20240508033256857"></p>
<h5 id="1-3-5-配置内核转发及网桥过滤"><a href="#1-3-5-配置内核转发及网桥过滤" class="headerlink" title="1.3.5 配置内核转发及网桥过滤"></a>1.3.5 配置内核转发及网桥过滤</h5><p>所有主机均需要操作</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">创建内核模块文件</span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /etc/modules-load.d/k8s.conf</span></span><br><span class="line">overlay</span><br><span class="line">br_netfilter</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">本次执行，手动加载此模块</span><br><span class="line"><span class="comment"># modprobe overlay</span></span><br><span class="line"><span class="comment"># modprobe br_netfilter</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">查看已加载的模块</span><br><span class="line"># lsmod | egrep &quot;overlay&quot;</span><br><span class="line"></span><br><span class="line"># lsmod | egrep &quot;br_netfilter&quot;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">添加网桥过滤及内核转发配置文件</span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /etc/sysctl.d/k8s.conf</span></span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">加载配置</span><br><span class="line"><span class="comment"># sysctl -p /etc/sysctl.d/k8s.conf</span></span><br><span class="line"></span><br><span class="line">查看配置是否生效</span><br><span class="line"><span class="comment"># sysctl -a | grep ip_forward</span></span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.ipv4.ip_forward_update_priority = 1</span><br><span class="line">net.ipv4.ip_forward_use_pmtu = 0</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">加载内核参数</span><br><span class="line"><span class="comment"># sysctl --system</span></span><br></pre></td></tr></table></figure>



<h5 id="1-3-6-安装-ipset-及-ipvsadm"><a href="#1-3-6-安装-ipset-及-ipvsadm" class="headerlink" title="1.3.6 安装 ipset 及 ipvsadm"></a>1.3.6 安装 ipset 及 ipvsadm</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">安装 ipset 及 ipvsadm</span><br><span class="line"># apt install ipset ipvsadm</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">配置 ipvsadm 模块加载</span><br><span class="line">添加需要加载的模块</span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /etc/modules-load.d/ipvs.conf</span></span><br><span class="line">ip_vs</span><br><span class="line">ip_vs_rr</span><br><span class="line">ip_vs_wrr</span><br><span class="line">ip_vs_sh</span><br><span class="line">nf_conntrack</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">创建加载模块脚本文件</span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /root/ipvs.sh</span></span><br><span class="line"><span class="comment">#!/bin/sh</span></span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">执行脚本文件</span><br><span class="line"><span class="comment"># bash ipvs.sh</span></span><br><span class="line"></span><br><span class="line">查看配置是否生效</span><br><span class="line"><span class="comment"># lsmod | grep ip_vs</span></span><br><span class="line">ip_vs_sh               12288  0</span><br><span class="line">ip_vs_wrr              12288  0</span><br><span class="line">ip_vs_rr               12288  0</span><br><span class="line">ip_vs                 225280  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr</span><br><span class="line">nf_conntrack          200704  1 ip_vs</span><br><span class="line">nf_defrag_ipv6         24576  2 nf_conntrack,ip_vs</span><br><span class="line">libcrc32c              12288  2 nf_conntrack,ip_vs</span><br></pre></td></tr></table></figure>



<h5 id="1-3-7-关闭-SWAP-分区"><a href="#1-3-7-关闭-SWAP-分区" class="headerlink" title="1.3.7 关闭 SWAP 分区"></a>1.3.7 关闭 SWAP 分区</h5><p>修改完成后需要重启操作系统，如不重启，可临时关闭，命令为 <code>swapoff -a</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">查看是否存在swap分区</span><br><span class="line"># free -m</span><br><span class="line">               total        used        free      shared  bu                                                                                ff/cache   available</span><br><span class="line">内存：          3868        1276        1818          33                                                                                        1043        2591</span><br><span class="line">交换：          3763           0        3763</span><br><span class="line"></span><br><span class="line">永远关闭 swap 分区，需要重启操作系统</span><br><span class="line"># vim /etc/fstab</span><br><span class="line">注释</span><br><span class="line">#/swap.img</span><br></pre></td></tr></table></figure>

<p><img src="/655061ae/image-20240508040530750.png" alt="image-20240508040530750"></p>
<h3 id="二、K8S-集群容器运行时-Containerd-准备"><a href="#二、K8S-集群容器运行时-Containerd-准备" class="headerlink" title="二、K8S 集群容器运行时 Containerd 准备"></a>二、K8S 集群容器运行时 Containerd 准备</h3><h4 id="2-1-Containerd-部署文件获取"><a href="#2-1-Containerd-部署文件获取" class="headerlink" title="2.1 Containerd 部署文件获取"></a>2.1 Containerd 部署文件获取</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">下载指定版本 containerd</span><br><span class="line"><span class="comment"># wget https://github.com/containerd/containerd/releases/download/v1.7.16/cri-containerd-1.7.16-linux-amd64.tar.gz</span></span><br><span class="line"></span><br><span class="line">解压安装</span><br><span class="line"><span class="comment"># tar xf cri-containerd-1.7.16-linux-amd64.tar.gz -C /</span></span><br><span class="line"></span><br><span class="line">验证是否安装成功</span><br><span class="line"><span class="comment"># which containerd</span></span><br><span class="line">/usr/local/bin/containerd</span><br><span class="line"><span class="comment"># which runc</span></span><br><span class="line">/usr/local/sbin/runc</span><br><span class="line"><span class="comment"># containerd --version</span></span><br><span class="line">containerd github.com/containerd/containerd v1.7.16 83031836b2cf55637d7abf847b17134c51b38e53</span><br><span class="line"><span class="comment"># runc --version</span></span><br><span class="line">runc version 1.1.12</span><br><span class="line">commit: v1.1.12-0-g51d5e946</span><br><span class="line">spec: 1.0.2-dev</span><br><span class="line">go: go1.21.9</span><br><span class="line">libseccomp: 2.5.5</span><br></pre></td></tr></table></figure>

<h4 id="2-2-Containerd-配置文件生成并修改"><a href="#2-2-Containerd-配置文件生成并修改" class="headerlink" title="2.2 Containerd 配置文件生成并修改"></a>2.2 Containerd 配置文件生成并修改</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">创建配置文件目录</span><br><span class="line"><span class="comment"># mkdir /etc/containerd</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">生成配置文件</span><br><span class="line"><span class="comment"># containerd config default &gt; /etc/containerd/config.toml</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">修改第67行</span><br><span class="line"><span class="comment"># vim /etc/containerd/config.toml</span></span><br><span class="line"></span><br><span class="line">sandbox_image = <span class="string">&quot;registry.k8s.io/pause:3.9&quot;</span> 由3.8修改为3.9</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">如果使用阿里云容器镜像仓库，也可以修改为：</span><br><span class="line">sandbox_image = <span class="string">&quot;registry.aliyuncs.com/google_containers/pause:3.9&quot;</span> 由3.8修改为3.9</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">修改第139行</span><br><span class="line"><span class="comment"># vim /etc/containerd/config.toml</span></span><br><span class="line"></span><br><span class="line">SystemdCgroup = <span class="literal">true</span> 由<span class="literal">false</span>修改为<span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h4 id="2-3-Containerd-启动及开机启动"><a href="#2-3-Containerd-启动及开机启动" class="headerlink" title="2.3 Containerd 启动及开机启动"></a>2.3 Containerd 启动及开机启动</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">设置开机自启动并现在启动</span><br><span class="line"><span class="comment"># systemctl enable --now containerd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ls /var/run/containerd</span></span><br><span class="line">containerd.sock        io.containerd.runtime.v1.linux</span><br><span class="line">containerd.sock.ttrpc  io.containerd.runtime.v2.task</span><br><span class="line"></span><br><span class="line">验证其版本</span><br><span class="line"><span class="comment"># containerd --version</span></span><br><span class="line">containerd github.com/containerd/containerd v1.7.16 83031836b2cf55637d7abf847b17134c51b38e53</span><br></pre></td></tr></table></figure>

<h3 id="三、K8S-集群部署"><a href="#三、K8S-集群部署" class="headerlink" title="三、K8S 集群部署"></a>三、K8S 集群部署</h3><h4 id="3-1-K8S-集群软件-apt-源准备"><a href="#3-1-K8S-集群软件-apt-源准备" class="headerlink" title="3.1 K8S 集群软件 apt 源准备"></a>3.1 K8S 集群软件 apt 源准备</h4><blockquote>
<p>本次使用kubernetes社区软件源或阿里云软件源仓库</p>
</blockquote>
<h5 id="下载用于-Kubernetes-软件包仓库的公共签名密钥"><a href="#下载用于-Kubernetes-软件包仓库的公共签名密钥" class="headerlink" title="下载用于 Kubernetes 软件包仓库的公共签名密钥"></a>下载用于 Kubernetes 软件包仓库的公共签名密钥</h5><blockquote>
<p>所有仓库都使用相同的签名密钥，因此你可以忽略URL中的版本</p>
</blockquote>
<p><strong>K8S社区</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -fsSL https://pkgs.k8s.io/core:/stable/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg</span></span><br></pre></td></tr></table></figure>

<p><strong>阿里云</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg</span></span><br></pre></td></tr></table></figure>

<h5 id="添加-Kubernetes-apt-仓库"><a href="#添加-Kubernetes-apt-仓库" class="headerlink" title="添加 Kubernetes apt 仓库"></a>添加 Kubernetes apt 仓库</h5><blockquote>
<p>请注意，此仓库仅包含适用于 Kubernetes 1.30 的软件包，对于其他 Kubernetes 次要版本，则需要修改 URL 中的 Kubernetes 次要版本以匹配你所需的次要版本。</p>
</blockquote>
<blockquote>
<p>此操作会覆盖 &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;kubernetes.list 中现存的所有配置，如果有的情况下。</p>
</blockquote>
<p><strong>K8S 社区</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># echo &quot;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list</span></span><br></pre></td></tr></table></figure>

<p><strong>阿里云</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># echo &quot;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/ /&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list</span></span><br></pre></td></tr></table></figure>

<h5 id="更新-apt-包索引"><a href="#更新-apt-包索引" class="headerlink" title="更新 apt 包索引"></a>更新 apt 包索引</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sudo apt-get update</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ls /etc/apt/sources.list.d/</span></span><br><span class="line">kubernetes.list  ubuntu.sources  ubuntu.sources.curtin.orig</span><br></pre></td></tr></table></figure>

<h4 id="3-2-K8S-集群软件安装及-kubelet-配置"><a href="#3-2-K8S-集群软件安装及-kubelet-配置" class="headerlink" title="3.2 K8S 集群软件安装及 kubelet 配置"></a>3.2 K8S 集群软件安装及 kubelet 配置</h4><h5 id="3-2-1-k8s-集群软件安装"><a href="#3-2-1-k8s-集群软件安装" class="headerlink" title="3.2.1 k8s 集群软件安装"></a>3.2.1 k8s 集群软件安装</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">查看软件列表</span><br><span class="line"><span class="comment"># apt-cache policy kubeadm</span></span><br><span class="line">kubeadm:</span><br><span class="line">  已安装：(无)</span><br><span class="line">  候选： 1.30.0-1.1</span><br><span class="line">  版本列表：</span><br><span class="line">     1.30.0-1.1 500</span><br><span class="line">        500 https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb  Packages</span><br><span class="line">        </span><br><span class="line"><span class="comment"># apt-cache policy kubelet</span></span><br><span class="line">kubelet:</span><br><span class="line">  已安装：(无)</span><br><span class="line">  候选： 1.30.0-1.1</span><br><span class="line">  版本列表：</span><br><span class="line">     1.30.0-1.1 500</span><br><span class="line">        500 https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb  Packages</span><br><span class="line">        </span><br><span class="line"><span class="comment"># apt-cache policy kubectl</span></span><br><span class="line">kubectl:</span><br><span class="line">  已安装：(无)</span><br><span class="line">  候选： 1.30.0-1.1</span><br><span class="line">  版本列表：</span><br><span class="line">     1.30.0-1.1 500</span><br><span class="line">        500 https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb  Packages</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">查看软件列表及其依赖关系</span><br><span class="line"><span class="comment"># apt-cache showpkg kubeadm</span></span><br><span class="line">Package: kubeadm</span><br><span class="line">Versions:</span><br><span class="line">1.30.0-1.1 (/var/lib/apt/lists/mirrors.aliyun.com_kubernetes-new_core_stable_v1.30_deb_Packages)</span><br><span class="line"> Description Language:</span><br><span class="line">                 File: /var/lib/apt/lists/mirrors.aliyun.com_kubernetes-new_core_stable_v1.30_deb_Packages</span><br><span class="line">                  MD5:</span><br><span class="line"> Description Language:</span><br><span class="line">                 File: /var/lib/apt/lists/mirrors.aliyun.com_kubernetes-new_core_stable_v1.30_deb_Packages</span><br><span class="line">                  MD5:</span><br><span class="line"> Description Language:</span><br><span class="line">                 File: /var/lib/apt/lists/mirrors.aliyun.com_kubernetes-new_core_stable_v1.30_deb_Packages</span><br><span class="line">                  MD5:</span><br><span class="line"> Description Language:</span><br><span class="line">                 File: /var/lib/apt/lists/mirrors.aliyun.com_kubernetes-new_core_stable_v1.30_deb_Packages</span><br><span class="line">                  MD5:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Reverse Depends:</span><br><span class="line">  kubeadm:arm64,kubeadm</span><br><span class="line">  kubeadm:ppc64el,kubeadm</span><br><span class="line">  kubeadm:s390x,kubeadm</span><br><span class="line">Dependencies:</span><br><span class="line">1.30.0-1.1 - cri-tools (2 1.30.0) kubeadm:arm64 (32 (null)) kubeadm:s390x (32 (null)) kubeadm:ppc64el (32 (null))</span><br><span class="line">Provides:</span><br><span class="line">1.30.0-1.1 -</span><br><span class="line">Reverse Provides:</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">查看软件版本</span><br><span class="line"><span class="comment"># apt-cache madison kubeadm</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">默认安装</span><br><span class="line"><span class="comment"># apt-get install -y kubelet kubeadm kubectl</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">安装指定版本</span><br><span class="line"><span class="comment"># apt-get install -y kubelet=1.30.0-1.1 kubeadm=1.30.0-1.1 kubectl=1.30.0-1.1</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">锁定版本，防止后期自动更新</span><br><span class="line"><span class="comment"># apt-mark hold kubelet kubeadm kubectl</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">解锁版本，执行更新</span><br><span class="line"><span class="comment"># spt-mark unhold kubelet kubeadm kubectl</span></span><br></pre></td></tr></table></figure>

<h5 id="3-2-2-配置-kubelet"><a href="#3-2-2-配置-kubelet" class="headerlink" title="3.2.2 配置 kubelet"></a>3.2.2 配置 kubelet</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim /etc/sysconfig/kubelet</span></span><br><span class="line">KUBELET_EXTRA_ARGS=<span class="string">&quot;--cgroup-driver=systemd&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">设置 kubelet 为开机自启即可，由于没有生成配置文件，集群初始化后自动启动</span><br><span class="line"><span class="comment"># systemctl enable kubelet</span></span><br></pre></td></tr></table></figure>

<h5 id="3-3-2-生成部署配置文件"><a href="#3-3-2-生成部署配置文件" class="headerlink" title="3.3.2 生成部署配置文件"></a>3.3.2 生成部署配置文件</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># kubeadm config print init-defaults &gt; kubeadm-config.yaml</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">使用 kubernetes 社区版容器镜像仓库</span><br><span class="line">root@k8s-master01:~<span class="comment"># vim kubeadm-config.yaml</span></span><br><span class="line">root@k8s-master01:~<span class="comment"># cat kubeadm-config.yaml</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta3</span><br><span class="line">bootstrapTokens:</span><br><span class="line">- <span class="built_in">groups</span>:</span><br><span class="line">  - system:bootstrappers:kubeadm:default-node-token</span><br><span class="line">  token: abcdef.0123456789abcdef</span><br><span class="line">  ttl: 24h0m0s</span><br><span class="line">  usages:</span><br><span class="line">  - signing</span><br><span class="line">  - authentication</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: 192.168.216.140</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: unix:///var/run/containerd/containerd.sock</span><br><span class="line">  imagePullPolicy: IfNotPresent</span><br><span class="line">  name: k8s-master01</span><br><span class="line">  taints: null</span><br><span class="line">---</span><br><span class="line">apiServer:</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta3</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns: &#123;&#125;</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    dataDir: /var/lib/etcd</span><br><span class="line"><span class="comment">#imageRepository: registry.k8s.io</span></span><br><span class="line">imageRepository: registry.aliyuncs.com/google_containers</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: 1.30.0</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  serviceSubnet: 10.96.0.0/12</span><br><span class="line">  podSubnet: 10.244.0.0/16</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">---</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">cgroupDriver: systemd</span><br></pre></td></tr></table></figure>

<p><img src="/655061ae/image-20240509001211658.png" alt="image-20240509001211658"></p>
<h5 id="3-3-3-查看并下载镜像"><a href="#3-3-3-查看并下载镜像" class="headerlink" title="3.3.3 查看并下载镜像"></a>3.3.3 查看并下载镜像</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># kubeadm config images list</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">使用 阿里云容器镜像仓库</span><br><span class="line">root@k8s-master01:~<span class="comment"># kubeadm config images list --image-repository registry.aliyuncs.com/google_containers</span></span><br><span class="line">registry.aliyuncs.com/google_containers/kube-apiserver:v1.30.0</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-scheduler:v1.30.0</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-proxy:v1.30.0</span><br><span class="line">registry.aliyuncs.com/google_containers/coredns:v1.11.1</span><br><span class="line">registry.aliyuncs.com/google_containers/pause:3.9</span><br><span class="line">registry.aliyuncs.com/google_containers/etcd:3.5.12-0</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers</span></span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.30.0</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.30.0</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.30.0</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.11.1</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.9</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.12-0</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">验证是否下载成功</span><br><span class="line">root@k8s-master01:~<span class="comment"># crictl images</span></span><br><span class="line">IMAGE                                                             TAG                 IMAGE ID            SIZE</span><br><span class="line">registry.aliyuncs.com/google_containers/coredns                   v1.11.1             cbb01a7bd410d       18.2MB</span><br><span class="line">registry.aliyuncs.com/google_containers/etcd                      3.5.12-0            3861cfcd7c04c       57.2MB</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-apiserver            v1.30.0             c42f13656d0b2       32.7MB</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-controller-manager   v1.30.0             c7aad43836fa5       31MB</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-proxy                v1.30.0             a0bf559e280cf       29MB</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-scheduler            v1.30.0             259c8277fcbbc       19.2MB</span><br><span class="line">registry.aliyuncs.com/google_containers/pause                     3.9                 e6f1816883972       322kB</span><br></pre></td></tr></table></figure>

<h5 id="3-3-4-使用部署配置文件初始化K8S集群"><a href="#3-3-4-使用部署配置文件初始化K8S集群" class="headerlink" title="3.3.4 使用部署配置文件初始化K8S集群"></a>3.3.4 使用部署配置文件初始化K8S集群</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># kubeadm init --config kubeadm-config.yaml</span></span><br><span class="line">[init] Using Kubernetes version: v1.30.0</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">&#x27;kubeadm config images pull&#x27;</span></span><br><span class="line">[certs] Using certificateDir folder <span class="string">&quot;/etc/kubernetes/pki&quot;</span></span><br><span class="line">[certs] Generating <span class="string">&quot;ca&quot;</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">&quot;apiserver&quot;</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [k8s-master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.216.140]</span><br><span class="line">[certs] Generating <span class="string">&quot;apiserver-kubelet-client&quot;</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">&quot;front-proxy-ca&quot;</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">&quot;front-proxy-client&quot;</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">&quot;etcd/ca&quot;</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">&quot;etcd/server&quot;</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [k8s-master01 localhost] and IPs [192.168.216.140 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">&quot;etcd/peer&quot;</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [k8s-master01 localhost] and IPs [192.168.216.140 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">&quot;etcd/healthcheck-client&quot;</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">&quot;apiserver-etcd-client&quot;</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">&quot;sa&quot;</span> key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">&quot;/etc/kubernetes&quot;</span></span><br><span class="line">[kubeconfig] Writing <span class="string">&quot;admin.conf&quot;</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">&quot;super-admin.conf&quot;</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">&quot;kubelet.conf&quot;</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">&quot;controller-manager.conf&quot;</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">&quot;scheduler.conf&quot;</span> kubeconfig file</span><br><span class="line">[etcd] Creating static Pod manifest <span class="keyword">for</span> <span class="built_in">local</span> etcd <span class="keyword">in</span> <span class="string">&quot;/etc/kubernetes/manifests&quot;</span></span><br><span class="line">[control-plane] Using manifest folder <span class="string">&quot;/etc/kubernetes/manifests&quot;</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">&quot;kube-apiserver&quot;</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">&quot;kube-controller-manager&quot;</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">&quot;kube-scheduler&quot;</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">&quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span></span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">&quot;/var/lib/kubelet/config.yaml&quot;</span></span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[wait-control-plane] Waiting <span class="keyword">for</span> the kubelet to boot up the control plane as static Pods from directory <span class="string">&quot;/etc/kubernetes/manifests&quot;</span></span><br><span class="line">[kubelet-check] Waiting <span class="keyword">for</span> a healthy kubelet. This can take up to 4m0s</span><br><span class="line">[kubelet-check] The kubelet is healthy after 501.759264ms</span><br><span class="line">[api-check] Waiting <span class="keyword">for</span> a healthy API server. This can take up to 4m0s</span><br><span class="line">[api-check] The API server is healthy after 4.003295146s</span><br><span class="line">[upload-config] Storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">&quot;kubeadm-config&quot;</span> <span class="keyword">in</span> the <span class="string">&quot;kube-system&quot;</span> Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap <span class="string">&quot;kubelet-config&quot;</span> <span class="keyword">in</span> namespace kube-system with the configuration <span class="keyword">for</span> the kubelets <span class="keyword">in</span> the cluster</span><br><span class="line">[upload-certs] Skipping phase. Please see --upload-certs</span><br><span class="line">[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]</span><br><span class="line">[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: abcdef.0123456789abcdef</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs <span class="keyword">in</span> order <span class="keyword">for</span> nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] Configured RBAC rules to allow certificate rotation <span class="keyword">for</span> all node client certificates <span class="keyword">in</span> the cluster</span><br><span class="line">[bootstrap-token] Creating the <span class="string">&quot;cluster-info&quot;</span> ConfigMap <span class="keyword">in</span> the <span class="string">&quot;kube-public&quot;</span> namespace</span><br><span class="line">[kubelet-finalize] Updating <span class="string">&quot;/etc/kubernetes/kubelet.conf&quot;</span> to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  <span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, <span class="keyword">if</span> you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  <span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">&quot;kubectl apply -f [podnetwork].yaml&quot;</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can <span class="built_in">join</span> any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm <span class="built_in">join</span> 192.168.216.140:6443 --token abcdef.0123456789abcdef \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:21664a8d4b8e888f46ae7437ba82dbab489030fbbdff2d0c6786433b6812bb81</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">root@k8s-master01:~<span class="comment"># cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">root@k8s-master01:~<span class="comment"># chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME           STATUS     ROLES           AGE    VERSION</span><br><span class="line">k8s-master01   NotReady   control-plane   4m5s   v1.30.0</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">加入工作节点</span><br><span class="line">root@k8s-worker01:/home/daniel<span class="comment"># kubeadm join 192.168.216.140:6443 --token abcdef.0123456789abcdef \</span></span><br><span class="line">        --discovery-token-ca-cert-hash sha256:21664a8d4b8e888f46ae7437ba82dbab489030fbbdff2d0c6786433b6812bb81</span><br><span class="line">root@k8s-worker02:/home/daniel<span class="comment"># kubeadm join 192.168.216.140:6443 --token abcdef.0123456789abcdef \</span></span><br><span class="line">        --discovery-token-ca-cert-hash sha256:21664a8d4b8e888f46ae7437ba82dbab489030fbbdff2d0c6786433b6812bb81</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">&#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span></span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">&quot;/var/lib/kubelet/config.yaml&quot;</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">&quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span></span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-check] Waiting <span class="keyword">for</span> a healthy kubelet. This can take up to 4m0s</span><br><span class="line">[kubelet-check] The kubelet is healthy after 1.001753984s</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run <span class="string">&#x27;kubectl get nodes&#x27;</span> on the control-plane to see this node <span class="built_in">join</span> the cluster.</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME           STATUS     ROLES           AGE     VERSION</span><br><span class="line">k8s-master01   NotReady   control-plane   7m48s   v1.30.0</span><br><span class="line">k8s-worker01   NotReady   &lt;none&gt;          2m13s   v1.30.0</span><br><span class="line">k8s-worker02   NotReady   &lt;none&gt;          2m7s    v1.30.0</span><br><span class="line"></span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl get pods -n kube-system</span></span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-7b5944fdcf-f9q4z               0/1     Pending   0          8m26s</span><br><span class="line">coredns-7b5944fdcf-s2wqz               0/1     Pending   0          8m26s</span><br><span class="line">etcd-k8s-master01                      1/1     Running   0          8m42s</span><br><span class="line">kube-apiserver-k8s-master01            1/1     Running   0          8m42s</span><br><span class="line">kube-controller-manager-k8s-master01   1/1     Running   0          8m42s</span><br><span class="line">kube-proxy-5bv9g                       1/1     Running   0          8m26s</span><br><span class="line">kube-proxy-c2nxf                       1/1     Running   0          3m9s</span><br><span class="line">kube-proxy-hgdpt                       1/1     Running   0          3m3s</span><br><span class="line">kube-scheduler-k8s-master01            1/1     Running   0          8m43s</span><br></pre></td></tr></table></figure>



<h3 id="四、K8S-集群网络插件-calico-部署"><a href="#四、K8S-集群网络插件-calico-部署" class="headerlink" title="四、K8S 集群网络插件 calico 部署"></a>四、K8S 集群网络插件 calico 部署</h3><p><a href="https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart">calico访问链接</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/tigera-operator.yaml</span></span><br><span class="line"></span><br><span class="line">验证是否安装成功</span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl get ns</span></span><br><span class="line">NAME              STATUS   AGE</span><br><span class="line">default           Active   14m</span><br><span class="line">kube-node-lease   Active   14m</span><br><span class="line">kube-public       Active   14m</span><br><span class="line">kube-system       Active   14m</span><br><span class="line">tigera-operator   Active   24s</span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl get pods -n tigera-operator</span></span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">tigera-operator-6678f5cb9d-br2g5   1/1     Running   0          61s</span><br><span class="line"></span><br><span class="line">忽略这步，建议先下载下来再启动</span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/custom-resources.yaml</span></span><br><span class="line"></span><br><span class="line">root@k8s-master01:~<span class="comment"># wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/custom-resources.yaml</span></span><br><span class="line">root@k8s-master01:~<span class="comment"># cat custom-resources.yaml</span></span><br><span class="line"><span class="comment"># This section includes base Calico installation configuration.</span></span><br><span class="line"><span class="comment"># For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.Installation</span></span><br><span class="line">apiVersion: operator.tigera.io/v1</span><br><span class="line">kind: Installation</span><br><span class="line">metadata:</span><br><span class="line">  name: default</span><br><span class="line">spec:</span><br><span class="line">  <span class="comment"># Configures Calico networking.</span></span><br><span class="line">  calicoNetwork:</span><br><span class="line">    <span class="comment"># Note: The ipPools section cannot be modified post-install.</span></span><br><span class="line">    ipPools:</span><br><span class="line">    - blockSize: 26</span><br><span class="line">      cidr: 10.244.0.0/16</span><br><span class="line">      encapsulation: VXLANCrossSubnet</span><br><span class="line">      natOutgoing: Enabled</span><br><span class="line">      nodeSelector: all()</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"><span class="comment"># This section configures the Calico API server.</span></span><br><span class="line"><span class="comment"># For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.APIServer</span></span><br><span class="line">apiVersion: operator.tigera.io/v1</span><br><span class="line">kind: APIServer</span><br><span class="line">metadata:</span><br><span class="line">  name: default</span><br><span class="line">spec: &#123;&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/655061ae/image-20240509003731803.png" alt="image-20240509003731803"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">安装 calico</span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl create -f custom-resources.yaml</span></span><br><span class="line">installation.operator.tigera.io/default created</span><br><span class="line">apiserver.operator.tigera.io/default created</span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl get ns</span></span><br><span class="line">NAME              STATUS   AGE</span><br><span class="line">calico-system     Active   7s</span><br><span class="line">default           Active   21m</span><br><span class="line">kube-node-lease   Active   21m</span><br><span class="line">kube-public       Active   21m</span><br><span class="line">kube-system       Active   21m</span><br><span class="line">tigera-operator   Active   7m41s</span><br><span class="line"></span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl get pods -n calico-system</span></span><br><span class="line">NAME                                       READY   STATUS              RESTARTS   AGE</span><br><span class="line">calico-kube-controllers-7856db5868-g7bhj   0/1     Pending             0          33s</span><br><span class="line">calico-node-2dwbp                          0/1     Init:0/2            0          33s</span><br><span class="line">calico-node-fwhqn                          0/1     Init:0/2            0          33s</span><br><span class="line">calico-node-swzvl                          0/1     Init:0/2            0          33s</span><br><span class="line">calico-typha-865f49f54f-gpdqg              0/1     ContainerCreating   0          30s</span><br><span class="line">calico-typha-865f49f54f-xz5vl              0/1     ContainerCreating   0          33s</span><br><span class="line">csi-node-driver-5l8gn                      0/2     ContainerCreating   0          33s</span><br><span class="line">csi-node-driver-tphmn                      0/2     ContainerCreating   0          33s</span><br><span class="line">csi-node-driver-whhv7                      0/2     ContainerCreating   0          33s</span><br><span class="line"></span><br><span class="line">root@k8s-master01:~<span class="comment"># watch kubectl get pods -n calico-system</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># kubectl get pods -n kube-system -o wide</span></span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE    IP                NODE           NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-7b5944fdcf-f9q4z               1/1     Running   0          161m   10.244.32.131     k8s-master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-7b5944fdcf-s2wqz               1/1     Running   0          161m   10.244.32.130     k8s-master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-k8s-master01                      1/1     Running   0          161m   192.168.216.140   k8s-master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-k8s-master01            1/1     Running   0          161m   192.168.216.140   k8s-master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-k8s-master01   1/1     Running   0          161m   192.168.216.140   k8s-master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-5bv9g                       1/1     Running   0          161m   192.168.216.140   k8s-master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-c2nxf                       1/1     Running   0          156m   192.168.216.141   k8s-worker01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-hgdpt                       1/1     Running   0          156m   192.168.216.142   k8s-worker02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-k8s-master01            1/1     Running   0          161m   192.168.216.140   k8s-master01   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>



<h3 id="五、部署-Nginx-应用验证-K8S-集群可用性"><a href="#五、部署-Nginx-应用验证-K8S-集群可用性" class="headerlink" title="五、部署 Nginx 应用验证 K8S 集群可用性"></a>五、部署 Nginx 应用验证 K8S 集群可用性</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># vim nginx.yaml</span></span><br><span class="line">root@k8s-master01:~<span class="comment"># cat nginx.yaml</span></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginxweb</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginxweb1</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginxweb1</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginxwebc</span><br><span class="line">          image: nginx:latest</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          ports:</span><br><span class="line">            - containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginxweb-service</span><br><span class="line">spec:</span><br><span class="line">  externalTrafficPolicy: Cluster</span><br><span class="line">  selector:</span><br><span class="line">    app: nginxweb1</span><br><span class="line">  ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">      nodePort: 30080</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># kubectl apply -f nginx.yaml</span></span><br><span class="line">deployment.apps/nginxweb created</span><br><span class="line">service/nginxweb-service created</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@k8s-master01:~<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginxweb-55dcdbb446-j2vsv   1/1     Running   0          77s</span><br><span class="line">nginxweb-55dcdbb446-xz6pl   1/1     Running   0          77s</span><br><span class="line"></span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl get deployment</span></span><br><span class="line">NAME       READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginxweb   2/2     2            2           95s</span><br><span class="line"></span><br><span class="line">root@k8s-master01:~<span class="comment"># kubectl get svc</span></span><br><span class="line">NAME               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes         ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP        157m</span><br><span class="line">nginxweb-service   NodePort    10.99.63.84   &lt;none&gt;        80:30080/TCP   2m13s</span><br><span class="line"></span><br><span class="line">k8s主机访问成功</span><br><span class="line">root@k8s-master01:~<span class="comment"># curl http://10.99.63.84</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;<span class="built_in">head</span>&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">html &#123; color-scheme: light dark; &#125;</span><br><span class="line">body &#123; width: 35em; margin: 0 auto;</span><br><span class="line">font-family: Tahoma, Verdana, Arial, sans-serif; &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=<span class="string">&quot;http://nginx.org/&quot;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=<span class="string">&quot;http://nginx.com/&quot;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you <span class="keyword">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/655061ae/image-20240509025712344.png" alt="image-20240509025712344"></p>
]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>输出 n 行空心菱形</title>
    <url>/4b76df2e/</url>
    <content><![CDATA[<h2 id="输入奇数-n，输出-n-行空心菱形"><a href="#输入奇数-n，输出-n-行空心菱形" class="headerlink" title="输入奇数 n，输出 n 行空心菱形"></a>输入奇数 n，输出 n 行空心菱形</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入奇数 n，输出 n 行空心菱形</span></span><br><span class="line"><span class="comment"> * &lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> n</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">printDiamond</span><span class="params">(<span class="type">int</span> n)</span> &#123;</span><br><span class="line">    <span class="comment">// x,y 坐标</span></span><br><span class="line">    <span class="type">int</span> x, y;</span><br><span class="line">    n = n / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">for</span> (x = -n; x &lt;= n; x++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (y = -n; y &lt;= n; y++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (Math.abs(x) + Math.abs(y) == n) &#123;</span><br><span class="line">                System.out.print(<span class="string">&quot;*&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.print(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.print(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>原神抽卡 Java 实现</title>
    <url>/877ff289/</url>
    <content><![CDATA[<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Random</span> <span class="variable">r</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">        <span class="type">Scanner</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">        <span class="type">int</span> <span class="variable">a</span> <span class="operator">=</span> <span class="number">0</span>, i = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">jin</span> <span class="operator">=</span> <span class="number">0</span>, zi = <span class="number">0</span>, P = <span class="number">0</span>, L = <span class="number">0</span>, jinCount = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">ys</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span>[] logs = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">200</span>];</span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;原石：&quot;</span> + ys);</span><br><span class="line">            System.out.println(<span class="string">&quot;请选择&quot;</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;A-单抽&quot;</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;B-十连&quot;</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;C-氪648&quot;</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;D-查询抽卡记录&quot;</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;其它-退出&quot;</span>);</span><br><span class="line">            <span class="type">char</span> <span class="variable">c</span> <span class="operator">=</span> sc.next().charAt(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">switch</span> (c) &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">&#x27;A&#x27;</span>:</span><br><span class="line">                    <span class="keyword">if</span> (ys &gt;= <span class="number">160</span>) &#123;</span><br><span class="line">                        a = <span class="number">1</span>;</span><br><span class="line">                        ys -= <span class="number">160</span>;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        a = -<span class="number">2</span>;</span><br><span class="line">                        System.out.println(<span class="string">&quot;\33[91;1m原石不足，请充值!\33[0m&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">&#x27;B&#x27;</span>:</span><br><span class="line">                    <span class="keyword">if</span> (ys &gt;= <span class="number">1600</span>) &#123;</span><br><span class="line">                        a = <span class="number">10</span>;</span><br><span class="line">                        ys -= <span class="number">1600</span>;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        a = -<span class="number">2</span>;</span><br><span class="line">                        System.out.println(<span class="string">&quot;\33[91;1m原石不足，请充值!\33[0m&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">&#x27;C&#x27;</span>:</span><br><span class="line">                    a = -<span class="number">2</span>;</span><br><span class="line">                    ys += <span class="number">6480</span>;</span><br><span class="line">                    System.out.println(<span class="string">&quot;\33[92;1m充值成功！\33[0m&quot;</span>);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">&#x27;D&#x27;</span>:</span><br><span class="line">                    a = -<span class="number">2</span>;</span><br><span class="line">                    <span class="keyword">if</span> (logs[<span class="number">0</span>] != <span class="number">0</span>) &#123;</span><br><span class="line">                        System.out.print(<span class="string">&quot;\33[96;1m抽卡记录：\33[0m&quot;</span>);</span><br><span class="line">                        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; L; i++) &#123;</span><br><span class="line">                            sum += logs[i];</span><br><span class="line">                            System.out.print(logs[i] + <span class="string">&quot; &quot;</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                        sum = sum / L;</span><br><span class="line">                        System.out.println(<span class="string">&quot;&quot;</span>);</span><br><span class="line">                        System.out.println(<span class="string">&quot;\33[96;1m平均出金：\33[0m&quot;</span> + sum);</span><br><span class="line">                        ou(sum);</span><br><span class="line">                    &#125; <span class="keyword">else</span></span><br><span class="line">                        System.out.println(<span class="string">&quot;\33[37;1m暂未出金\33[0m&quot;</span>);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">default</span>:</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (a == -<span class="number">2</span>) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; a; i++) &#123;</span><br><span class="line">                <span class="type">int</span> <span class="variable">num1</span> <span class="operator">=</span> r.nextInt(<span class="number">1000</span>) + <span class="number">1</span>;</span><br><span class="line">                <span class="type">int</span> <span class="variable">num2</span> <span class="operator">=</span> r.nextInt(<span class="number">1000</span>) + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span> (jin &gt;= <span class="number">89</span> || (num1 &gt; <span class="number">0</span> &amp;&amp; num1 &lt;= <span class="number">6</span> &amp;&amp; jin &lt;= <span class="number">72</span>) || ((num1 &gt; <span class="number">0</span> &amp;&amp; num1 &lt;= <span class="number">6</span> + (jin - <span class="number">72</span>) * <span class="number">60</span>) &amp;&amp; jin &gt;= <span class="number">73</span>)) &#123;</span><br><span class="line">                    System.out.print(<span class="string">&quot;\33[93;1m金\33[0m &quot;</span>);</span><br><span class="line">                    &#123;</span><br><span class="line">                        P = jin + <span class="number">1</span>;</span><br><span class="line">                        logs[L] = P;</span><br><span class="line">                        L++;</span><br><span class="line">                        jinCount++;</span><br><span class="line">                        jin = <span class="number">0</span>;</span><br><span class="line">                        zi++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((zi &gt;= <span class="number">9</span> || (num2 &gt;= <span class="number">1</span> &amp;&amp; num2 &lt;= <span class="number">51</span> &amp;&amp; zi &lt;= <span class="number">7</span>) || (num2 &gt;= <span class="number">1</span> &amp;&amp; num2 &lt;= <span class="number">561</span> &amp;&amp; zi == <span class="number">8</span>))) &#123;</span><br><span class="line">                    System.out.print(<span class="string">&quot;\33[95;1m紫\33[0m &quot;</span>);</span><br><span class="line">                    zi = <span class="number">0</span>;</span><br><span class="line">                    jin++;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    System.out.print(<span class="string">&quot;\33[94;1m蓝\33[0m &quot;</span>);</span><br><span class="line">                    jin++;</span><br><span class="line">                    zi++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(jin + <span class="string">&quot;\33[37;1m抽未出金\33[0m&quot;</span>);</span><br><span class="line">            <span class="keyword">if</span> (P != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (jinCount == <span class="number">1</span>)</span><br><span class="line">                    System.out.println(<span class="string">&quot;\33[96;1m第\33[0m&quot;</span> + P + <span class="string">&quot;\33[96;1m抽出金\33[0m&quot;</span>);</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (jinCount &gt;= <span class="number">1</span>)</span><br><span class="line">                    System.out.println(jinCount + <span class="string">&quot;\33[93;1m连金\33[0m&quot;</span>);</span><br><span class="line">                jinCount = <span class="number">0</span>;</span><br><span class="line">                ou(P);</span><br><span class="line">                P = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">ou</span><span class="params">(<span class="type">int</span> a)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (a &lt;= <span class="number">10</span>)</span><br><span class="line">            System.out.println(<span class="string">&quot;\33[93;1m终极无敌至尊欧皇！！\33[0m&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (a &lt;= <span class="number">30</span>)</span><br><span class="line">            System.out.println(<span class="string">&quot;\33[93;1m大欧皇！\33[0m&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (a &lt;= <span class="number">50</span>)</span><br><span class="line">            System.out.println(<span class="string">&quot;\33[95;1m欧皇\33[0m&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (a &lt;= <span class="number">70</span>)</span><br><span class="line">            System.out.println(<span class="string">&quot;\33[94;1m欧非守恒\33[0m&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (a &lt;= <span class="number">80</span>)</span><br><span class="line">            System.out.println(<span class="string">&quot;\33[91;1m非酋\33[0m&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (a &lt;= <span class="number">85</span>)</span><br><span class="line">            System.out.println(<span class="string">&quot;\33[91;1m大非酋！\33[0m&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (a &lt;= <span class="number">90</span>)</span><br><span class="line">            System.out.println(<span class="string">&quot;\33[91;1m终极无敌至尊非酋王！！\33[0m&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>原神</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 的安装和使用</title>
    <url>/fb9a69c2/</url>
    <content><![CDATA[<h3 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h3><h4 id="运行以下命令以卸载所有冲突的软件包"><a href="#运行以下命令以卸载所有冲突的软件包" class="headerlink" title="运行以下命令以卸载所有冲突的软件包"></a>运行以下命令以卸载所有冲突的软件包</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> pkg <span class="keyword">in</span> docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; <span class="keyword">do</span> sudo apt-get remove <span class="variable">$pkg</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h4 id="使用-Apt-存储库链接进行安装"><a href="#使用-Apt-存储库链接进行安装" class="headerlink" title="使用 Apt 存储库链接进行安装"></a>使用 Apt 存储库链接进行安装</h4><ol>
<li><p>设置 Docker 的 Apt 存储库。</p>
<div class="tabs" id="t1"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="t1-1">安装最新版本（阿里云）</button><button type="button" class="tab " data-href="t1-2">安装最新版本（官方）</button></ul><div class="tab-contents"><div class="tab-item-content active" id="t1-1"><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Add Docker&#x27;s official GPG key:</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install ca-certificates curl</span><br><span class="line">sudo install -m 0755 -d /etc/apt/keyrings</span><br><span class="line">sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc</span><br><span class="line">sudo <span class="built_in">chmod</span> a+r /etc/apt/keyrings/docker.asc</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the repository to Apt sources:</span></span><br><span class="line"><span class="built_in">echo</span> \</span><br><span class="line">  <span class="string">&quot;deb [arch=<span class="subst">$(dpkg --print-architecture)</span> signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \</span></span><br><span class="line"><span class="string">  <span class="subst">$(. /etc/os-release &amp;&amp; echo <span class="string">&quot;<span class="variable">$VERSION_CODENAME</span>&quot;</span>)</span> stable&quot;</span> | \</span><br><span class="line">  sudo <span class="built_in">tee</span> /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></div><div class="tab-item-content" id="t1-2"><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Add Docker&#x27;s official GPG key:</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install ca-certificates curl</span><br><span class="line">sudo install -m 0755 -d /etc/apt/keyrings</span><br><span class="line">sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc</span><br><span class="line">sudo <span class="built_in">chmod</span> a+r /etc/apt/keyrings/docker.asc</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the repository to Apt sources:</span></span><br><span class="line"><span class="built_in">echo</span> \</span><br><span class="line">  <span class="string">&quot;deb [arch=<span class="subst">$(dpkg --print-architecture)</span> signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string">  <span class="subst">$(. /etc/os-release &amp;&amp; echo <span class="string">&quot;<span class="variable">$VERSION_CODENAME</span>&quot;</span>)</span> stable&quot;</span> | \</span><br><span class="line">  sudo <span class="built_in">tee</span> /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div>
</li>
<li><p>安装 Docker 软件包。</p>
<div class="tabs" id="t2"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="t2-1">安装最新版本</button><button type="button" class="tab " data-href="t2-2">安装特定版本</button></ul><div class="tab-contents"><div class="tab-item-content active" id="t2-1"><p>要安装最新版本，请运行：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</span><br></pre></td></tr></table></figure></div><div class="tab-item-content" id="t2-2"><ol>
<li><p>要安装特定版本的 Docker 引擎，请首先列出存储库中的可用版本：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># List the available versions:</span></span><br><span class="line">apt-cache madison docker-ce | awk <span class="string">&#x27;&#123; print $3 &#125;&#x27;</span></span><br><span class="line"></span><br><span class="line">5:24.0.0-1~ubuntu.22.04~jammy</span><br><span class="line">5:23.0.6-1~ubuntu.22.04~jammy</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
<li><p>选择所需的版本并安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">VERSION_STRING=5:24.0.0-1~ubuntu.22.04~jammy</span><br><span class="line">sudo apt-get install docker-ce=<span class="variable">$VERSION_STRING</span> docker-ce-cli=<span class="variable">$VERSION_STRING</span> containerd.io docker-buildx-plugin docker-compose-plugin</span><br></pre></td></tr></table></figure></li>
</ol></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div>
</li>
<li><p>通过运行 <code>hello-world</code> 映像验证 Docker 引擎安装是否成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run hello-world</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20230911114932788.png" alt="image-20230911114932788"></p>
</li>
</ol>
<h4 id="设置代理加速"><a href="#设置代理加速" class="headerlink" title="设置代理加速"></a>设置代理加速</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">mkdir</span> -p /etc/docker</span><br><span class="line">sudo <span class="built_in">tee</span> /etc/docker/daemon.json &lt;&lt;-<span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;builder&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;gc&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;defaultKeepStorage&quot;</span>: <span class="string">&quot;20GB&quot;</span>,</span><br><span class="line">      <span class="string">&quot;enabled&quot;</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;experimental&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="string">&quot;registry-mirrors&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;https://docker.1panel.live&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://hub.rat.dev&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://mirror.ccs.tencentyun.com&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.rainbond.cc&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.1ms.run&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.registry.cyou&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker-cf.registry.cyou&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://dockercf.jsdelivr.fyi&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.jsdelivr.fyi&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://dockertest.jsdelivr.fyi&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://mirror.aliyuncs.com&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://dockerproxy.com&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://mirror.baidubce.com&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.m.daocloud.io&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.nju.edu.cn&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.mirrors.ustc.edu.cn&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://mirror.iscas.ac.cn&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.rainbond.cc&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.hpcloud.cloud&quot;</span>,</span><br><span class="line">    <span class="string">&quot;http://mirrors.ustc.edu.cn&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.chenby.cn&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://docker.ckyl.me&quot;</span>,</span><br><span class="line">    <span class="string">&quot;http://mirror.azure.cn&quot;</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>

<h3 id="安装-Docker-Compose"><a href="#安装-Docker-Compose" class="headerlink" title="安装 Docker Compose"></a>安装 Docker Compose</h3><ol>
<li><p>下载</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -SL https://github.com/docker/compose/releases/download/v2.30.3/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置软链接</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">ln</span> -s /usr/local/bin/docker-compose /usr/bin/docker-compose</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看 Docker Compose 版本号</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker-compose --version</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20241123060438141.png" alt="image-20241123060438141"></p>
</li>
<li><p>Docker Compose 常用命令</p>
<ul>
<li><p>启动容器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker-compose up -d</span><br></pre></td></tr></table></figure>
</li>
<li><p>停止容器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker-compose down</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="Docker-网络"><a href="#Docker-网络" class="headerlink" title="Docker 网络"></a>Docker 网络</h3><ol>
<li><p>帮助</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker network --<span class="built_in">help</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看网络</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker network <span class="built_in">ls</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看网络源数据</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker network inspect &lt;网络名字&gt;</span><br><span class="line"><span class="comment"># 只显示后 20 行</span></span><br><span class="line">docker network inspect &lt;网络名字&gt; | <span class="built_in">tail</span> -n 20</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建网络</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker network create &lt;网络名字&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除网络</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker network <span class="built_in">rm</span> &lt;网络名字&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="总体介绍"><a href="#总体介绍" class="headerlink" title="总体介绍"></a>总体介绍</h4><table>
<thead>
<tr>
<th>模式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bridge</td>
<td>为每一个容器分配、设置 IP 等，并将容器连接到一个 <code>docker0</code></td>
</tr>
<tr>
<td>host</td>
<td>容器将不会虚拟出自己的网卡、配置自己的 IP 等，而是使用宿主机的 IP 和 端口。</td>
</tr>
<tr>
<td>none</td>
<td>容器有独立的 Network Namespace，但并没有对其进行任何网络设置，如分配 veth pair 和 网桥连接、IP 等。</td>
</tr>
<tr>
<td>container</td>
<td>新创建的容器不会创建自己的网卡和配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。</td>
</tr>
</tbody></table>
<ul>
<li>bridge 模式：使用 <code>--network bridge</code> 指定，默认使用 docker0</li>
<li>host 模式：使用 <code>--network host</code> 指定</li>
<li>none 模式：使用 <code>--network none</code> 指定</li>
<li>container 模式：使用 <code>--network container:NAME</code> 或者 容器 ID 指定</li>
</ul>
<h3 id="安装-Redis"><a href="#安装-Redis" class="headerlink" title="安装 Redis"></a>安装 Redis</h3><h4 id="简单版"><a href="#简单版" class="headerlink" title="简单版"></a>简单版</h4><p>命令提醒：容器卷记得加入 <code>--privileged=true</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -d -p 6379:6379 redis:6.0.8</span><br><span class="line">docker <span class="built_in">exec</span> -it f216c38ea035 /bin/bash</span><br><span class="line">redis-cli</span><br></pre></td></tr></table></figure>

<h4 id="实战版"><a href="#实战版" class="headerlink" title="实战版"></a>实战版</h4><ol>
<li><p>在宿主机下新建目录 <code>/app/redis</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /app/redis</span><br></pre></td></tr></table></figure>
</li>
<li><p>将一个 <code>redis.conf</code> 文件模板拷贝进 <code>/app/redis</code> 目录下</p>
<ol>
<li><p>创建 <code>redis.conf</code> 文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">touch</span> redis.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>将 redis 7.2.1 的 redis.conf 保存到 &#x2F;app&#x2F;redis&#x2F;redis_7.2.1.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Redis configuration file example.</span><br><span class="line">#</span><br><span class="line"># Note that in order to read the configuration file, Redis must be</span><br><span class="line"># started with the file path as first argument:</span><br><span class="line">#</span><br><span class="line"># ./redis-server /path/to/redis.conf</span><br><span class="line"></span><br><span class="line"># Note on units: when memory size is needed, it is possible to specify</span><br><span class="line"># it in the usual form of 1k 5GB 4M and so forth:</span><br><span class="line">#</span><br><span class="line"># 1k =&gt; 1000 bytes</span><br><span class="line"># 1kb =&gt; 1024 bytes</span><br><span class="line"># 1m =&gt; 1000000 bytes</span><br><span class="line"># 1mb =&gt; 1024*1024 bytes</span><br><span class="line"># 1g =&gt; 1000000000 bytes</span><br><span class="line"># 1gb =&gt; 1024*1024*1024 bytes</span><br><span class="line">#</span><br><span class="line"># units are case insensitive so 1GB 1Gb 1gB are all the same.</span><br><span class="line"></span><br><span class="line">################################## INCLUDES ###################################</span><br><span class="line"></span><br><span class="line"># Include one or more other config files here.  This is useful if you</span><br><span class="line"># have a standard template that goes to all Redis servers but also need</span><br><span class="line"># to customize a few per-server settings.  Include files can include</span><br><span class="line"># other files, so use this wisely.</span><br><span class="line">#</span><br><span class="line"># Note that option &quot;include&quot; won&#x27;t be rewritten by command &quot;CONFIG REWRITE&quot;</span><br><span class="line"># from admin or Redis Sentinel. Since Redis always uses the last processed</span><br><span class="line"># line as value of a configuration directive, you&#x27;d better put includes</span><br><span class="line"># at the beginning of this file to avoid overwriting config change at runtime.</span><br><span class="line">#</span><br><span class="line"># If instead you are interested in using includes to override configuration</span><br><span class="line"># options, it is better to use include as the last line.</span><br><span class="line">#</span><br><span class="line"># Included paths may contain wildcards. All files matching the wildcards will</span><br><span class="line"># be included in alphabetical order.</span><br><span class="line"># Note that if an include path contains a wildcards but no files match it when</span><br><span class="line"># the server is started, the include statement will be ignored and no error will</span><br><span class="line"># be emitted.  It is safe, therefore, to include wildcard files from empty</span><br><span class="line"># directories.</span><br><span class="line">#</span><br><span class="line"># include /path/to/local.conf</span><br><span class="line"># include /path/to/other.conf</span><br><span class="line"># include /path/to/fragments/*.conf</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">################################## MODULES #####################################</span><br><span class="line"></span><br><span class="line"># Load modules at startup. If the server is not able to load modules</span><br><span class="line"># it will abort. It is possible to use multiple loadmodule directives.</span><br><span class="line">#</span><br><span class="line"># loadmodule /path/to/my_module.so</span><br><span class="line"># loadmodule /path/to/other_module.so</span><br><span class="line"></span><br><span class="line">################################## NETWORK #####################################</span><br><span class="line"></span><br><span class="line"># By default, if no &quot;bind&quot; configuration directive is specified, Redis listens</span><br><span class="line"># for connections from all available network interfaces on the host machine.</span><br><span class="line"># It is possible to listen to just one or multiple selected interfaces using</span><br><span class="line"># the &quot;bind&quot; configuration directive, followed by one or more IP addresses.</span><br><span class="line"># Each address can be prefixed by &quot;-&quot;, which means that redis will not fail to</span><br><span class="line"># start if the address is not available. Being not available only refers to</span><br><span class="line"># addresses that does not correspond to any network interface. Addresses that</span><br><span class="line"># are already in use will always fail, and unsupported protocols will always BE</span><br><span class="line"># silently skipped.</span><br><span class="line">#</span><br><span class="line"># Examples:</span><br><span class="line">#</span><br><span class="line"># bind 192.168.1.100 10.0.0.1     # listens on two specific IPv4 addresses</span><br><span class="line"># bind 127.0.0.1 ::1              # listens on loopback IPv4 and IPv6</span><br><span class="line"># bind * -::*                     # like the default, all available interfaces</span><br><span class="line">#</span><br><span class="line"># ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the</span><br><span class="line"># internet, binding to all the interfaces is dangerous and will expose the</span><br><span class="line"># instance to everybody on the internet. So by default we uncomment the</span><br><span class="line"># following bind directive, that will force Redis to listen only on the</span><br><span class="line"># IPv4 and IPv6 (if available) loopback interface addresses (this means Redis</span><br><span class="line"># will only be able to accept client connections from the same host that it is</span><br><span class="line"># running on).</span><br><span class="line">#</span><br><span class="line"># IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES</span><br><span class="line"># COMMENT OUT THE FOLLOWING LINE.</span><br><span class="line">#</span><br><span class="line"># You will also need to set a password unless you explicitly disable protected</span><br><span class="line"># mode.</span><br><span class="line"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">bind 127.0.0.1 -::1</span><br><span class="line"></span><br><span class="line"># By default, outgoing connections (from replica to master, from Sentinel to</span><br><span class="line"># instances, cluster bus, etc.) are not bound to a specific local address. In</span><br><span class="line"># most cases, this means the operating system will handle that based on routing</span><br><span class="line"># and the interface through which the connection goes out.</span><br><span class="line">#</span><br><span class="line"># Using bind-source-addr it is possible to configure a specific address to bind</span><br><span class="line"># to, which may also affect how the connection gets routed.</span><br><span class="line">#</span><br><span class="line"># Example:</span><br><span class="line">#</span><br><span class="line"># bind-source-addr 10.0.0.1</span><br><span class="line"></span><br><span class="line"># Protected mode is a layer of security protection, in order to avoid that</span><br><span class="line"># Redis instances left open on the internet are accessed and exploited.</span><br><span class="line">#</span><br><span class="line"># When protected mode is on and the default user has no password, the server</span><br><span class="line"># only accepts local connections from the IPv4 address (127.0.0.1), IPv6 address</span><br><span class="line"># (::1) or Unix domain sockets.</span><br><span class="line">#</span><br><span class="line"># By default protected mode is enabled. You should disable it only if</span><br><span class="line"># you are sure you want clients from other hosts to connect to Redis</span><br><span class="line"># even if no authentication is configured.</span><br><span class="line">protected-mode yes</span><br><span class="line"></span><br><span class="line"># Redis uses default hardened security configuration directives to reduce the</span><br><span class="line"># attack surface on innocent users. Therefore, several sensitive configuration</span><br><span class="line"># directives are immutable, and some potentially-dangerous commands are blocked.</span><br><span class="line">#</span><br><span class="line"># Configuration directives that control files that Redis writes to (e.g., &#x27;dir&#x27;</span><br><span class="line"># and &#x27;dbfilename&#x27;) and that aren&#x27;t usually modified during runtime</span><br><span class="line"># are protected by making them immutable.</span><br><span class="line">#</span><br><span class="line"># Commands that can increase the attack surface of Redis and that aren&#x27;t usually</span><br><span class="line"># called by users are blocked by default.</span><br><span class="line">#</span><br><span class="line"># These can be exposed to either all connections or just local ones by setting</span><br><span class="line"># each of the configs listed below to either of these values:</span><br><span class="line">#</span><br><span class="line"># no    - Block for any connection (remain immutable)</span><br><span class="line"># yes   - Allow for any connection (no protection)</span><br><span class="line"># local - Allow only for local connections. Ones originating from the</span><br><span class="line">#         IPv4 address (127.0.0.1), IPv6 address (::1) or Unix domain sockets.</span><br><span class="line">#</span><br><span class="line"># enable-protected-configs no</span><br><span class="line"># enable-debug-command no</span><br><span class="line"># enable-module-command no</span><br><span class="line"></span><br><span class="line"># Accept connections on the specified port, default is 6379 (IANA #815344).</span><br><span class="line"># If port 0 is specified Redis will not listen on a TCP socket.</span><br><span class="line">port 6379</span><br><span class="line"></span><br><span class="line"># TCP listen() backlog.</span><br><span class="line">#</span><br><span class="line"># In high requests-per-second environments you need a high backlog in order</span><br><span class="line"># to avoid slow clients connection issues. Note that the Linux kernel</span><br><span class="line"># will silently truncate it to the value of /proc/sys/net/core/somaxconn so</span><br><span class="line"># make sure to raise both the value of somaxconn and tcp_max_syn_backlog</span><br><span class="line"># in order to get the desired effect.</span><br><span class="line">tcp-backlog 511</span><br><span class="line"></span><br><span class="line"># Unix socket.</span><br><span class="line">#</span><br><span class="line"># Specify the path for the Unix socket that will be used to listen for</span><br><span class="line"># incoming connections. There is no default, so Redis will not listen</span><br><span class="line"># on a unix socket when not specified.</span><br><span class="line">#</span><br><span class="line"># unixsocket /run/redis.sock</span><br><span class="line"># unixsocketperm 700</span><br><span class="line"></span><br><span class="line"># Close the connection after a client is idle for N seconds (0 to disable)</span><br><span class="line">timeout 0</span><br><span class="line"></span><br><span class="line"># TCP keepalive.</span><br><span class="line">#</span><br><span class="line"># If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence</span><br><span class="line"># of communication. This is useful for two reasons:</span><br><span class="line">#</span><br><span class="line"># 1) Detect dead peers.</span><br><span class="line"># 2) Force network equipment in the middle to consider the connection to be</span><br><span class="line">#    alive.</span><br><span class="line">#</span><br><span class="line"># On Linux, the specified value (in seconds) is the period used to send ACKs.</span><br><span class="line"># Note that to close the connection the double of the time is needed.</span><br><span class="line"># On other kernels the period depends on the kernel configuration.</span><br><span class="line">#</span><br><span class="line"># A reasonable value for this option is 300 seconds, which is the new</span><br><span class="line"># Redis default starting with Redis 3.2.1.</span><br><span class="line">tcp-keepalive 300</span><br><span class="line"></span><br><span class="line"># Apply OS-specific mechanism to mark the listening socket with the specified</span><br><span class="line"># ID, to support advanced routing and filtering capabilities.</span><br><span class="line">#</span><br><span class="line"># On Linux, the ID represents a connection mark.</span><br><span class="line"># On FreeBSD, the ID represents a socket cookie ID.</span><br><span class="line"># On OpenBSD, the ID represents a route table ID.</span><br><span class="line">#</span><br><span class="line"># The default value is 0, which implies no marking is required.</span><br><span class="line"># socket-mark-id 0</span><br><span class="line"></span><br><span class="line">################################# TLS/SSL #####################################</span><br><span class="line"></span><br><span class="line"># By default, TLS/SSL is disabled. To enable it, the &quot;tls-port&quot; configuration</span><br><span class="line"># directive can be used to define TLS-listening ports. To enable TLS on the</span><br><span class="line"># default port, use:</span><br><span class="line">#</span><br><span class="line"># port 0</span><br><span class="line"># tls-port 6379</span><br><span class="line"></span><br><span class="line"># Configure a X.509 certificate and private key to use for authenticating the</span><br><span class="line"># server to connected clients, masters or cluster peers.  These files should be</span><br><span class="line"># PEM formatted.</span><br><span class="line">#</span><br><span class="line"># tls-cert-file redis.crt</span><br><span class="line"># tls-key-file redis.key</span><br><span class="line">#</span><br><span class="line"># If the key file is encrypted using a passphrase, it can be included here</span><br><span class="line"># as well.</span><br><span class="line">#</span><br><span class="line"># tls-key-file-pass secret</span><br><span class="line"></span><br><span class="line"># Normally Redis uses the same certificate for both server functions (accepting</span><br><span class="line"># connections) and client functions (replicating from a master, establishing</span><br><span class="line"># cluster bus connections, etc.).</span><br><span class="line">#</span><br><span class="line"># Sometimes certificates are issued with attributes that designate them as</span><br><span class="line"># client-only or server-only certificates. In that case it may be desired to use</span><br><span class="line"># different certificates for incoming (server) and outgoing (client)</span><br><span class="line"># connections. To do that, use the following directives:</span><br><span class="line">#</span><br><span class="line"># tls-client-cert-file client.crt</span><br><span class="line"># tls-client-key-file client.key</span><br><span class="line">#</span><br><span class="line"># If the key file is encrypted using a passphrase, it can be included here</span><br><span class="line"># as well.</span><br><span class="line">#</span><br><span class="line"># tls-client-key-file-pass secret</span><br><span class="line"></span><br><span class="line"># Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange,</span><br><span class="line"># required by older versions of OpenSSL (&lt;3.0). Newer versions do not require</span><br><span class="line"># this configuration and recommend against it.</span><br><span class="line">#</span><br><span class="line"># tls-dh-params-file redis.dh</span><br><span class="line"></span><br><span class="line"># Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL</span><br><span class="line"># clients and peers.  Redis requires an explicit configuration of at least one</span><br><span class="line"># of these, and will not implicitly use the system wide configuration.</span><br><span class="line">#</span><br><span class="line"># tls-ca-cert-file ca.crt</span><br><span class="line"># tls-ca-cert-dir /etc/ssl/certs</span><br><span class="line"></span><br><span class="line"># By default, clients (including replica servers) on a TLS port are required</span><br><span class="line"># to authenticate using valid client side certificates.</span><br><span class="line">#</span><br><span class="line"># If &quot;no&quot; is specified, client certificates are not required and not accepted.</span><br><span class="line"># If &quot;optional&quot; is specified, client certificates are accepted and must be</span><br><span class="line"># valid if provided, but are not required.</span><br><span class="line">#</span><br><span class="line"># tls-auth-clients no</span><br><span class="line"># tls-auth-clients optional</span><br><span class="line"></span><br><span class="line"># By default, a Redis replica does not attempt to establish a TLS connection</span><br><span class="line"># with its master.</span><br><span class="line">#</span><br><span class="line"># Use the following directive to enable TLS on replication links.</span><br><span class="line">#</span><br><span class="line"># tls-replication yes</span><br><span class="line"></span><br><span class="line"># By default, the Redis Cluster bus uses a plain TCP connection. To enable</span><br><span class="line"># TLS for the bus protocol, use the following directive:</span><br><span class="line">#</span><br><span class="line"># tls-cluster yes</span><br><span class="line"></span><br><span class="line"># By default, only TLSv1.2 and TLSv1.3 are enabled and it is highly recommended</span><br><span class="line"># that older formally deprecated versions are kept disabled to reduce the attack surface.</span><br><span class="line"># You can explicitly specify TLS versions to support.</span><br><span class="line"># Allowed values are case insensitive and include &quot;TLSv1&quot;, &quot;TLSv1.1&quot;, &quot;TLSv1.2&quot;,</span><br><span class="line"># &quot;TLSv1.3&quot; (OpenSSL &gt;= 1.1.1) or any combination.</span><br><span class="line"># To enable only TLSv1.2 and TLSv1.3, use:</span><br><span class="line">#</span><br><span class="line"># tls-protocols &quot;TLSv1.2 TLSv1.3&quot;</span><br><span class="line"></span><br><span class="line"># Configure allowed ciphers.  See the ciphers(1ssl) manpage for more information</span><br><span class="line"># about the syntax of this string.</span><br><span class="line">#</span><br><span class="line"># Note: this configuration applies only to &lt;= TLSv1.2.</span><br><span class="line">#</span><br><span class="line"># tls-ciphers DEFAULT:!MEDIUM</span><br><span class="line"></span><br><span class="line"># Configure allowed TLSv1.3 ciphersuites.  See the ciphers(1ssl) manpage for more</span><br><span class="line"># information about the syntax of this string, and specifically for TLSv1.3</span><br><span class="line"># ciphersuites.</span><br><span class="line">#</span><br><span class="line"># tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256</span><br><span class="line"></span><br><span class="line"># When choosing a cipher, use the server&#x27;s preference instead of the client</span><br><span class="line"># preference. By default, the server follows the client&#x27;s preference.</span><br><span class="line">#</span><br><span class="line"># tls-prefer-server-ciphers yes</span><br><span class="line"></span><br><span class="line"># By default, TLS session caching is enabled to allow faster and less expensive</span><br><span class="line"># reconnections by clients that support it. Use the following directive to disable</span><br><span class="line"># caching.</span><br><span class="line">#</span><br><span class="line"># tls-session-caching no</span><br><span class="line"></span><br><span class="line"># Change the default number of TLS sessions cached. A zero value sets the cache</span><br><span class="line"># to unlimited size. The default size is 20480.</span><br><span class="line">#</span><br><span class="line"># tls-session-cache-size 5000</span><br><span class="line"></span><br><span class="line"># Change the default timeout of cached TLS sessions. The default timeout is 300</span><br><span class="line"># seconds.</span><br><span class="line">#</span><br><span class="line"># tls-session-cache-timeout 60</span><br><span class="line"></span><br><span class="line">################################# GENERAL #####################################</span><br><span class="line"></span><br><span class="line"># By default Redis does not run as a daemon. Use &#x27;yes&#x27; if you need it.</span><br><span class="line"># Note that Redis will write a pid file in /var/run/redis.pid when daemonized.</span><br><span class="line"># When Redis is supervised by upstart or systemd, this parameter has no impact.</span><br><span class="line">daemonize no</span><br><span class="line"></span><br><span class="line"># If you run Redis from upstart or systemd, Redis can interact with your</span><br><span class="line"># supervision tree. Options:</span><br><span class="line">#   supervised no      - no supervision interaction</span><br><span class="line">#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode</span><br><span class="line">#                        requires &quot;expect stop&quot; in your upstart job config</span><br><span class="line">#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET</span><br><span class="line">#                        on startup, and updating Redis status on a regular</span><br><span class="line">#                        basis.</span><br><span class="line">#   supervised auto    - detect upstart or systemd method based on</span><br><span class="line">#                        UPSTART_JOB or NOTIFY_SOCKET environment variables</span><br><span class="line"># Note: these supervision methods only signal &quot;process is ready.&quot;</span><br><span class="line">#       They do not enable continuous pings back to your supervisor.</span><br><span class="line">#</span><br><span class="line"># The default is &quot;no&quot;. To run under upstart/systemd, you can simply uncomment</span><br><span class="line"># the line below:</span><br><span class="line">#</span><br><span class="line"># supervised auto</span><br><span class="line"></span><br><span class="line"># If a pid file is specified, Redis writes it where specified at startup</span><br><span class="line"># and removes it at exit.</span><br><span class="line">#</span><br><span class="line"># When the server runs non daemonized, no pid file is created if none is</span><br><span class="line"># specified in the configuration. When the server is daemonized, the pid file</span><br><span class="line"># is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.</span><br><span class="line">#</span><br><span class="line"># Creating a pid file is best effort: if Redis is not able to create it</span><br><span class="line"># nothing bad happens, the server will start and run normally.</span><br><span class="line">#</span><br><span class="line"># Note that on modern Linux systems &quot;/run/redis.pid&quot; is more conforming</span><br><span class="line"># and should be used instead.</span><br><span class="line">pidfile /var/run/redis_6379.pid</span><br><span class="line"></span><br><span class="line"># Specify the server verbosity level.</span><br><span class="line"># This can be one of:</span><br><span class="line"># debug (a lot of information, useful for development/testing)</span><br><span class="line"># verbose (many rarely useful info, but not a mess like the debug level)</span><br><span class="line"># notice (moderately verbose, what you want in production probably)</span><br><span class="line"># warning (only very important / critical messages are logged)</span><br><span class="line"># nothing (nothing is logged)</span><br><span class="line">loglevel notice</span><br><span class="line"></span><br><span class="line"># Specify the log file name. Also the empty string can be used to force</span><br><span class="line"># Redis to log on the standard output. Note that if you use standard</span><br><span class="line"># output for logging but daemonize, logs will be sent to /dev/null</span><br><span class="line">logfile &quot;&quot;</span><br><span class="line"></span><br><span class="line"># To enable logging to the system logger, just set &#x27;syslog-enabled&#x27; to yes,</span><br><span class="line"># and optionally update the other syslog parameters to suit your needs.</span><br><span class="line"># syslog-enabled no</span><br><span class="line"></span><br><span class="line"># Specify the syslog identity.</span><br><span class="line"># syslog-ident redis</span><br><span class="line"></span><br><span class="line"># Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.</span><br><span class="line"># syslog-facility local0</span><br><span class="line"></span><br><span class="line"># To disable the built in crash log, which will possibly produce cleaner core</span><br><span class="line"># dumps when they are needed, uncomment the following:</span><br><span class="line">#</span><br><span class="line"># crash-log-enabled no</span><br><span class="line"></span><br><span class="line"># To disable the fast memory check that&#x27;s run as part of the crash log, which</span><br><span class="line"># will possibly let redis terminate sooner, uncomment the following:</span><br><span class="line">#</span><br><span class="line"># crash-memcheck-enabled no</span><br><span class="line"></span><br><span class="line"># Set the number of databases. The default database is DB 0, you can select</span><br><span class="line"># a different one on a per-connection basis using SELECT &lt;dbid&gt; where</span><br><span class="line"># dbid is a number between 0 and &#x27;databases&#x27;-1</span><br><span class="line">databases 16</span><br><span class="line"></span><br><span class="line"># By default Redis shows an ASCII art logo only when started to log to the</span><br><span class="line"># standard output and if the standard output is a TTY and syslog logging is</span><br><span class="line"># disabled. Basically this means that normally a logo is displayed only in</span><br><span class="line"># interactive sessions.</span><br><span class="line">#</span><br><span class="line"># However it is possible to force the pre-4.0 behavior and always show a</span><br><span class="line"># ASCII art logo in startup logs by setting the following option to yes.</span><br><span class="line">always-show-logo no</span><br><span class="line"></span><br><span class="line"># By default, Redis modifies the process title (as seen in &#x27;top&#x27; and &#x27;ps&#x27;) to</span><br><span class="line"># provide some runtime information. It is possible to disable this and leave</span><br><span class="line"># the process name as executed by setting the following to no.</span><br><span class="line">set-proc-title yes</span><br><span class="line"></span><br><span class="line"># When changing the process title, Redis uses the following template to construct</span><br><span class="line"># the modified title.</span><br><span class="line">#</span><br><span class="line"># Template variables are specified in curly brackets. The following variables are</span><br><span class="line"># supported:</span><br><span class="line">#</span><br><span class="line"># &#123;title&#125;           Name of process as executed if parent, or type of child process.</span><br><span class="line"># &#123;listen-addr&#125;     Bind address or &#x27;*&#x27; followed by TCP or TLS port listening on, or</span><br><span class="line">#                   Unix socket if only that&#x27;s available.</span><br><span class="line"># &#123;server-mode&#125;     Special mode, i.e. &quot;[sentinel]&quot; or &quot;[cluster]&quot;.</span><br><span class="line"># &#123;port&#125;            TCP port listening on, or 0.</span><br><span class="line"># &#123;tls-port&#125;        TLS port listening on, or 0.</span><br><span class="line"># &#123;unixsocket&#125;      Unix domain socket listening on, or &quot;&quot;.</span><br><span class="line"># &#123;config-file&#125;     Name of configuration file used.</span><br><span class="line">#</span><br><span class="line">proc-title-template &quot;&#123;title&#125; &#123;listen-addr&#125; &#123;server-mode&#125;&quot;</span><br><span class="line"></span><br><span class="line"># Set the local environment which is used for string comparison operations, and </span><br><span class="line"># also affect the performance of Lua scripts. Empty String indicates the locale </span><br><span class="line"># is derived from the environment variables.</span><br><span class="line">locale-collate &quot;&quot;</span><br><span class="line"></span><br><span class="line">################################ SNAPSHOTTING  ################################</span><br><span class="line"></span><br><span class="line"># Save the DB to disk.</span><br><span class="line">#</span><br><span class="line"># save &lt;seconds&gt; &lt;changes&gt; [&lt;seconds&gt; &lt;changes&gt; ...]</span><br><span class="line">#</span><br><span class="line"># Redis will save the DB if the given number of seconds elapsed and it</span><br><span class="line"># surpassed the given number of write operations against the DB.</span><br><span class="line">#</span><br><span class="line"># Snapshotting can be completely disabled with a single empty string argument</span><br><span class="line"># as in following example:</span><br><span class="line">#</span><br><span class="line"># save &quot;&quot;</span><br><span class="line">#</span><br><span class="line"># Unless specified otherwise, by default Redis will save the DB:</span><br><span class="line">#   * After 3600 seconds (an hour) if at least 1 change was performed</span><br><span class="line">#   * After 300 seconds (5 minutes) if at least 100 changes were performed</span><br><span class="line">#   * After 60 seconds if at least 10000 changes were performed</span><br><span class="line">#</span><br><span class="line"># You can set these explicitly by uncommenting the following line.</span><br><span class="line">#</span><br><span class="line"># save 3600 1 300 100 60 10000</span><br><span class="line"></span><br><span class="line"># By default Redis will stop accepting writes if RDB snapshots are enabled</span><br><span class="line"># (at least one save point) and the latest background save failed.</span><br><span class="line"># This will make the user aware (in a hard way) that data is not persisting</span><br><span class="line"># on disk properly, otherwise chances are that no one will notice and some</span><br><span class="line"># disaster will happen.</span><br><span class="line">#</span><br><span class="line"># If the background saving process will start working again Redis will</span><br><span class="line"># automatically allow writes again.</span><br><span class="line">#</span><br><span class="line"># However if you have setup your proper monitoring of the Redis server</span><br><span class="line"># and persistence, you may want to disable this feature so that Redis will</span><br><span class="line"># continue to work as usual even if there are problems with disk,</span><br><span class="line"># permissions, and so forth.</span><br><span class="line">stop-writes-on-bgsave-error yes</span><br><span class="line"></span><br><span class="line"># Compress string objects using LZF when dump .rdb databases?</span><br><span class="line"># By default compression is enabled as it&#x27;s almost always a win.</span><br><span class="line"># If you want to save some CPU in the saving child set it to &#x27;no&#x27; but</span><br><span class="line"># the dataset will likely be bigger if you have compressible values or keys.</span><br><span class="line">rdbcompression yes</span><br><span class="line"></span><br><span class="line"># Since version 5 of RDB a CRC64 checksum is placed at the end of the file.</span><br><span class="line"># This makes the format more resistant to corruption but there is a performance</span><br><span class="line"># hit to pay (around 10%) when saving and loading RDB files, so you can disable it</span><br><span class="line"># for maximum performances.</span><br><span class="line">#</span><br><span class="line"># RDB files created with checksum disabled have a checksum of zero that will</span><br><span class="line"># tell the loading code to skip the check.</span><br><span class="line">rdbchecksum yes</span><br><span class="line"></span><br><span class="line"># Enables or disables full sanitization checks for ziplist and listpack etc when</span><br><span class="line"># loading an RDB or RESTORE payload. This reduces the chances of a assertion or</span><br><span class="line"># crash later on while processing commands.</span><br><span class="line"># Options:</span><br><span class="line">#   no         - Never perform full sanitization</span><br><span class="line">#   yes        - Always perform full sanitization</span><br><span class="line">#   clients    - Perform full sanitization only for user connections.</span><br><span class="line">#                Excludes: RDB files, RESTORE commands received from the master</span><br><span class="line">#                connection, and client connections which have the</span><br><span class="line">#                skip-sanitize-payload ACL flag.</span><br><span class="line"># The default should be &#x27;clients&#x27; but since it currently affects cluster</span><br><span class="line"># resharding via MIGRATE, it is temporarily set to &#x27;no&#x27; by default.</span><br><span class="line">#</span><br><span class="line"># sanitize-dump-payload no</span><br><span class="line"></span><br><span class="line"># The filename where to dump the DB</span><br><span class="line">dbfilename dump.rdb</span><br><span class="line"></span><br><span class="line"># Remove RDB files used by replication in instances without persistence</span><br><span class="line"># enabled. By default this option is disabled, however there are environments</span><br><span class="line"># where for regulations or other security concerns, RDB files persisted on</span><br><span class="line"># disk by masters in order to feed replicas, or stored on disk by replicas</span><br><span class="line"># in order to load them for the initial synchronization, should be deleted</span><br><span class="line"># ASAP. Note that this option ONLY WORKS in instances that have both AOF</span><br><span class="line"># and RDB persistence disabled, otherwise is completely ignored.</span><br><span class="line">#</span><br><span class="line"># An alternative (and sometimes better) way to obtain the same effect is</span><br><span class="line"># to use diskless replication on both master and replicas instances. However</span><br><span class="line"># in the case of replicas, diskless is not always an option.</span><br><span class="line">rdb-del-sync-files no</span><br><span class="line"></span><br><span class="line"># The working directory.</span><br><span class="line">#</span><br><span class="line"># The DB will be written inside this directory, with the filename specified</span><br><span class="line"># above using the &#x27;dbfilename&#x27; configuration directive.</span><br><span class="line">#</span><br><span class="line"># The Append Only File will also be created inside this directory.</span><br><span class="line">#</span><br><span class="line"># Note that you must specify a directory here, not a file name.</span><br><span class="line">dir ./</span><br><span class="line"></span><br><span class="line">################################# REPLICATION #################################</span><br><span class="line"></span><br><span class="line"># Master-Replica replication. Use replicaof to make a Redis instance a copy of</span><br><span class="line"># another Redis server. A few things to understand ASAP about Redis replication.</span><br><span class="line">#</span><br><span class="line">#   +------------------+      +---------------+</span><br><span class="line">#   |      Master      | ---&gt; |    Replica    |</span><br><span class="line">#   | (receive writes) |      |  (exact copy) |</span><br><span class="line">#   +------------------+      +---------------+</span><br><span class="line">#</span><br><span class="line"># 1) Redis replication is asynchronous, but you can configure a master to</span><br><span class="line">#    stop accepting writes if it appears to be not connected with at least</span><br><span class="line">#    a given number of replicas.</span><br><span class="line"># 2) Redis replicas are able to perform a partial resynchronization with the</span><br><span class="line">#    master if the replication link is lost for a relatively small amount of</span><br><span class="line">#    time. You may want to configure the replication backlog size (see the next</span><br><span class="line">#    sections of this file) with a sensible value depending on your needs.</span><br><span class="line"># 3) Replication is automatic and does not need user intervention. After a</span><br><span class="line">#    network partition replicas automatically try to reconnect to masters</span><br><span class="line">#    and resynchronize with them.</span><br><span class="line">#</span><br><span class="line"># replicaof &lt;masterip&gt; &lt;masterport&gt;</span><br><span class="line"></span><br><span class="line"># If the master is password protected (using the &quot;requirepass&quot; configuration</span><br><span class="line"># directive below) it is possible to tell the replica to authenticate before</span><br><span class="line"># starting the replication synchronization process, otherwise the master will</span><br><span class="line"># refuse the replica request.</span><br><span class="line">#</span><br><span class="line"># masterauth &lt;master-password&gt;</span><br><span class="line">#</span><br><span class="line"># However this is not enough if you are using Redis ACLs (for Redis version</span><br><span class="line"># 6 or greater), and the default user is not capable of running the PSYNC</span><br><span class="line"># command and/or other commands needed for replication. In this case it&#x27;s</span><br><span class="line"># better to configure a special user to use with replication, and specify the</span><br><span class="line"># masteruser configuration as such:</span><br><span class="line">#</span><br><span class="line"># masteruser &lt;username&gt;</span><br><span class="line">#</span><br><span class="line"># When masteruser is specified, the replica will authenticate against its</span><br><span class="line"># master using the new AUTH form: AUTH &lt;username&gt; &lt;password&gt;.</span><br><span class="line"></span><br><span class="line"># When a replica loses its connection with the master, or when the replication</span><br><span class="line"># is still in progress, the replica can act in two different ways:</span><br><span class="line">#</span><br><span class="line"># 1) if replica-serve-stale-data is set to &#x27;yes&#x27; (the default) the replica will</span><br><span class="line">#    still reply to client requests, possibly with out of date data, or the</span><br><span class="line">#    data set may just be empty if this is the first synchronization.</span><br><span class="line">#</span><br><span class="line"># 2) If replica-serve-stale-data is set to &#x27;no&#x27; the replica will reply with error</span><br><span class="line">#    &quot;MASTERDOWN Link with MASTER is down and replica-serve-stale-data is set to &#x27;no&#x27;&quot;</span><br><span class="line">#    to all data access commands, excluding commands such as:</span><br><span class="line">#    INFO, REPLICAOF, AUTH, SHUTDOWN, REPLCONF, ROLE, CONFIG, SUBSCRIBE,</span><br><span class="line">#    UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, COMMAND, POST,</span><br><span class="line">#    HOST and LATENCY.</span><br><span class="line">#</span><br><span class="line">replica-serve-stale-data yes</span><br><span class="line"></span><br><span class="line"># You can configure a replica instance to accept writes or not. Writing against</span><br><span class="line"># a replica instance may be useful to store some ephemeral data (because data</span><br><span class="line"># written on a replica will be easily deleted after resync with the master) but</span><br><span class="line"># may also cause problems if clients are writing to it because of a</span><br><span class="line"># misconfiguration.</span><br><span class="line">#</span><br><span class="line"># Since Redis 2.6 by default replicas are read-only.</span><br><span class="line">#</span><br><span class="line"># Note: read only replicas are not designed to be exposed to untrusted clients</span><br><span class="line"># on the internet. It&#x27;s just a protection layer against misuse of the instance.</span><br><span class="line"># Still a read only replica exports by default all the administrative commands</span><br><span class="line"># such as CONFIG, DEBUG, and so forth. To a limited extent you can improve</span><br><span class="line"># security of read only replicas using &#x27;rename-command&#x27; to shadow all the</span><br><span class="line"># administrative / dangerous commands.</span><br><span class="line">replica-read-only yes</span><br><span class="line"></span><br><span class="line"># Replication SYNC strategy: disk or socket.</span><br><span class="line">#</span><br><span class="line"># New replicas and reconnecting replicas that are not able to continue the</span><br><span class="line"># replication process just receiving differences, need to do what is called a</span><br><span class="line"># &quot;full synchronization&quot;. An RDB file is transmitted from the master to the</span><br><span class="line"># replicas.</span><br><span class="line">#</span><br><span class="line"># The transmission can happen in two different ways:</span><br><span class="line">#</span><br><span class="line"># 1) Disk-backed: The Redis master creates a new process that writes the RDB</span><br><span class="line">#                 file on disk. Later the file is transferred by the parent</span><br><span class="line">#                 process to the replicas incrementally.</span><br><span class="line"># 2) Diskless: The Redis master creates a new process that directly writes the</span><br><span class="line">#              RDB file to replica sockets, without touching the disk at all.</span><br><span class="line">#</span><br><span class="line"># With disk-backed replication, while the RDB file is generated, more replicas</span><br><span class="line"># can be queued and served with the RDB file as soon as the current child</span><br><span class="line"># producing the RDB file finishes its work. With diskless replication instead</span><br><span class="line"># once the transfer starts, new replicas arriving will be queued and a new</span><br><span class="line"># transfer will start when the current one terminates.</span><br><span class="line">#</span><br><span class="line"># When diskless replication is used, the master waits a configurable amount of</span><br><span class="line"># time (in seconds) before starting the transfer in the hope that multiple</span><br><span class="line"># replicas will arrive and the transfer can be parallelized.</span><br><span class="line">#</span><br><span class="line"># With slow disks and fast (large bandwidth) networks, diskless replication</span><br><span class="line"># works better.</span><br><span class="line">repl-diskless-sync yes</span><br><span class="line"></span><br><span class="line"># When diskless replication is enabled, it is possible to configure the delay</span><br><span class="line"># the server waits in order to spawn the child that transfers the RDB via socket</span><br><span class="line"># to the replicas.</span><br><span class="line">#</span><br><span class="line"># This is important since once the transfer starts, it is not possible to serve</span><br><span class="line"># new replicas arriving, that will be queued for the next RDB transfer, so the</span><br><span class="line"># server waits a delay in order to let more replicas arrive.</span><br><span class="line">#</span><br><span class="line"># The delay is specified in seconds, and by default is 5 seconds. To disable</span><br><span class="line"># it entirely just set it to 0 seconds and the transfer will start ASAP.</span><br><span class="line">repl-diskless-sync-delay 5</span><br><span class="line"></span><br><span class="line"># When diskless replication is enabled with a delay, it is possible to let</span><br><span class="line"># the replication start before the maximum delay is reached if the maximum</span><br><span class="line"># number of replicas expected have connected. Default of 0 means that the</span><br><span class="line"># maximum is not defined and Redis will wait the full delay.</span><br><span class="line">repl-diskless-sync-max-replicas 0</span><br><span class="line"></span><br><span class="line"># -----------------------------------------------------------------------------</span><br><span class="line"># WARNING: Since in this setup the replica does not immediately store an RDB on</span><br><span class="line"># disk, it may cause data loss during failovers. RDB diskless load + Redis</span><br><span class="line"># modules not handling I/O reads may cause Redis to abort in case of I/O errors</span><br><span class="line"># during the initial synchronization stage with the master.</span><br><span class="line"># -----------------------------------------------------------------------------</span><br><span class="line">#</span><br><span class="line"># Replica can load the RDB it reads from the replication link directly from the</span><br><span class="line"># socket, or store the RDB to a file and read that file after it was completely</span><br><span class="line"># received from the master.</span><br><span class="line">#</span><br><span class="line"># In many cases the disk is slower than the network, and storing and loading</span><br><span class="line"># the RDB file may increase replication time (and even increase the master&#x27;s</span><br><span class="line"># Copy on Write memory and replica buffers).</span><br><span class="line"># However, when parsing the RDB file directly from the socket, in order to avoid</span><br><span class="line"># data loss it&#x27;s only safe to flush the current dataset when the new dataset is</span><br><span class="line"># fully loaded in memory, resulting in higher memory usage.</span><br><span class="line"># For this reason we have the following options:</span><br><span class="line">#</span><br><span class="line"># &quot;disabled&quot;    - Don&#x27;t use diskless load (store the rdb file to the disk first)</span><br><span class="line"># &quot;swapdb&quot;      - Keep current db contents in RAM while parsing the data directly</span><br><span class="line">#                 from the socket. Replicas in this mode can keep serving current</span><br><span class="line">#                 dataset while replication is in progress, except for cases where</span><br><span class="line">#                 they can&#x27;t recognize master as having a data set from same</span><br><span class="line">#                 replication history.</span><br><span class="line">#                 Note that this requires sufficient memory, if you don&#x27;t have it,</span><br><span class="line">#                 you risk an OOM kill.</span><br><span class="line"># &quot;on-empty-db&quot; - Use diskless load only when current dataset is empty. This is </span><br><span class="line">#                 safer and avoid having old and new dataset loaded side by side</span><br><span class="line">#                 during replication.</span><br><span class="line">repl-diskless-load disabled</span><br><span class="line"></span><br><span class="line"># Master send PINGs to its replicas in a predefined interval. It&#x27;s possible to</span><br><span class="line"># change this interval with the repl_ping_replica_period option. The default</span><br><span class="line"># value is 10 seconds.</span><br><span class="line">#</span><br><span class="line"># repl-ping-replica-period 10</span><br><span class="line"></span><br><span class="line"># The following option sets the replication timeout for:</span><br><span class="line">#</span><br><span class="line"># 1) Bulk transfer I/O during SYNC, from the point of view of replica.</span><br><span class="line"># 2) Master timeout from the point of view of replicas (data, pings).</span><br><span class="line"># 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).</span><br><span class="line">#</span><br><span class="line"># It is important to make sure that this value is greater than the value</span><br><span class="line"># specified for repl-ping-replica-period otherwise a timeout will be detected</span><br><span class="line"># every time there is low traffic between the master and the replica. The default</span><br><span class="line"># value is 60 seconds.</span><br><span class="line">#</span><br><span class="line"># repl-timeout 60</span><br><span class="line"></span><br><span class="line"># Disable TCP_NODELAY on the replica socket after SYNC?</span><br><span class="line">#</span><br><span class="line"># If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and</span><br><span class="line"># less bandwidth to send data to replicas. But this can add a delay for</span><br><span class="line"># the data to appear on the replica side, up to 40 milliseconds with</span><br><span class="line"># Linux kernels using a default configuration.</span><br><span class="line">#</span><br><span class="line"># If you select &quot;no&quot; the delay for data to appear on the replica side will</span><br><span class="line"># be reduced but more bandwidth will be used for replication.</span><br><span class="line">#</span><br><span class="line"># By default we optimize for low latency, but in very high traffic conditions</span><br><span class="line"># or when the master and replicas are many hops away, turning this to &quot;yes&quot; may</span><br><span class="line"># be a good idea.</span><br><span class="line">repl-disable-tcp-nodelay no</span><br><span class="line"></span><br><span class="line"># Set the replication backlog size. The backlog is a buffer that accumulates</span><br><span class="line"># replica data when replicas are disconnected for some time, so that when a</span><br><span class="line"># replica wants to reconnect again, often a full resync is not needed, but a</span><br><span class="line"># partial resync is enough, just passing the portion of data the replica</span><br><span class="line"># missed while disconnected.</span><br><span class="line">#</span><br><span class="line"># The bigger the replication backlog, the longer the replica can endure the</span><br><span class="line"># disconnect and later be able to perform a partial resynchronization.</span><br><span class="line">#</span><br><span class="line"># The backlog is only allocated if there is at least one replica connected.</span><br><span class="line">#</span><br><span class="line"># repl-backlog-size 1mb</span><br><span class="line"></span><br><span class="line"># After a master has no connected replicas for some time, the backlog will be</span><br><span class="line"># freed. The following option configures the amount of seconds that need to</span><br><span class="line"># elapse, starting from the time the last replica disconnected, for the backlog</span><br><span class="line"># buffer to be freed.</span><br><span class="line">#</span><br><span class="line"># Note that replicas never free the backlog for timeout, since they may be</span><br><span class="line"># promoted to masters later, and should be able to correctly &quot;partially</span><br><span class="line"># resynchronize&quot; with other replicas: hence they should always accumulate backlog.</span><br><span class="line">#</span><br><span class="line"># A value of 0 means to never release the backlog.</span><br><span class="line">#</span><br><span class="line"># repl-backlog-ttl 3600</span><br><span class="line"></span><br><span class="line"># The replica priority is an integer number published by Redis in the INFO</span><br><span class="line"># output. It is used by Redis Sentinel in order to select a replica to promote</span><br><span class="line"># into a master if the master is no longer working correctly.</span><br><span class="line">#</span><br><span class="line"># A replica with a low priority number is considered better for promotion, so</span><br><span class="line"># for instance if there are three replicas with priority 10, 100, 25 Sentinel</span><br><span class="line"># will pick the one with priority 10, that is the lowest.</span><br><span class="line">#</span><br><span class="line"># However a special priority of 0 marks the replica as not able to perform the</span><br><span class="line"># role of master, so a replica with priority of 0 will never be selected by</span><br><span class="line"># Redis Sentinel for promotion.</span><br><span class="line">#</span><br><span class="line"># By default the priority is 100.</span><br><span class="line">replica-priority 100</span><br><span class="line"></span><br><span class="line"># The propagation error behavior controls how Redis will behave when it is</span><br><span class="line"># unable to handle a command being processed in the replication stream from a master</span><br><span class="line"># or processed while reading from an AOF file. Errors that occur during propagation</span><br><span class="line"># are unexpected, and can cause data inconsistency. However, there are edge cases</span><br><span class="line"># in earlier versions of Redis where it was possible for the server to replicate or persist</span><br><span class="line"># commands that would fail on future versions. For this reason the default behavior</span><br><span class="line"># is to ignore such errors and continue processing commands.</span><br><span class="line">#</span><br><span class="line"># If an application wants to ensure there is no data divergence, this configuration</span><br><span class="line"># should be set to &#x27;panic&#x27; instead. The value can also be set to &#x27;panic-on-replicas&#x27;</span><br><span class="line"># to only panic when a replica encounters an error on the replication stream. One of</span><br><span class="line"># these two panic values will become the default value in the future once there are</span><br><span class="line"># sufficient safety mechanisms in place to prevent false positive crashes.</span><br><span class="line">#</span><br><span class="line"># propagation-error-behavior ignore</span><br><span class="line"></span><br><span class="line"># Replica ignore disk write errors controls the behavior of a replica when it is</span><br><span class="line"># unable to persist a write command received from its master to disk. By default,</span><br><span class="line"># this configuration is set to &#x27;no&#x27; and will crash the replica in this condition.</span><br><span class="line"># It is not recommended to change this default, however in order to be compatible</span><br><span class="line"># with older versions of Redis this config can be toggled to &#x27;yes&#x27; which will just</span><br><span class="line"># log a warning and execute the write command it got from the master.</span><br><span class="line">#</span><br><span class="line"># replica-ignore-disk-write-errors no</span><br><span class="line"></span><br><span class="line"># -----------------------------------------------------------------------------</span><br><span class="line"># By default, Redis Sentinel includes all replicas in its reports. A replica</span><br><span class="line"># can be excluded from Redis Sentinel&#x27;s announcements. An unannounced replica</span><br><span class="line"># will be ignored by the &#x27;sentinel replicas &lt;master&gt;&#x27; command and won&#x27;t be</span><br><span class="line"># exposed to Redis Sentinel&#x27;s clients.</span><br><span class="line">#</span><br><span class="line"># This option does not change the behavior of replica-priority. Even with</span><br><span class="line"># replica-announced set to &#x27;no&#x27;, the replica can be promoted to master. To</span><br><span class="line"># prevent this behavior, set replica-priority to 0.</span><br><span class="line">#</span><br><span class="line"># replica-announced yes</span><br><span class="line"></span><br><span class="line"># It is possible for a master to stop accepting writes if there are less than</span><br><span class="line"># N replicas connected, having a lag less or equal than M seconds.</span><br><span class="line">#</span><br><span class="line"># The N replicas need to be in &quot;online&quot; state.</span><br><span class="line">#</span><br><span class="line"># The lag in seconds, that must be &lt;= the specified value, is calculated from</span><br><span class="line"># the last ping received from the replica, that is usually sent every second.</span><br><span class="line">#</span><br><span class="line"># This option does not GUARANTEE that N replicas will accept the write, but</span><br><span class="line"># will limit the window of exposure for lost writes in case not enough replicas</span><br><span class="line"># are available, to the specified number of seconds.</span><br><span class="line">#</span><br><span class="line"># For example to require at least 3 replicas with a lag &lt;= 10 seconds use:</span><br><span class="line">#</span><br><span class="line"># min-replicas-to-write 3</span><br><span class="line"># min-replicas-max-lag 10</span><br><span class="line">#</span><br><span class="line"># Setting one or the other to 0 disables the feature.</span><br><span class="line">#</span><br><span class="line"># By default min-replicas-to-write is set to 0 (feature disabled) and</span><br><span class="line"># min-replicas-max-lag is set to 10.</span><br><span class="line"></span><br><span class="line"># A Redis master is able to list the address and port of the attached</span><br><span class="line"># replicas in different ways. For example the &quot;INFO replication&quot; section</span><br><span class="line"># offers this information, which is used, among other tools, by</span><br><span class="line"># Redis Sentinel in order to discover replica instances.</span><br><span class="line"># Another place where this info is available is in the output of the</span><br><span class="line"># &quot;ROLE&quot; command of a master.</span><br><span class="line">#</span><br><span class="line"># The listed IP address and port normally reported by a replica is</span><br><span class="line"># obtained in the following way:</span><br><span class="line">#</span><br><span class="line">#   IP: The address is auto detected by checking the peer address</span><br><span class="line">#   of the socket used by the replica to connect with the master.</span><br><span class="line">#</span><br><span class="line">#   Port: The port is communicated by the replica during the replication</span><br><span class="line">#   handshake, and is normally the port that the replica is using to</span><br><span class="line">#   listen for connections.</span><br><span class="line">#</span><br><span class="line"># However when port forwarding or Network Address Translation (NAT) is</span><br><span class="line"># used, the replica may actually be reachable via different IP and port</span><br><span class="line"># pairs. The following two options can be used by a replica in order to</span><br><span class="line"># report to its master a specific set of IP and port, so that both INFO</span><br><span class="line"># and ROLE will report those values.</span><br><span class="line">#</span><br><span class="line"># There is no need to use both the options if you need to override just</span><br><span class="line"># the port or the IP address.</span><br><span class="line">#</span><br><span class="line"># replica-announce-ip 5.5.5.5</span><br><span class="line"># replica-announce-port 1234</span><br><span class="line"></span><br><span class="line">############################### KEYS TRACKING #################################</span><br><span class="line"></span><br><span class="line"># Redis implements server assisted support for client side caching of values.</span><br><span class="line"># This is implemented using an invalidation table that remembers, using</span><br><span class="line"># a radix key indexed by key name, what clients have which keys. In turn</span><br><span class="line"># this is used in order to send invalidation messages to clients. Please</span><br><span class="line"># check this page to understand more about the feature:</span><br><span class="line">#</span><br><span class="line">#   https://redis.io/topics/client-side-caching</span><br><span class="line">#</span><br><span class="line"># When tracking is enabled for a client, all the read only queries are assumed</span><br><span class="line"># to be cached: this will force Redis to store information in the invalidation</span><br><span class="line"># table. When keys are modified, such information is flushed away, and</span><br><span class="line"># invalidation messages are sent to the clients. However if the workload is</span><br><span class="line"># heavily dominated by reads, Redis could use more and more memory in order</span><br><span class="line"># to track the keys fetched by many clients.</span><br><span class="line">#</span><br><span class="line"># For this reason it is possible to configure a maximum fill value for the</span><br><span class="line"># invalidation table. By default it is set to 1M of keys, and once this limit</span><br><span class="line"># is reached, Redis will start to evict keys in the invalidation table</span><br><span class="line"># even if they were not modified, just to reclaim memory: this will in turn</span><br><span class="line"># force the clients to invalidate the cached values. Basically the table</span><br><span class="line"># maximum size is a trade off between the memory you want to spend server</span><br><span class="line"># side to track information about who cached what, and the ability of clients</span><br><span class="line"># to retain cached objects in memory.</span><br><span class="line">#</span><br><span class="line"># If you set the value to 0, it means there are no limits, and Redis will</span><br><span class="line"># retain as many keys as needed in the invalidation table.</span><br><span class="line"># In the &quot;stats&quot; INFO section, you can find information about the number of</span><br><span class="line"># keys in the invalidation table at every given moment.</span><br><span class="line">#</span><br><span class="line"># Note: when key tracking is used in broadcasting mode, no memory is used</span><br><span class="line"># in the server side so this setting is useless.</span><br><span class="line">#</span><br><span class="line"># tracking-table-max-keys 1000000</span><br><span class="line"></span><br><span class="line">################################## SECURITY ###################################</span><br><span class="line"></span><br><span class="line"># Warning: since Redis is pretty fast, an outside user can try up to</span><br><span class="line"># 1 million passwords per second against a modern box. This means that you</span><br><span class="line"># should use very strong passwords, otherwise they will be very easy to break.</span><br><span class="line"># Note that because the password is really a shared secret between the client</span><br><span class="line"># and the server, and should not be memorized by any human, the password</span><br><span class="line"># can be easily a long string from /dev/urandom or whatever, so by using a</span><br><span class="line"># long and unguessable password no brute force attack will be possible.</span><br><span class="line"></span><br><span class="line"># Redis ACL users are defined in the following format:</span><br><span class="line">#</span><br><span class="line">#   user &lt;username&gt; ... acl rules ...</span><br><span class="line">#</span><br><span class="line"># For example:</span><br><span class="line">#</span><br><span class="line">#   user worker +@list +@connection ~jobs:* on &gt;ffa9203c493aa99</span><br><span class="line">#</span><br><span class="line"># The special username &quot;default&quot; is used for new connections. If this user</span><br><span class="line"># has the &quot;nopass&quot; rule, then new connections will be immediately authenticated</span><br><span class="line"># as the &quot;default&quot; user without the need of any password provided via the</span><br><span class="line"># AUTH command. Otherwise if the &quot;default&quot; user is not flagged with &quot;nopass&quot;</span><br><span class="line"># the connections will start in not authenticated state, and will require</span><br><span class="line"># AUTH (or the HELLO command AUTH option) in order to be authenticated and</span><br><span class="line"># start to work.</span><br><span class="line">#</span><br><span class="line"># The ACL rules that describe what a user can do are the following:</span><br><span class="line">#</span><br><span class="line">#  on           Enable the user: it is possible to authenticate as this user.</span><br><span class="line">#  off          Disable the user: it&#x27;s no longer possible to authenticate</span><br><span class="line">#               with this user, however the already authenticated connections</span><br><span class="line">#               will still work.</span><br><span class="line">#  skip-sanitize-payload    RESTORE dump-payload sanitization is skipped.</span><br><span class="line">#  sanitize-payload         RESTORE dump-payload is sanitized (default).</span><br><span class="line">#  +&lt;command&gt;   Allow the execution of that command.</span><br><span class="line">#               May be used with `|` for allowing subcommands (e.g &quot;+config|get&quot;)</span><br><span class="line">#  -&lt;command&gt;   Disallow the execution of that command.</span><br><span class="line">#               May be used with `|` for blocking subcommands (e.g &quot;-config|set&quot;)</span><br><span class="line">#  +@&lt;category&gt; Allow the execution of all the commands in such category</span><br><span class="line">#               with valid categories are like @admin, @set, @sortedset, ...</span><br><span class="line">#               and so forth, see the full list in the server.c file where</span><br><span class="line">#               the Redis command table is described and defined.</span><br><span class="line">#               The special category @all means all the commands, but currently</span><br><span class="line">#               present in the server, and that will be loaded in the future</span><br><span class="line">#               via modules.</span><br><span class="line">#  +&lt;command&gt;|first-arg  Allow a specific first argument of an otherwise</span><br><span class="line">#                        disabled command. It is only supported on commands with</span><br><span class="line">#                        no sub-commands, and is not allowed as negative form</span><br><span class="line">#                        like -SELECT|1, only additive starting with &quot;+&quot;. This</span><br><span class="line">#                        feature is deprecated and may be removed in the future.</span><br><span class="line">#  allcommands  Alias for +@all. Note that it implies the ability to execute</span><br><span class="line">#               all the future commands loaded via the modules system.</span><br><span class="line">#  nocommands   Alias for -@all.</span><br><span class="line">#  ~&lt;pattern&gt;   Add a pattern of keys that can be mentioned as part of</span><br><span class="line">#               commands. For instance ~* allows all the keys. The pattern</span><br><span class="line">#               is a glob-style pattern like the one of KEYS.</span><br><span class="line">#               It is possible to specify multiple patterns.</span><br><span class="line"># %R~&lt;pattern&gt;  Add key read pattern that specifies which keys can be read </span><br><span class="line">#               from.</span><br><span class="line"># %W~&lt;pattern&gt;  Add key write pattern that specifies which keys can be</span><br><span class="line">#               written to. </span><br><span class="line">#  allkeys      Alias for ~*</span><br><span class="line">#  resetkeys    Flush the list of allowed keys patterns.</span><br><span class="line">#  &amp;&lt;pattern&gt;   Add a glob-style pattern of Pub/Sub channels that can be</span><br><span class="line">#               accessed by the user. It is possible to specify multiple channel</span><br><span class="line">#               patterns.</span><br><span class="line">#  allchannels  Alias for &amp;*</span><br><span class="line">#  resetchannels            Flush the list of allowed channel patterns.</span><br><span class="line">#  &gt;&lt;password&gt;  Add this password to the list of valid password for the user.</span><br><span class="line">#               For example &gt;mypass will add &quot;mypass&quot; to the list.</span><br><span class="line">#               This directive clears the &quot;nopass&quot; flag (see later).</span><br><span class="line">#  &lt;&lt;password&gt;  Remove this password from the list of valid passwords.</span><br><span class="line">#  nopass       All the set passwords of the user are removed, and the user</span><br><span class="line">#               is flagged as requiring no password: it means that every</span><br><span class="line">#               password will work against this user. If this directive is</span><br><span class="line">#               used for the default user, every new connection will be</span><br><span class="line">#               immediately authenticated with the default user without</span><br><span class="line">#               any explicit AUTH command required. Note that the &quot;resetpass&quot;</span><br><span class="line">#               directive will clear this condition.</span><br><span class="line">#  resetpass    Flush the list of allowed passwords. Moreover removes the</span><br><span class="line">#               &quot;nopass&quot; status. After &quot;resetpass&quot; the user has no associated</span><br><span class="line">#               passwords and there is no way to authenticate without adding</span><br><span class="line">#               some password (or setting it as &quot;nopass&quot; later).</span><br><span class="line">#  reset        Performs the following actions: resetpass, resetkeys, resetchannels,</span><br><span class="line">#               allchannels (if acl-pubsub-default is set), off, clearselectors, -@all.</span><br><span class="line">#               The user returns to the same state it has immediately after its creation.</span><br><span class="line"># (&lt;options&gt;)   Create a new selector with the options specified within the</span><br><span class="line">#               parentheses and attach it to the user. Each option should be </span><br><span class="line">#               space separated. The first character must be ( and the last </span><br><span class="line">#               character must be ).</span><br><span class="line"># clearselectors            Remove all of the currently attached selectors. </span><br><span class="line">#                           Note this does not change the &quot;root&quot; user permissions,</span><br><span class="line">#                           which are the permissions directly applied onto the</span><br><span class="line">#                           user (outside the parentheses).</span><br><span class="line">#</span><br><span class="line"># ACL rules can be specified in any order: for instance you can start with</span><br><span class="line"># passwords, then flags, or key patterns. However note that the additive</span><br><span class="line"># and subtractive rules will CHANGE MEANING depending on the ordering.</span><br><span class="line"># For instance see the following example:</span><br><span class="line">#</span><br><span class="line">#   user alice on +@all -DEBUG ~* &gt;somepassword</span><br><span class="line">#</span><br><span class="line"># This will allow &quot;alice&quot; to use all the commands with the exception of the</span><br><span class="line"># DEBUG command, since +@all added all the commands to the set of the commands</span><br><span class="line"># alice can use, and later DEBUG was removed. However if we invert the order</span><br><span class="line"># of two ACL rules the result will be different:</span><br><span class="line">#</span><br><span class="line">#   user alice on -DEBUG +@all ~* &gt;somepassword</span><br><span class="line">#</span><br><span class="line"># Now DEBUG was removed when alice had yet no commands in the set of allowed</span><br><span class="line"># commands, later all the commands are added, so the user will be able to</span><br><span class="line"># execute everything.</span><br><span class="line">#</span><br><span class="line"># Basically ACL rules are processed left-to-right.</span><br><span class="line">#</span><br><span class="line"># The following is a list of command categories and their meanings:</span><br><span class="line"># * keyspace - Writing or reading from keys, databases, or their metadata </span><br><span class="line">#     in a type agnostic way. Includes DEL, RESTORE, DUMP, RENAME, EXISTS, DBSIZE,</span><br><span class="line">#     KEYS, EXPIRE, TTL, FLUSHALL, etc. Commands that may modify the keyspace,</span><br><span class="line">#     key or metadata will also have `write` category. Commands that only read</span><br><span class="line">#     the keyspace, key or metadata will have the `read` category.</span><br><span class="line"># * read - Reading from keys (values or metadata). Note that commands that don&#x27;t</span><br><span class="line">#     interact with keys, will not have either `read` or `write`.</span><br><span class="line"># * write - Writing to keys (values or metadata)</span><br><span class="line"># * admin - Administrative commands. Normal applications will never need to use</span><br><span class="line">#     these. Includes REPLICAOF, CONFIG, DEBUG, SAVE, MONITOR, ACL, SHUTDOWN, etc.</span><br><span class="line"># * dangerous - Potentially dangerous (each should be considered with care for</span><br><span class="line">#     various reasons). This includes FLUSHALL, MIGRATE, RESTORE, SORT, KEYS,</span><br><span class="line">#     CLIENT, DEBUG, INFO, CONFIG, SAVE, REPLICAOF, etc.</span><br><span class="line"># * connection - Commands affecting the connection or other connections.</span><br><span class="line">#     This includes AUTH, SELECT, COMMAND, CLIENT, ECHO, PING, etc.</span><br><span class="line"># * blocking - Potentially blocking the connection until released by another</span><br><span class="line">#     command.</span><br><span class="line"># * fast - Fast O(1) commands. May loop on the number of arguments, but not the</span><br><span class="line">#     number of elements in the key.</span><br><span class="line"># * slow - All commands that are not Fast.</span><br><span class="line"># * pubsub - PUBLISH / SUBSCRIBE related</span><br><span class="line"># * transaction - WATCH / MULTI / EXEC related commands.</span><br><span class="line"># * scripting - Scripting related.</span><br><span class="line"># * set - Data type: sets related.</span><br><span class="line"># * sortedset - Data type: zsets related.</span><br><span class="line"># * list - Data type: lists related.</span><br><span class="line"># * hash - Data type: hashes related.</span><br><span class="line"># * string - Data type: strings related.</span><br><span class="line"># * bitmap - Data type: bitmaps related.</span><br><span class="line"># * hyperloglog - Data type: hyperloglog related.</span><br><span class="line"># * geo - Data type: geo related.</span><br><span class="line"># * stream - Data type: streams related.</span><br><span class="line">#</span><br><span class="line"># For more information about ACL configuration please refer to</span><br><span class="line"># the Redis web site at https://redis.io/topics/acl</span><br><span class="line"></span><br><span class="line"># ACL LOG</span><br><span class="line">#</span><br><span class="line"># The ACL Log tracks failed commands and authentication events associated</span><br><span class="line"># with ACLs. The ACL Log is useful to troubleshoot failed commands blocked</span><br><span class="line"># by ACLs. The ACL Log is stored in memory. You can reclaim memory with</span><br><span class="line"># ACL LOG RESET. Define the maximum entry length of the ACL Log below.</span><br><span class="line">acllog-max-len 128</span><br><span class="line"></span><br><span class="line"># Using an external ACL file</span><br><span class="line">#</span><br><span class="line"># Instead of configuring users here in this file, it is possible to use</span><br><span class="line"># a stand-alone file just listing users. The two methods cannot be mixed:</span><br><span class="line"># if you configure users here and at the same time you activate the external</span><br><span class="line"># ACL file, the server will refuse to start.</span><br><span class="line">#</span><br><span class="line"># The format of the external ACL user file is exactly the same as the</span><br><span class="line"># format that is used inside redis.conf to describe users.</span><br><span class="line">#</span><br><span class="line"># aclfile /etc/redis/users.acl</span><br><span class="line"></span><br><span class="line"># IMPORTANT NOTE: starting with Redis 6 &quot;requirepass&quot; is just a compatibility</span><br><span class="line"># layer on top of the new ACL system. The option effect will be just setting</span><br><span class="line"># the password for the default user. Clients will still authenticate using</span><br><span class="line"># AUTH &lt;password&gt; as usually, or more explicitly with AUTH default &lt;password&gt;</span><br><span class="line"># if they follow the new protocol: both will work.</span><br><span class="line">#</span><br><span class="line"># The requirepass is not compatible with aclfile option and the ACL LOAD</span><br><span class="line"># command, these will cause requirepass to be ignored.</span><br><span class="line">#</span><br><span class="line"># requirepass foobared</span><br><span class="line"></span><br><span class="line"># New users are initialized with restrictive permissions by default, via the</span><br><span class="line"># equivalent of this ACL rule &#x27;off resetkeys -@all&#x27;. Starting with Redis 6.2, it</span><br><span class="line"># is possible to manage access to Pub/Sub channels with ACL rules as well. The</span><br><span class="line"># default Pub/Sub channels permission if new users is controlled by the</span><br><span class="line"># acl-pubsub-default configuration directive, which accepts one of these values:</span><br><span class="line">#</span><br><span class="line"># allchannels: grants access to all Pub/Sub channels</span><br><span class="line"># resetchannels: revokes access to all Pub/Sub channels</span><br><span class="line">#</span><br><span class="line"># From Redis 7.0, acl-pubsub-default defaults to &#x27;resetchannels&#x27; permission.</span><br><span class="line">#</span><br><span class="line"># acl-pubsub-default resetchannels</span><br><span class="line"></span><br><span class="line"># Command renaming (DEPRECATED).</span><br><span class="line">#</span><br><span class="line"># ------------------------------------------------------------------------</span><br><span class="line"># WARNING: avoid using this option if possible. Instead use ACLs to remove</span><br><span class="line"># commands from the default user, and put them only in some admin user you</span><br><span class="line"># create for administrative purposes.</span><br><span class="line"># ------------------------------------------------------------------------</span><br><span class="line">#</span><br><span class="line"># It is possible to change the name of dangerous commands in a shared</span><br><span class="line"># environment. For instance the CONFIG command may be renamed into something</span><br><span class="line"># hard to guess so that it will still be available for internal-use tools</span><br><span class="line"># but not available for general clients.</span><br><span class="line">#</span><br><span class="line"># Example:</span><br><span class="line">#</span><br><span class="line"># rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52</span><br><span class="line">#</span><br><span class="line"># It is also possible to completely kill a command by renaming it into</span><br><span class="line"># an empty string:</span><br><span class="line">#</span><br><span class="line"># rename-command CONFIG &quot;&quot;</span><br><span class="line">#</span><br><span class="line"># Please note that changing the name of commands that are logged into the</span><br><span class="line"># AOF file or transmitted to replicas may cause problems.</span><br><span class="line"></span><br><span class="line">################################### CLIENTS ####################################</span><br><span class="line"></span><br><span class="line"># Set the max number of connected clients at the same time. By default</span><br><span class="line"># this limit is set to 10000 clients, however if the Redis server is not</span><br><span class="line"># able to configure the process file limit to allow for the specified limit</span><br><span class="line"># the max number of allowed clients is set to the current file limit</span><br><span class="line"># minus 32 (as Redis reserves a few file descriptors for internal uses).</span><br><span class="line">#</span><br><span class="line"># Once the limit is reached Redis will close all the new connections sending</span><br><span class="line"># an error &#x27;max number of clients reached&#x27;.</span><br><span class="line">#</span><br><span class="line"># IMPORTANT: When Redis Cluster is used, the max number of connections is also</span><br><span class="line"># shared with the cluster bus: every node in the cluster will use two</span><br><span class="line"># connections, one incoming and another outgoing. It is important to size the</span><br><span class="line"># limit accordingly in case of very large clusters.</span><br><span class="line">#</span><br><span class="line"># maxclients 10000</span><br><span class="line"></span><br><span class="line">############################## MEMORY MANAGEMENT ################################</span><br><span class="line"></span><br><span class="line"># Set a memory usage limit to the specified amount of bytes.</span><br><span class="line"># When the memory limit is reached Redis will try to remove keys</span><br><span class="line"># according to the eviction policy selected (see maxmemory-policy).</span><br><span class="line">#</span><br><span class="line"># If Redis can&#x27;t remove keys according to the policy, or if the policy is</span><br><span class="line"># set to &#x27;noeviction&#x27;, Redis will start to reply with errors to commands</span><br><span class="line"># that would use more memory, like SET, LPUSH, and so on, and will continue</span><br><span class="line"># to reply to read-only commands like GET.</span><br><span class="line">#</span><br><span class="line"># This option is usually useful when using Redis as an LRU or LFU cache, or to</span><br><span class="line"># set a hard memory limit for an instance (using the &#x27;noeviction&#x27; policy).</span><br><span class="line">#</span><br><span class="line"># WARNING: If you have replicas attached to an instance with maxmemory on,</span><br><span class="line"># the size of the output buffers needed to feed the replicas are subtracted</span><br><span class="line"># from the used memory count, so that network problems / resyncs will</span><br><span class="line"># not trigger a loop where keys are evicted, and in turn the output</span><br><span class="line"># buffer of replicas is full with DELs of keys evicted triggering the deletion</span><br><span class="line"># of more keys, and so forth until the database is completely emptied.</span><br><span class="line">#</span><br><span class="line"># In short... if you have replicas attached it is suggested that you set a lower</span><br><span class="line"># limit for maxmemory so that there is some free RAM on the system for replica</span><br><span class="line"># output buffers (but this is not needed if the policy is &#x27;noeviction&#x27;).</span><br><span class="line">#</span><br><span class="line"># maxmemory &lt;bytes&gt;</span><br><span class="line"></span><br><span class="line"># MAXMEMORY POLICY: how Redis will select what to remove when maxmemory</span><br><span class="line"># is reached. You can select one from the following behaviors:</span><br><span class="line">#</span><br><span class="line"># volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set.</span><br><span class="line"># allkeys-lru -&gt; Evict any key using approximated LRU.</span><br><span class="line"># volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set.</span><br><span class="line"># allkeys-lfu -&gt; Evict any key using approximated LFU.</span><br><span class="line"># volatile-random -&gt; Remove a random key having an expire set.</span><br><span class="line"># allkeys-random -&gt; Remove a random key, any key.</span><br><span class="line"># volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)</span><br><span class="line"># noeviction -&gt; Don&#x27;t evict anything, just return an error on write operations.</span><br><span class="line">#</span><br><span class="line"># LRU means Least Recently Used</span><br><span class="line"># LFU means Least Frequently Used</span><br><span class="line">#</span><br><span class="line"># Both LRU, LFU and volatile-ttl are implemented using approximated</span><br><span class="line"># randomized algorithms.</span><br><span class="line">#</span><br><span class="line"># Note: with any of the above policies, when there are no suitable keys for</span><br><span class="line"># eviction, Redis will return an error on write operations that require</span><br><span class="line"># more memory. These are usually commands that create new keys, add data or</span><br><span class="line"># modify existing keys. A few examples are: SET, INCR, HSET, LPUSH, SUNIONSTORE,</span><br><span class="line"># SORT (due to the STORE argument), and EXEC (if the transaction includes any</span><br><span class="line"># command that requires memory).</span><br><span class="line">#</span><br><span class="line"># The default is:</span><br><span class="line">#</span><br><span class="line"># maxmemory-policy noeviction</span><br><span class="line"></span><br><span class="line"># LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated</span><br><span class="line"># algorithms (in order to save memory), so you can tune it for speed or</span><br><span class="line"># accuracy. By default Redis will check five keys and pick the one that was</span><br><span class="line"># used least recently, you can change the sample size using the following</span><br><span class="line"># configuration directive.</span><br><span class="line">#</span><br><span class="line"># The default of 5 produces good enough results. 10 Approximates very closely</span><br><span class="line"># true LRU but costs more CPU. 3 is faster but not very accurate.</span><br><span class="line">#</span><br><span class="line"># maxmemory-samples 5</span><br><span class="line"></span><br><span class="line"># Eviction processing is designed to function well with the default setting.</span><br><span class="line"># If there is an unusually large amount of write traffic, this value may need to</span><br><span class="line"># be increased.  Decreasing this value may reduce latency at the risk of</span><br><span class="line"># eviction processing effectiveness</span><br><span class="line">#   0 = minimum latency, 10 = default, 100 = process without regard to latency</span><br><span class="line">#</span><br><span class="line"># maxmemory-eviction-tenacity 10</span><br><span class="line"></span><br><span class="line"># Starting from Redis 5, by default a replica will ignore its maxmemory setting</span><br><span class="line"># (unless it is promoted to master after a failover or manually). It means</span><br><span class="line"># that the eviction of keys will be just handled by the master, sending the</span><br><span class="line"># DEL commands to the replica as keys evict in the master side.</span><br><span class="line">#</span><br><span class="line"># This behavior ensures that masters and replicas stay consistent, and is usually</span><br><span class="line"># what you want, however if your replica is writable, or you want the replica</span><br><span class="line"># to have a different memory setting, and you are sure all the writes performed</span><br><span class="line"># to the replica are idempotent, then you may change this default (but be sure</span><br><span class="line"># to understand what you are doing).</span><br><span class="line">#</span><br><span class="line"># Note that since the replica by default does not evict, it may end using more</span><br><span class="line"># memory than the one set via maxmemory (there are certain buffers that may</span><br><span class="line"># be larger on the replica, or data structures may sometimes take more memory</span><br><span class="line"># and so forth). So make sure you monitor your replicas and make sure they</span><br><span class="line"># have enough memory to never hit a real out-of-memory condition before the</span><br><span class="line"># master hits the configured maxmemory setting.</span><br><span class="line">#</span><br><span class="line"># replica-ignore-maxmemory yes</span><br><span class="line"></span><br><span class="line"># Redis reclaims expired keys in two ways: upon access when those keys are</span><br><span class="line"># found to be expired, and also in background, in what is called the</span><br><span class="line"># &quot;active expire key&quot;. The key space is slowly and interactively scanned</span><br><span class="line"># looking for expired keys to reclaim, so that it is possible to free memory</span><br><span class="line"># of keys that are expired and will never be accessed again in a short time.</span><br><span class="line">#</span><br><span class="line"># The default effort of the expire cycle will try to avoid having more than</span><br><span class="line"># ten percent of expired keys still in memory, and will try to avoid consuming</span><br><span class="line"># more than 25% of total memory and to add latency to the system. However</span><br><span class="line"># it is possible to increase the expire &quot;effort&quot; that is normally set to</span><br><span class="line"># &quot;1&quot;, to a greater value, up to the value &quot;10&quot;. At its maximum value the</span><br><span class="line"># system will use more CPU, longer cycles (and technically may introduce</span><br><span class="line"># more latency), and will tolerate less already expired keys still present</span><br><span class="line"># in the system. It&#x27;s a tradeoff between memory, CPU and latency.</span><br><span class="line">#</span><br><span class="line"># active-expire-effort 1</span><br><span class="line"></span><br><span class="line">############################# LAZY FREEING ####################################</span><br><span class="line"></span><br><span class="line"># Redis has two primitives to delete keys. One is called DEL and is a blocking</span><br><span class="line"># deletion of the object. It means that the server stops processing new commands</span><br><span class="line"># in order to reclaim all the memory associated with an object in a synchronous</span><br><span class="line"># way. If the key deleted is associated with a small object, the time needed</span><br><span class="line"># in order to execute the DEL command is very small and comparable to most other</span><br><span class="line"># O(1) or O(log_N) commands in Redis. However if the key is associated with an</span><br><span class="line"># aggregated value containing millions of elements, the server can block for</span><br><span class="line"># a long time (even seconds) in order to complete the operation.</span><br><span class="line">#</span><br><span class="line"># For the above reasons Redis also offers non blocking deletion primitives</span><br><span class="line"># such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and</span><br><span class="line"># FLUSHDB commands, in order to reclaim memory in background. Those commands</span><br><span class="line"># are executed in constant time. Another thread will incrementally free the</span><br><span class="line"># object in the background as fast as possible.</span><br><span class="line">#</span><br><span class="line"># DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.</span><br><span class="line"># It&#x27;s up to the design of the application to understand when it is a good</span><br><span class="line"># idea to use one or the other. However the Redis server sometimes has to</span><br><span class="line"># delete keys or flush the whole database as a side effect of other operations.</span><br><span class="line"># Specifically Redis deletes objects independently of a user call in the</span><br><span class="line"># following scenarios:</span><br><span class="line">#</span><br><span class="line"># 1) On eviction, because of the maxmemory and maxmemory policy configurations,</span><br><span class="line">#    in order to make room for new data, without going over the specified</span><br><span class="line">#    memory limit.</span><br><span class="line"># 2) Because of expire: when a key with an associated time to live (see the</span><br><span class="line">#    EXPIRE command) must be deleted from memory.</span><br><span class="line"># 3) Because of a side effect of a command that stores data on a key that may</span><br><span class="line">#    already exist. For example the RENAME command may delete the old key</span><br><span class="line">#    content when it is replaced with another one. Similarly SUNIONSTORE</span><br><span class="line">#    or SORT with STORE option may delete existing keys. The SET command</span><br><span class="line">#    itself removes any old content of the specified key in order to replace</span><br><span class="line">#    it with the specified string.</span><br><span class="line"># 4) During replication, when a replica performs a full resynchronization with</span><br><span class="line">#    its master, the content of the whole database is removed in order to</span><br><span class="line">#    load the RDB file just transferred.</span><br><span class="line">#</span><br><span class="line"># In all the above cases the default is to delete objects in a blocking way,</span><br><span class="line"># like if DEL was called. However you can configure each case specifically</span><br><span class="line"># in order to instead release memory in a non-blocking way like if UNLINK</span><br><span class="line"># was called, using the following configuration directives.</span><br><span class="line"></span><br><span class="line">lazyfree-lazy-eviction no</span><br><span class="line">lazyfree-lazy-expire no</span><br><span class="line">lazyfree-lazy-server-del no</span><br><span class="line">replica-lazy-flush no</span><br><span class="line"></span><br><span class="line"># It is also possible, for the case when to replace the user code DEL calls</span><br><span class="line"># with UNLINK calls is not easy, to modify the default behavior of the DEL</span><br><span class="line"># command to act exactly like UNLINK, using the following configuration</span><br><span class="line"># directive:</span><br><span class="line"></span><br><span class="line">lazyfree-lazy-user-del no</span><br><span class="line"></span><br><span class="line"># FLUSHDB, FLUSHALL, SCRIPT FLUSH and FUNCTION FLUSH support both asynchronous and synchronous</span><br><span class="line"># deletion, which can be controlled by passing the [SYNC|ASYNC] flags into the</span><br><span class="line"># commands. When neither flag is passed, this directive will be used to determine</span><br><span class="line"># if the data should be deleted asynchronously.</span><br><span class="line"></span><br><span class="line">lazyfree-lazy-user-flush no</span><br><span class="line"></span><br><span class="line">################################ THREADED I/O #################################</span><br><span class="line"></span><br><span class="line"># Redis is mostly single threaded, however there are certain threaded</span><br><span class="line"># operations such as UNLINK, slow I/O accesses and other things that are</span><br><span class="line"># performed on side threads.</span><br><span class="line">#</span><br><span class="line"># Now it is also possible to handle Redis clients socket reads and writes</span><br><span class="line"># in different I/O threads. Since especially writing is so slow, normally</span><br><span class="line"># Redis users use pipelining in order to speed up the Redis performances per</span><br><span class="line"># core, and spawn multiple instances in order to scale more. Using I/O</span><br><span class="line"># threads it is possible to easily speedup two times Redis without resorting</span><br><span class="line"># to pipelining nor sharding of the instance.</span><br><span class="line">#</span><br><span class="line"># By default threading is disabled, we suggest enabling it only in machines</span><br><span class="line"># that have at least 4 or more cores, leaving at least one spare core.</span><br><span class="line"># Using more than 8 threads is unlikely to help much. We also recommend using</span><br><span class="line"># threaded I/O only if you actually have performance problems, with Redis</span><br><span class="line"># instances being able to use a quite big percentage of CPU time, otherwise</span><br><span class="line"># there is no point in using this feature.</span><br><span class="line">#</span><br><span class="line"># So for instance if you have a four cores boxes, try to use 2 or 3 I/O</span><br><span class="line"># threads, if you have a 8 cores, try to use 6 threads. In order to</span><br><span class="line"># enable I/O threads use the following configuration directive:</span><br><span class="line">#</span><br><span class="line"># io-threads 4</span><br><span class="line">#</span><br><span class="line"># Setting io-threads to 1 will just use the main thread as usual.</span><br><span class="line"># When I/O threads are enabled, we only use threads for writes, that is</span><br><span class="line"># to thread the write(2) syscall and transfer the client buffers to the</span><br><span class="line"># socket. However it is also possible to enable threading of reads and</span><br><span class="line"># protocol parsing using the following configuration directive, by setting</span><br><span class="line"># it to yes:</span><br><span class="line">#</span><br><span class="line"># io-threads-do-reads no</span><br><span class="line">#</span><br><span class="line"># Usually threading reads doesn&#x27;t help much.</span><br><span class="line">#</span><br><span class="line"># NOTE 1: This configuration directive cannot be changed at runtime via</span><br><span class="line"># CONFIG SET. Also, this feature currently does not work when SSL is</span><br><span class="line"># enabled.</span><br><span class="line">#</span><br><span class="line"># NOTE 2: If you want to test the Redis speedup using redis-benchmark, make</span><br><span class="line"># sure you also run the benchmark itself in threaded mode, using the</span><br><span class="line"># --threads option to match the number of Redis threads, otherwise you&#x27;ll not</span><br><span class="line"># be able to notice the improvements.</span><br><span class="line"></span><br><span class="line">############################ KERNEL OOM CONTROL ##############################</span><br><span class="line"></span><br><span class="line"># On Linux, it is possible to hint the kernel OOM killer on what processes</span><br><span class="line"># should be killed first when out of memory.</span><br><span class="line">#</span><br><span class="line"># Enabling this feature makes Redis actively control the oom_score_adj value</span><br><span class="line"># for all its processes, depending on their role. The default scores will</span><br><span class="line"># attempt to have background child processes killed before all others, and</span><br><span class="line"># replicas killed before masters.</span><br><span class="line">#</span><br><span class="line"># Redis supports these options:</span><br><span class="line">#</span><br><span class="line"># no:       Don&#x27;t make changes to oom-score-adj (default).</span><br><span class="line"># yes:      Alias to &quot;relative&quot; see below.</span><br><span class="line"># absolute: Values in oom-score-adj-values are written as is to the kernel.</span><br><span class="line"># relative: Values are used relative to the initial value of oom_score_adj when</span><br><span class="line">#           the server starts and are then clamped to a range of -1000 to 1000.</span><br><span class="line">#           Because typically the initial value is 0, they will often match the</span><br><span class="line">#           absolute values.</span><br><span class="line">oom-score-adj no</span><br><span class="line"></span><br><span class="line"># When oom-score-adj is used, this directive controls the specific values used</span><br><span class="line"># for master, replica and background child processes. Values range -2000 to</span><br><span class="line"># 2000 (higher means more likely to be killed).</span><br><span class="line">#</span><br><span class="line"># Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)</span><br><span class="line"># can freely increase their value, but not decrease it below its initial</span><br><span class="line"># settings. This means that setting oom-score-adj to &quot;relative&quot; and setting the</span><br><span class="line"># oom-score-adj-values to positive values will always succeed.</span><br><span class="line">oom-score-adj-values 0 200 800</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#################### KERNEL transparent hugepage CONTROL ######################</span><br><span class="line"></span><br><span class="line"># Usually the kernel Transparent Huge Pages control is set to &quot;madvise&quot; or</span><br><span class="line"># or &quot;never&quot; by default (/sys/kernel/mm/transparent_hugepage/enabled), in which</span><br><span class="line"># case this config has no effect. On systems in which it is set to &quot;always&quot;,</span><br><span class="line"># redis will attempt to disable it specifically for the redis process in order</span><br><span class="line"># to avoid latency problems specifically with fork(2) and CoW.</span><br><span class="line"># If for some reason you prefer to keep it enabled, you can set this config to</span><br><span class="line"># &quot;no&quot; and the kernel global to &quot;always&quot;.</span><br><span class="line"></span><br><span class="line">disable-thp yes</span><br><span class="line"></span><br><span class="line">############################## APPEND ONLY MODE ###############################</span><br><span class="line"></span><br><span class="line"># By default Redis asynchronously dumps the dataset on disk. This mode is</span><br><span class="line"># good enough in many applications, but an issue with the Redis process or</span><br><span class="line"># a power outage may result into a few minutes of writes lost (depending on</span><br><span class="line"># the configured save points).</span><br><span class="line">#</span><br><span class="line"># The Append Only File is an alternative persistence mode that provides</span><br><span class="line"># much better durability. For instance using the default data fsync policy</span><br><span class="line"># (see later in the config file) Redis can lose just one second of writes in a</span><br><span class="line"># dramatic event like a server power outage, or a single write if something</span><br><span class="line"># wrong with the Redis process itself happens, but the operating system is</span><br><span class="line"># still running correctly.</span><br><span class="line">#</span><br><span class="line"># AOF and RDB persistence can be enabled at the same time without problems.</span><br><span class="line"># If the AOF is enabled on startup Redis will load the AOF, that is the file</span><br><span class="line"># with the better durability guarantees.</span><br><span class="line">#</span><br><span class="line"># Please check https://redis.io/topics/persistence for more information.</span><br><span class="line"></span><br><span class="line">appendonly no</span><br><span class="line"></span><br><span class="line"># The base name of the append only file.</span><br><span class="line">#</span><br><span class="line"># Redis 7 and newer use a set of append-only files to persist the dataset</span><br><span class="line"># and changes applied to it. There are two basic types of files in use:</span><br><span class="line">#</span><br><span class="line"># - Base files, which are a snapshot representing the complete state of the</span><br><span class="line">#   dataset at the time the file was created. Base files can be either in</span><br><span class="line">#   the form of RDB (binary serialized) or AOF (textual commands).</span><br><span class="line"># - Incremental files, which contain additional commands that were applied</span><br><span class="line">#   to the dataset following the previous file.</span><br><span class="line">#</span><br><span class="line"># In addition, manifest files are used to track the files and the order in</span><br><span class="line"># which they were created and should be applied.</span><br><span class="line">#</span><br><span class="line"># Append-only file names are created by Redis following a specific pattern.</span><br><span class="line"># The file name&#x27;s prefix is based on the &#x27;appendfilename&#x27; configuration</span><br><span class="line"># parameter, followed by additional information about the sequence and type.</span><br><span class="line">#</span><br><span class="line"># For example, if appendfilename is set to appendonly.aof, the following file</span><br><span class="line"># names could be derived:</span><br><span class="line">#</span><br><span class="line"># - appendonly.aof.1.base.rdb as a base file.</span><br><span class="line"># - appendonly.aof.1.incr.aof, appendonly.aof.2.incr.aof as incremental files.</span><br><span class="line"># - appendonly.aof.manifest as a manifest file.</span><br><span class="line"></span><br><span class="line">appendfilename &quot;appendonly.aof&quot;</span><br><span class="line"></span><br><span class="line"># For convenience, Redis stores all persistent append-only files in a dedicated</span><br><span class="line"># directory. The name of the directory is determined by the appenddirname</span><br><span class="line"># configuration parameter.</span><br><span class="line"></span><br><span class="line">appenddirname &quot;appendonlydir&quot;</span><br><span class="line"></span><br><span class="line"># The fsync() call tells the Operating System to actually write data on disk</span><br><span class="line"># instead of waiting for more data in the output buffer. Some OS will really flush</span><br><span class="line"># data on disk, some other OS will just try to do it ASAP.</span><br><span class="line">#</span><br><span class="line"># Redis supports three different modes:</span><br><span class="line">#</span><br><span class="line"># no: don&#x27;t fsync, just let the OS flush the data when it wants. Faster.</span><br><span class="line"># always: fsync after every write to the append only log. Slow, Safest.</span><br><span class="line"># everysec: fsync only one time every second. Compromise.</span><br><span class="line">#</span><br><span class="line"># The default is &quot;everysec&quot;, as that&#x27;s usually the right compromise between</span><br><span class="line"># speed and data safety. It&#x27;s up to you to understand if you can relax this to</span><br><span class="line"># &quot;no&quot; that will let the operating system flush the output buffer when</span><br><span class="line"># it wants, for better performances (but if you can live with the idea of</span><br><span class="line"># some data loss consider the default persistence mode that&#x27;s snapshotting),</span><br><span class="line"># or on the contrary, use &quot;always&quot; that&#x27;s very slow but a bit safer than</span><br><span class="line"># everysec.</span><br><span class="line">#</span><br><span class="line"># More details please check the following article:</span><br><span class="line"># http://antirez.com/post/redis-persistence-demystified.html</span><br><span class="line">#</span><br><span class="line"># If unsure, use &quot;everysec&quot;.</span><br><span class="line"></span><br><span class="line"># appendfsync always</span><br><span class="line">appendfsync everysec</span><br><span class="line"># appendfsync no</span><br><span class="line"></span><br><span class="line"># When the AOF fsync policy is set to always or everysec, and a background</span><br><span class="line"># saving process (a background save or AOF log background rewriting) is</span><br><span class="line"># performing a lot of I/O against the disk, in some Linux configurations</span><br><span class="line"># Redis may block too long on the fsync() call. Note that there is no fix for</span><br><span class="line"># this currently, as even performing fsync in a different thread will block</span><br><span class="line"># our synchronous write(2) call.</span><br><span class="line">#</span><br><span class="line"># In order to mitigate this problem it&#x27;s possible to use the following option</span><br><span class="line"># that will prevent fsync() from being called in the main process while a</span><br><span class="line"># BGSAVE or BGREWRITEAOF is in progress.</span><br><span class="line">#</span><br><span class="line"># This means that while another child is saving, the durability of Redis is</span><br><span class="line"># the same as &quot;appendfsync no&quot;. In practical terms, this means that it is</span><br><span class="line"># possible to lose up to 30 seconds of log in the worst scenario (with the</span><br><span class="line"># default Linux settings).</span><br><span class="line">#</span><br><span class="line"># If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as</span><br><span class="line"># &quot;no&quot; that is the safest pick from the point of view of durability.</span><br><span class="line"></span><br><span class="line">no-appendfsync-on-rewrite no</span><br><span class="line"></span><br><span class="line"># Automatic rewrite of the append only file.</span><br><span class="line"># Redis is able to automatically rewrite the log file implicitly calling</span><br><span class="line"># BGREWRITEAOF when the AOF log size grows by the specified percentage.</span><br><span class="line">#</span><br><span class="line"># This is how it works: Redis remembers the size of the AOF file after the</span><br><span class="line"># latest rewrite (if no rewrite has happened since the restart, the size of</span><br><span class="line"># the AOF at startup is used).</span><br><span class="line">#</span><br><span class="line"># This base size is compared to the current size. If the current size is</span><br><span class="line"># bigger than the specified percentage, the rewrite is triggered. Also</span><br><span class="line"># you need to specify a minimal size for the AOF file to be rewritten, this</span><br><span class="line"># is useful to avoid rewriting the AOF file even if the percentage increase</span><br><span class="line"># is reached but it is still pretty small.</span><br><span class="line">#</span><br><span class="line"># Specify a percentage of zero in order to disable the automatic AOF</span><br><span class="line"># rewrite feature.</span><br><span class="line"></span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br><span class="line"></span><br><span class="line"># An AOF file may be found to be truncated at the end during the Redis</span><br><span class="line"># startup process, when the AOF data gets loaded back into memory.</span><br><span class="line"># This may happen when the system where Redis is running</span><br><span class="line"># crashes, especially when an ext4 filesystem is mounted without the</span><br><span class="line"># data=ordered option (however this can&#x27;t happen when Redis itself</span><br><span class="line"># crashes or aborts but the operating system still works correctly).</span><br><span class="line">#</span><br><span class="line"># Redis can either exit with an error when this happens, or load as much</span><br><span class="line"># data as possible (the default now) and start if the AOF file is found</span><br><span class="line"># to be truncated at the end. The following option controls this behavior.</span><br><span class="line">#</span><br><span class="line"># If aof-load-truncated is set to yes, a truncated AOF file is loaded and</span><br><span class="line"># the Redis server starts emitting a log to inform the user of the event.</span><br><span class="line"># Otherwise if the option is set to no, the server aborts with an error</span><br><span class="line"># and refuses to start. When the option is set to no, the user requires</span><br><span class="line"># to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart</span><br><span class="line"># the server.</span><br><span class="line">#</span><br><span class="line"># Note that if the AOF file will be found to be corrupted in the middle</span><br><span class="line"># the server will still exit with an error. This option only applies when</span><br><span class="line"># Redis will try to read more data from the AOF file but not enough bytes</span><br><span class="line"># will be found.</span><br><span class="line">aof-load-truncated yes</span><br><span class="line"></span><br><span class="line"># Redis can create append-only base files in either RDB or AOF formats. Using</span><br><span class="line"># the RDB format is always faster and more efficient, and disabling it is only</span><br><span class="line"># supported for backward compatibility purposes.</span><br><span class="line">aof-use-rdb-preamble yes</span><br><span class="line"></span><br><span class="line"># Redis supports recording timestamp annotations in the AOF to support restoring</span><br><span class="line"># the data from a specific point-in-time. However, using this capability changes</span><br><span class="line"># the AOF format in a way that may not be compatible with existing AOF parsers.</span><br><span class="line">aof-timestamp-enabled no</span><br><span class="line"></span><br><span class="line">################################ SHUTDOWN #####################################</span><br><span class="line"></span><br><span class="line"># Maximum time to wait for replicas when shutting down, in seconds.</span><br><span class="line">#</span><br><span class="line"># During shut down, a grace period allows any lagging replicas to catch up with</span><br><span class="line"># the latest replication offset before the master exists. This period can</span><br><span class="line"># prevent data loss, especially for deployments without configured disk backups.</span><br><span class="line">#</span><br><span class="line"># The &#x27;shutdown-timeout&#x27; value is the grace period&#x27;s duration in seconds. It is</span><br><span class="line"># only applicable when the instance has replicas. To disable the feature, set</span><br><span class="line"># the value to 0.</span><br><span class="line">#</span><br><span class="line"># shutdown-timeout 10</span><br><span class="line"></span><br><span class="line"># When Redis receives a SIGINT or SIGTERM, shutdown is initiated and by default</span><br><span class="line"># an RDB snapshot is written to disk in a blocking operation if save points are configured.</span><br><span class="line"># The options used on signaled shutdown can include the following values:</span><br><span class="line"># default:  Saves RDB snapshot only if save points are configured.</span><br><span class="line">#           Waits for lagging replicas to catch up.</span><br><span class="line"># save:     Forces a DB saving operation even if no save points are configured.</span><br><span class="line"># nosave:   Prevents DB saving operation even if one or more save points are configured.</span><br><span class="line"># now:      Skips waiting for lagging replicas.</span><br><span class="line"># force:    Ignores any errors that would normally prevent the server from exiting.</span><br><span class="line">#</span><br><span class="line"># Any combination of values is allowed as long as &quot;save&quot; and &quot;nosave&quot; are not set simultaneously.</span><br><span class="line"># Example: &quot;nosave force now&quot;</span><br><span class="line">#</span><br><span class="line"># shutdown-on-sigint default</span><br><span class="line"># shutdown-on-sigterm default</span><br><span class="line"></span><br><span class="line">################ NON-DETERMINISTIC LONG BLOCKING COMMANDS #####################</span><br><span class="line"></span><br><span class="line"># Maximum time in milliseconds for EVAL scripts, functions and in some cases</span><br><span class="line"># modules&#x27; commands before Redis can start processing or rejecting other clients.</span><br><span class="line">#</span><br><span class="line"># If the maximum execution time is reached Redis will start to reply to most</span><br><span class="line"># commands with a BUSY error.</span><br><span class="line">#</span><br><span class="line"># In this state Redis will only allow a handful of commands to be executed.</span><br><span class="line"># For instance, SCRIPT KILL, FUNCTION KILL, SHUTDOWN NOSAVE and possibly some</span><br><span class="line"># module specific &#x27;allow-busy&#x27; commands.</span><br><span class="line">#</span><br><span class="line"># SCRIPT KILL and FUNCTION KILL will only be able to stop a script that did not</span><br><span class="line"># yet call any write commands, so SHUTDOWN NOSAVE may be the only way to stop</span><br><span class="line"># the server in the case a write command was already issued by the script when</span><br><span class="line"># the user doesn&#x27;t want to wait for the natural termination of the script.</span><br><span class="line">#</span><br><span class="line"># The default is 5 seconds. It is possible to set it to 0 or a negative value</span><br><span class="line"># to disable this mechanism (uninterrupted execution). Note that in the past</span><br><span class="line"># this config had a different name, which is now an alias, so both of these do</span><br><span class="line"># the same:</span><br><span class="line"># lua-time-limit 5000</span><br><span class="line"># busy-reply-threshold 5000</span><br><span class="line"></span><br><span class="line">################################ REDIS CLUSTER  ###############################</span><br><span class="line"></span><br><span class="line"># Normal Redis instances can&#x27;t be part of a Redis Cluster; only nodes that are</span><br><span class="line"># started as cluster nodes can. In order to start a Redis instance as a</span><br><span class="line"># cluster node enable the cluster support uncommenting the following:</span><br><span class="line">#</span><br><span class="line"># cluster-enabled yes</span><br><span class="line"></span><br><span class="line"># Every cluster node has a cluster configuration file. This file is not</span><br><span class="line"># intended to be edited by hand. It is created and updated by Redis nodes.</span><br><span class="line"># Every Redis Cluster node requires a different cluster configuration file.</span><br><span class="line"># Make sure that instances running in the same system do not have</span><br><span class="line"># overlapping cluster configuration file names.</span><br><span class="line">#</span><br><span class="line"># cluster-config-file nodes-6379.conf</span><br><span class="line"></span><br><span class="line"># Cluster node timeout is the amount of milliseconds a node must be unreachable</span><br><span class="line"># for it to be considered in failure state.</span><br><span class="line"># Most other internal time limits are a multiple of the node timeout.</span><br><span class="line">#</span><br><span class="line"># cluster-node-timeout 15000</span><br><span class="line"></span><br><span class="line"># The cluster port is the port that the cluster bus will listen for inbound connections on. When set </span><br><span class="line"># to the default value, 0, it will be bound to the command port + 10000. Setting this value requires </span><br><span class="line"># you to specify the cluster bus port when executing cluster meet.</span><br><span class="line"># cluster-port 0</span><br><span class="line"></span><br><span class="line"># A replica of a failing master will avoid to start a failover if its data</span><br><span class="line"># looks too old.</span><br><span class="line">#</span><br><span class="line"># There is no simple way for a replica to actually have an exact measure of</span><br><span class="line"># its &quot;data age&quot;, so the following two checks are performed:</span><br><span class="line">#</span><br><span class="line"># 1) If there are multiple replicas able to failover, they exchange messages</span><br><span class="line">#    in order to try to give an advantage to the replica with the best</span><br><span class="line">#    replication offset (more data from the master processed).</span><br><span class="line">#    Replicas will try to get their rank by offset, and apply to the start</span><br><span class="line">#    of the failover a delay proportional to their rank.</span><br><span class="line">#</span><br><span class="line"># 2) Every single replica computes the time of the last interaction with</span><br><span class="line">#    its master. This can be the last ping or command received (if the master</span><br><span class="line">#    is still in the &quot;connected&quot; state), or the time that elapsed since the</span><br><span class="line">#    disconnection with the master (if the replication link is currently down).</span><br><span class="line">#    If the last interaction is too old, the replica will not try to failover</span><br><span class="line">#    at all.</span><br><span class="line">#</span><br><span class="line"># The point &quot;2&quot; can be tuned by user. Specifically a replica will not perform</span><br><span class="line"># the failover if, since the last interaction with the master, the time</span><br><span class="line"># elapsed is greater than:</span><br><span class="line">#</span><br><span class="line">#   (node-timeout * cluster-replica-validity-factor) + repl-ping-replica-period</span><br><span class="line">#</span><br><span class="line"># So for example if node-timeout is 30 seconds, and the cluster-replica-validity-factor</span><br><span class="line"># is 10, and assuming a default repl-ping-replica-period of 10 seconds, the</span><br><span class="line"># replica will not try to failover if it was not able to talk with the master</span><br><span class="line"># for longer than 310 seconds.</span><br><span class="line">#</span><br><span class="line"># A large cluster-replica-validity-factor may allow replicas with too old data to failover</span><br><span class="line"># a master, while a too small value may prevent the cluster from being able to</span><br><span class="line"># elect a replica at all.</span><br><span class="line">#</span><br><span class="line"># For maximum availability, it is possible to set the cluster-replica-validity-factor</span><br><span class="line"># to a value of 0, which means, that replicas will always try to failover the</span><br><span class="line"># master regardless of the last time they interacted with the master.</span><br><span class="line"># (However they&#x27;ll always try to apply a delay proportional to their</span><br><span class="line"># offset rank).</span><br><span class="line">#</span><br><span class="line"># Zero is the only value able to guarantee that when all the partitions heal</span><br><span class="line"># the cluster will always be able to continue.</span><br><span class="line">#</span><br><span class="line"># cluster-replica-validity-factor 10</span><br><span class="line"></span><br><span class="line"># Cluster replicas are able to migrate to orphaned masters, that are masters</span><br><span class="line"># that are left without working replicas. This improves the cluster ability</span><br><span class="line"># to resist to failures as otherwise an orphaned master can&#x27;t be failed over</span><br><span class="line"># in case of failure if it has no working replicas.</span><br><span class="line">#</span><br><span class="line"># Replicas migrate to orphaned masters only if there are still at least a</span><br><span class="line"># given number of other working replicas for their old master. This number</span><br><span class="line"># is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica</span><br><span class="line"># will migrate only if there is at least 1 other working replica for its master</span><br><span class="line"># and so forth. It usually reflects the number of replicas you want for every</span><br><span class="line"># master in your cluster.</span><br><span class="line">#</span><br><span class="line"># Default is 1 (replicas migrate only if their masters remain with at least</span><br><span class="line"># one replica). To disable migration just set it to a very large value or</span><br><span class="line"># set cluster-allow-replica-migration to &#x27;no&#x27;.</span><br><span class="line"># A value of 0 can be set but is useful only for debugging and dangerous</span><br><span class="line"># in production.</span><br><span class="line">#</span><br><span class="line"># cluster-migration-barrier 1</span><br><span class="line"></span><br><span class="line"># Turning off this option allows to use less automatic cluster configuration.</span><br><span class="line"># It both disables migration to orphaned masters and migration from masters</span><br><span class="line"># that became empty.</span><br><span class="line">#</span><br><span class="line"># Default is &#x27;yes&#x27; (allow automatic migrations).</span><br><span class="line">#</span><br><span class="line"># cluster-allow-replica-migration yes</span><br><span class="line"></span><br><span class="line"># By default Redis Cluster nodes stop accepting queries if they detect there</span><br><span class="line"># is at least a hash slot uncovered (no available node is serving it).</span><br><span class="line"># This way if the cluster is partially down (for example a range of hash slots</span><br><span class="line"># are no longer covered) all the cluster becomes, eventually, unavailable.</span><br><span class="line"># It automatically returns available as soon as all the slots are covered again.</span><br><span class="line">#</span><br><span class="line"># However sometimes you want the subset of the cluster which is working,</span><br><span class="line"># to continue to accept queries for the part of the key space that is still</span><br><span class="line"># covered. In order to do so, just set the cluster-require-full-coverage</span><br><span class="line"># option to no.</span><br><span class="line">#</span><br><span class="line"># cluster-require-full-coverage yes</span><br><span class="line"></span><br><span class="line"># This option, when set to yes, prevents replicas from trying to failover its</span><br><span class="line"># master during master failures. However the replica can still perform a</span><br><span class="line"># manual failover, if forced to do so.</span><br><span class="line">#</span><br><span class="line"># This is useful in different scenarios, especially in the case of multiple</span><br><span class="line"># data center operations, where we want one side to never be promoted if not</span><br><span class="line"># in the case of a total DC failure.</span><br><span class="line">#</span><br><span class="line"># cluster-replica-no-failover no</span><br><span class="line"></span><br><span class="line"># This option, when set to yes, allows nodes to serve read traffic while the</span><br><span class="line"># cluster is in a down state, as long as it believes it owns the slots.</span><br><span class="line">#</span><br><span class="line"># This is useful for two cases.  The first case is for when an application</span><br><span class="line"># doesn&#x27;t require consistency of data during node failures or network partitions.</span><br><span class="line"># One example of this is a cache, where as long as the node has the data it</span><br><span class="line"># should be able to serve it.</span><br><span class="line">#</span><br><span class="line"># The second use case is for configurations that don&#x27;t meet the recommended</span><br><span class="line"># three shards but want to enable cluster mode and scale later. A</span><br><span class="line"># master outage in a 1 or 2 shard configuration causes a read/write outage to the</span><br><span class="line"># entire cluster without this option set, with it set there is only a write outage.</span><br><span class="line"># Without a quorum of masters, slot ownership will not change automatically.</span><br><span class="line">#</span><br><span class="line"># cluster-allow-reads-when-down no</span><br><span class="line"></span><br><span class="line"># This option, when set to yes, allows nodes to serve pubsub shard traffic while</span><br><span class="line"># the cluster is in a down state, as long as it believes it owns the slots.</span><br><span class="line">#</span><br><span class="line"># This is useful if the application would like to use the pubsub feature even when</span><br><span class="line"># the cluster global stable state is not OK. If the application wants to make sure only</span><br><span class="line"># one shard is serving a given channel, this feature should be kept as yes.</span><br><span class="line">#</span><br><span class="line"># cluster-allow-pubsubshard-when-down yes</span><br><span class="line"></span><br><span class="line"># Cluster link send buffer limit is the limit on the memory usage of an individual</span><br><span class="line"># cluster bus link&#x27;s send buffer in bytes. Cluster links would be freed if they exceed</span><br><span class="line"># this limit. This is to primarily prevent send buffers from growing unbounded on links</span><br><span class="line"># toward slow peers (E.g. PubSub messages being piled up).</span><br><span class="line"># This limit is disabled by default. Enable this limit when &#x27;mem_cluster_links&#x27; INFO field</span><br><span class="line"># and/or &#x27;send-buffer-allocated&#x27; entries in the &#x27;CLUSTER LINKS` command output continuously increase.</span><br><span class="line"># Minimum limit of 1gb is recommended so that cluster link buffer can fit in at least a single</span><br><span class="line"># PubSub message by default. (client-query-buffer-limit default value is 1gb)</span><br><span class="line">#</span><br><span class="line"># cluster-link-sendbuf-limit 0</span><br><span class="line"> </span><br><span class="line"># Clusters can configure their announced hostname using this config. This is a common use case for </span><br><span class="line"># applications that need to use TLS Server Name Indication (SNI) or dealing with DNS based</span><br><span class="line"># routing. By default this value is only shown as additional metadata in the CLUSTER SLOTS</span><br><span class="line"># command, but can be changed using &#x27;cluster-preferred-endpoint-type&#x27; config. This value is </span><br><span class="line"># communicated along the clusterbus to all nodes, setting it to an empty string will remove </span><br><span class="line"># the hostname and also propagate the removal.</span><br><span class="line">#</span><br><span class="line"># cluster-announce-hostname &quot;&quot;</span><br><span class="line"></span><br><span class="line"># Clusters can configure an optional nodename to be used in addition to the node ID for</span><br><span class="line"># debugging and admin information. This name is broadcasted between nodes, so will be used</span><br><span class="line"># in addition to the node ID when reporting cross node events such as node failures.</span><br><span class="line"># cluster-announce-human-nodename &quot;&quot;</span><br><span class="line"></span><br><span class="line"># Clusters can advertise how clients should connect to them using either their IP address,</span><br><span class="line"># a user defined hostname, or by declaring they have no endpoint. Which endpoint is</span><br><span class="line"># shown as the preferred endpoint is set by using the cluster-preferred-endpoint-type</span><br><span class="line"># config with values &#x27;ip&#x27;, &#x27;hostname&#x27;, or &#x27;unknown-endpoint&#x27;. This value controls how</span><br><span class="line"># the endpoint returned for MOVED/ASKING requests as well as the first field of CLUSTER SLOTS. </span><br><span class="line"># If the preferred endpoint type is set to hostname, but no announced hostname is set, a &#x27;?&#x27; </span><br><span class="line"># will be returned instead.</span><br><span class="line">#</span><br><span class="line"># When a cluster advertises itself as having an unknown endpoint, it&#x27;s indicating that</span><br><span class="line"># the server doesn&#x27;t know how clients can reach the cluster. This can happen in certain </span><br><span class="line"># networking situations where there are multiple possible routes to the node, and the </span><br><span class="line"># server doesn&#x27;t know which one the client took. In this case, the server is expecting</span><br><span class="line"># the client to reach out on the same endpoint it used for making the last request, but use</span><br><span class="line"># the port provided in the response.</span><br><span class="line">#</span><br><span class="line"># cluster-preferred-endpoint-type ip</span><br><span class="line"></span><br><span class="line"># In order to setup your cluster make sure to read the documentation</span><br><span class="line"># available at https://redis.io web site.</span><br><span class="line"></span><br><span class="line">########################## CLUSTER DOCKER/NAT support  ########################</span><br><span class="line"></span><br><span class="line"># In certain deployments, Redis Cluster nodes address discovery fails, because</span><br><span class="line"># addresses are NAT-ted or because ports are forwarded (the typical case is</span><br><span class="line"># Docker and other containers).</span><br><span class="line">#</span><br><span class="line"># In order to make Redis Cluster working in such environments, a static</span><br><span class="line"># configuration where each node knows its public address is needed. The</span><br><span class="line"># following four options are used for this scope, and are:</span><br><span class="line">#</span><br><span class="line"># * cluster-announce-ip</span><br><span class="line"># * cluster-announce-port</span><br><span class="line"># * cluster-announce-tls-port</span><br><span class="line"># * cluster-announce-bus-port</span><br><span class="line">#</span><br><span class="line"># Each instructs the node about its address, client ports (for connections</span><br><span class="line"># without and with TLS) and cluster message bus port. The information is then</span><br><span class="line"># published in the header of the bus packets so that other nodes will be able to</span><br><span class="line"># correctly map the address of the node publishing the information.</span><br><span class="line">#</span><br><span class="line"># If tls-cluster is set to yes and cluster-announce-tls-port is omitted or set</span><br><span class="line"># to zero, then cluster-announce-port refers to the TLS port. Note also that</span><br><span class="line"># cluster-announce-tls-port has no effect if tls-cluster is set to no.</span><br><span class="line">#</span><br><span class="line"># If the above options are not used, the normal Redis Cluster auto-detection</span><br><span class="line"># will be used instead.</span><br><span class="line">#</span><br><span class="line"># Note that when remapped, the bus port may not be at the fixed offset of</span><br><span class="line"># clients port + 10000, so you can specify any port and bus-port depending</span><br><span class="line"># on how they get remapped. If the bus-port is not set, a fixed offset of</span><br><span class="line"># 10000 will be used as usual.</span><br><span class="line">#</span><br><span class="line"># Example:</span><br><span class="line">#</span><br><span class="line"># cluster-announce-ip 10.1.1.5</span><br><span class="line"># cluster-announce-tls-port 6379</span><br><span class="line"># cluster-announce-port 0</span><br><span class="line"># cluster-announce-bus-port 6380</span><br><span class="line"></span><br><span class="line">################################## SLOW LOG ###################################</span><br><span class="line"></span><br><span class="line"># The Redis Slow Log is a system to log queries that exceeded a specified</span><br><span class="line"># execution time. The execution time does not include the I/O operations</span><br><span class="line"># like talking with the client, sending the reply and so forth,</span><br><span class="line"># but just the time needed to actually execute the command (this is the only</span><br><span class="line"># stage of command execution where the thread is blocked and can not serve</span><br><span class="line"># other requests in the meantime).</span><br><span class="line">#</span><br><span class="line"># You can configure the slow log with two parameters: one tells Redis</span><br><span class="line"># what is the execution time, in microseconds, to exceed in order for the</span><br><span class="line"># command to get logged, and the other parameter is the length of the</span><br><span class="line"># slow log. When a new command is logged the oldest one is removed from the</span><br><span class="line"># queue of logged commands.</span><br><span class="line"></span><br><span class="line"># The following time is expressed in microseconds, so 1000000 is equivalent</span><br><span class="line"># to one second. Note that a negative number disables the slow log, while</span><br><span class="line"># a value of zero forces the logging of every command.</span><br><span class="line">slowlog-log-slower-than 10000</span><br><span class="line"></span><br><span class="line"># There is no limit to this length. Just be aware that it will consume memory.</span><br><span class="line"># You can reclaim memory used by the slow log with SLOWLOG RESET.</span><br><span class="line">slowlog-max-len 128</span><br><span class="line"></span><br><span class="line">################################ LATENCY MONITOR ##############################</span><br><span class="line"></span><br><span class="line"># The Redis latency monitoring subsystem samples different operations</span><br><span class="line"># at runtime in order to collect data related to possible sources of</span><br><span class="line"># latency of a Redis instance.</span><br><span class="line">#</span><br><span class="line"># Via the LATENCY command this information is available to the user that can</span><br><span class="line"># print graphs and obtain reports.</span><br><span class="line">#</span><br><span class="line"># The system only logs operations that were performed in a time equal or</span><br><span class="line"># greater than the amount of milliseconds specified via the</span><br><span class="line"># latency-monitor-threshold configuration directive. When its value is set</span><br><span class="line"># to zero, the latency monitor is turned off.</span><br><span class="line">#</span><br><span class="line"># By default latency monitoring is disabled since it is mostly not needed</span><br><span class="line"># if you don&#x27;t have latency issues, and collecting data has a performance</span><br><span class="line"># impact, that while very small, can be measured under big load. Latency</span><br><span class="line"># monitoring can easily be enabled at runtime using the command</span><br><span class="line"># &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.</span><br><span class="line">latency-monitor-threshold 0</span><br><span class="line"></span><br><span class="line">################################ LATENCY TRACKING ##############################</span><br><span class="line"></span><br><span class="line"># The Redis extended latency monitoring tracks the per command latencies and enables</span><br><span class="line"># exporting the percentile distribution via the INFO latencystats command,</span><br><span class="line"># and cumulative latency distributions (histograms) via the LATENCY command.</span><br><span class="line">#</span><br><span class="line"># By default, the extended latency monitoring is enabled since the overhead</span><br><span class="line"># of keeping track of the command latency is very small.</span><br><span class="line"># latency-tracking yes</span><br><span class="line"></span><br><span class="line"># By default the exported latency percentiles via the INFO latencystats command</span><br><span class="line"># are the p50, p99, and p999.</span><br><span class="line"># latency-tracking-info-percentiles 50 99 99.9</span><br><span class="line"></span><br><span class="line">############################# EVENT NOTIFICATION ##############################</span><br><span class="line"></span><br><span class="line"># Redis can notify Pub/Sub clients about events happening in the key space.</span><br><span class="line"># This feature is documented at https://redis.io/topics/notifications</span><br><span class="line">#</span><br><span class="line"># For instance if keyspace events notification is enabled, and a client</span><br><span class="line"># performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two</span><br><span class="line"># messages will be published via Pub/Sub:</span><br><span class="line">#</span><br><span class="line"># PUBLISH __keyspace@0__:foo del</span><br><span class="line"># PUBLISH __keyevent@0__:del foo</span><br><span class="line">#</span><br><span class="line"># It is possible to select the events that Redis will notify among a set</span><br><span class="line"># of classes. Every class is identified by a single character:</span><br><span class="line">#</span><br><span class="line">#  K     Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.</span><br><span class="line">#  E     Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.</span><br><span class="line">#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...</span><br><span class="line">#  $     String commands</span><br><span class="line">#  l     List commands</span><br><span class="line">#  s     Set commands</span><br><span class="line">#  h     Hash commands</span><br><span class="line">#  z     Sorted set commands</span><br><span class="line">#  x     Expired events (events generated every time a key expires)</span><br><span class="line">#  e     Evicted events (events generated when a key is evicted for maxmemory)</span><br><span class="line">#  n     New key events (Note: not included in the &#x27;A&#x27; class)</span><br><span class="line">#  t     Stream commands</span><br><span class="line">#  d     Module key type events</span><br><span class="line">#  m     Key-miss events (Note: It is not included in the &#x27;A&#x27; class)</span><br><span class="line">#  A     Alias for g$lshzxetd, so that the &quot;AKE&quot; string means all the events</span><br><span class="line">#        (Except key-miss events which are excluded from &#x27;A&#x27; due to their</span><br><span class="line">#         unique nature).</span><br><span class="line">#</span><br><span class="line">#  The &quot;notify-keyspace-events&quot; takes as argument a string that is composed</span><br><span class="line">#  of zero or multiple characters. The empty string means that notifications</span><br><span class="line">#  are disabled.</span><br><span class="line">#</span><br><span class="line">#  Example: to enable list and generic events, from the point of view of the</span><br><span class="line">#           event name, use:</span><br><span class="line">#</span><br><span class="line">#  notify-keyspace-events Elg</span><br><span class="line">#</span><br><span class="line">#  Example 2: to get the stream of the expired keys subscribing to channel</span><br><span class="line">#             name __keyevent@0__:expired use:</span><br><span class="line">#</span><br><span class="line">#  notify-keyspace-events Ex</span><br><span class="line">#</span><br><span class="line">#  By default all notifications are disabled because most users don&#x27;t need</span><br><span class="line">#  this feature and the feature has some overhead. Note that if you don&#x27;t</span><br><span class="line">#  specify at least one of K or E, no events will be delivered.</span><br><span class="line">notify-keyspace-events &quot;&quot;</span><br><span class="line"></span><br><span class="line">############################### ADVANCED CONFIG ###############################</span><br><span class="line"></span><br><span class="line"># Hashes are encoded using a memory efficient data structure when they have a</span><br><span class="line"># small number of entries, and the biggest entry does not exceed a given</span><br><span class="line"># threshold. These thresholds can be configured using the following directives.</span><br><span class="line">hash-max-listpack-entries 512</span><br><span class="line">hash-max-listpack-value 64</span><br><span class="line"></span><br><span class="line"># Lists are also encoded in a special way to save a lot of space.</span><br><span class="line"># The number of entries allowed per internal list node can be specified</span><br><span class="line"># as a fixed maximum size or a maximum number of elements.</span><br><span class="line"># For a fixed maximum size, use -5 through -1, meaning:</span><br><span class="line"># -5: max size: 64 Kb  &lt;-- not recommended for normal workloads</span><br><span class="line"># -4: max size: 32 Kb  &lt;-- not recommended</span><br><span class="line"># -3: max size: 16 Kb  &lt;-- probably not recommended</span><br><span class="line"># -2: max size: 8 Kb   &lt;-- good</span><br><span class="line"># -1: max size: 4 Kb   &lt;-- good</span><br><span class="line"># Positive numbers mean store up to _exactly_ that number of elements</span><br><span class="line"># per list node.</span><br><span class="line"># The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),</span><br><span class="line"># but if your use case is unique, adjust the settings as necessary.</span><br><span class="line">list-max-listpack-size -2</span><br><span class="line"></span><br><span class="line"># Lists may also be compressed.</span><br><span class="line"># Compress depth is the number of quicklist ziplist nodes from *each* side of</span><br><span class="line"># the list to *exclude* from compression.  The head and tail of the list</span><br><span class="line"># are always uncompressed for fast push/pop operations.  Settings are:</span><br><span class="line"># 0: disable all list compression</span><br><span class="line"># 1: depth 1 means &quot;don&#x27;t start compressing until after 1 node into the list,</span><br><span class="line">#    going from either the head or tail&quot;</span><br><span class="line">#    So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]</span><br><span class="line">#    [head], [tail] will always be uncompressed; inner nodes will compress.</span><br><span class="line"># 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]</span><br><span class="line">#    2 here means: don&#x27;t compress head or head-&gt;next or tail-&gt;prev or tail,</span><br><span class="line">#    but compress all nodes between them.</span><br><span class="line"># 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]</span><br><span class="line"># etc.</span><br><span class="line">list-compress-depth 0</span><br><span class="line"></span><br><span class="line"># Sets have a special encoding when a set is composed</span><br><span class="line"># of just strings that happen to be integers in radix 10 in the range</span><br><span class="line"># of 64 bit signed integers.</span><br><span class="line"># The following configuration setting sets the limit in the size of the</span><br><span class="line"># set in order to use this special memory saving encoding.</span><br><span class="line">set-max-intset-entries 512</span><br><span class="line"></span><br><span class="line"># Sets containing non-integer values are also encoded using a memory efficient</span><br><span class="line"># data structure when they have a small number of entries, and the biggest entry</span><br><span class="line"># does not exceed a given threshold. These thresholds can be configured using</span><br><span class="line"># the following directives.</span><br><span class="line">set-max-listpack-entries 128</span><br><span class="line">set-max-listpack-value 64</span><br><span class="line"></span><br><span class="line"># Similarly to hashes and lists, sorted sets are also specially encoded in</span><br><span class="line"># order to save a lot of space. This encoding is only used when the length and</span><br><span class="line"># elements of a sorted set are below the following limits:</span><br><span class="line">zset-max-listpack-entries 128</span><br><span class="line">zset-max-listpack-value 64</span><br><span class="line"></span><br><span class="line"># HyperLogLog sparse representation bytes limit. The limit includes the</span><br><span class="line"># 16 bytes header. When a HyperLogLog using the sparse representation crosses</span><br><span class="line"># this limit, it is converted into the dense representation.</span><br><span class="line">#</span><br><span class="line"># A value greater than 16000 is totally useless, since at that point the</span><br><span class="line"># dense representation is more memory efficient.</span><br><span class="line">#</span><br><span class="line"># The suggested value is ~ 3000 in order to have the benefits of</span><br><span class="line"># the space efficient encoding without slowing down too much PFADD,</span><br><span class="line"># which is O(N) with the sparse encoding. The value can be raised to</span><br><span class="line"># ~ 10000 when CPU is not a concern, but space is, and the data set is</span><br><span class="line"># composed of many HyperLogLogs with cardinality in the 0 - 15000 range.</span><br><span class="line">hll-sparse-max-bytes 3000</span><br><span class="line"></span><br><span class="line"># Streams macro node max size / items. The stream data structure is a radix</span><br><span class="line"># tree of big nodes that encode multiple items inside. Using this configuration</span><br><span class="line"># it is possible to configure how big a single node can be in bytes, and the</span><br><span class="line"># maximum number of items it may contain before switching to a new node when</span><br><span class="line"># appending new stream entries. If any of the following settings are set to</span><br><span class="line"># zero, the limit is ignored, so for instance it is possible to set just a</span><br><span class="line"># max entries limit by setting max-bytes to 0 and max-entries to the desired</span><br><span class="line"># value.</span><br><span class="line">stream-node-max-bytes 4096</span><br><span class="line">stream-node-max-entries 100</span><br><span class="line"></span><br><span class="line"># Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in</span><br><span class="line"># order to help rehashing the main Redis hash table (the one mapping top-level</span><br><span class="line"># keys to values). The hash table implementation Redis uses (see dict.c)</span><br><span class="line"># performs a lazy rehashing: the more operation you run into a hash table</span><br><span class="line"># that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the</span><br><span class="line"># server is idle the rehashing is never complete and some more memory is used</span><br><span class="line"># by the hash table.</span><br><span class="line">#</span><br><span class="line"># The default is to use this millisecond 10 times every second in order to</span><br><span class="line"># actively rehash the main dictionaries, freeing memory when possible.</span><br><span class="line">#</span><br><span class="line"># If unsure:</span><br><span class="line"># use &quot;activerehashing no&quot; if you have hard latency requirements and it is</span><br><span class="line"># not a good thing in your environment that Redis can reply from time to time</span><br><span class="line"># to queries with 2 milliseconds delay.</span><br><span class="line">#</span><br><span class="line"># use &quot;activerehashing yes&quot; if you don&#x27;t have such hard requirements but</span><br><span class="line"># want to free memory asap when possible.</span><br><span class="line">activerehashing yes</span><br><span class="line"></span><br><span class="line"># The client output buffer limits can be used to force disconnection of clients</span><br><span class="line"># that are not reading data from the server fast enough for some reason (a</span><br><span class="line"># common reason is that a Pub/Sub client can&#x27;t consume messages as fast as the</span><br><span class="line"># publisher can produce them).</span><br><span class="line">#</span><br><span class="line"># The limit can be set differently for the three different classes of clients:</span><br><span class="line">#</span><br><span class="line"># normal -&gt; normal clients including MONITOR clients</span><br><span class="line"># replica -&gt; replica clients</span><br><span class="line"># pubsub -&gt; clients subscribed to at least one pubsub channel or pattern</span><br><span class="line">#</span><br><span class="line"># The syntax of every client-output-buffer-limit directive is the following:</span><br><span class="line">#</span><br><span class="line"># client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;</span><br><span class="line">#</span><br><span class="line"># A client is immediately disconnected once the hard limit is reached, or if</span><br><span class="line"># the soft limit is reached and remains reached for the specified number of</span><br><span class="line"># seconds (continuously).</span><br><span class="line"># So for instance if the hard limit is 32 megabytes and the soft limit is</span><br><span class="line"># 16 megabytes / 10 seconds, the client will get disconnected immediately</span><br><span class="line"># if the size of the output buffers reach 32 megabytes, but will also get</span><br><span class="line"># disconnected if the client reaches 16 megabytes and continuously overcomes</span><br><span class="line"># the limit for 10 seconds.</span><br><span class="line">#</span><br><span class="line"># By default normal clients are not limited because they don&#x27;t receive data</span><br><span class="line"># without asking (in a push way), but just after a request, so only</span><br><span class="line"># asynchronous clients may create a scenario where data is requested faster</span><br><span class="line"># than it can read.</span><br><span class="line">#</span><br><span class="line"># Instead there is a default limit for pubsub and replica clients, since</span><br><span class="line"># subscribers and replicas receive data in a push fashion.</span><br><span class="line">#</span><br><span class="line"># Note that it doesn&#x27;t make sense to set the replica clients output buffer</span><br><span class="line"># limit lower than the repl-backlog-size config (partial sync will succeed</span><br><span class="line"># and then replica will get disconnected).</span><br><span class="line"># Such a configuration is ignored (the size of repl-backlog-size will be used).</span><br><span class="line"># This doesn&#x27;t have memory consumption implications since the replica client</span><br><span class="line"># will share the backlog buffers memory.</span><br><span class="line">#</span><br><span class="line"># Both the hard or the soft limit can be disabled by setting them to zero.</span><br><span class="line">client-output-buffer-limit normal 0 0 0</span><br><span class="line">client-output-buffer-limit replica 256mb 64mb 60</span><br><span class="line">client-output-buffer-limit pubsub 32mb 8mb 60</span><br><span class="line"></span><br><span class="line"># Client query buffers accumulate new commands. They are limited to a fixed</span><br><span class="line"># amount by default in order to avoid that a protocol desynchronization (for</span><br><span class="line"># instance due to a bug in the client) will lead to unbound memory usage in</span><br><span class="line"># the query buffer. However you can configure it here if you have very special</span><br><span class="line"># needs, such us huge multi/exec requests or alike.</span><br><span class="line">#</span><br><span class="line"># client-query-buffer-limit 1gb</span><br><span class="line"></span><br><span class="line"># In some scenarios client connections can hog up memory leading to OOM</span><br><span class="line"># errors or data eviction. To avoid this we can cap the accumulated memory</span><br><span class="line"># used by all client connections (all pubsub and normal clients). Once we</span><br><span class="line"># reach that limit connections will be dropped by the server freeing up</span><br><span class="line"># memory. The server will attempt to drop the connections using the most </span><br><span class="line"># memory first. We call this mechanism &quot;client eviction&quot;.</span><br><span class="line">#</span><br><span class="line"># Client eviction is configured using the maxmemory-clients setting as follows:</span><br><span class="line"># 0 - client eviction is disabled (default)</span><br><span class="line">#</span><br><span class="line"># A memory value can be used for the client eviction threshold,</span><br><span class="line"># for example:</span><br><span class="line"># maxmemory-clients 1g</span><br><span class="line">#</span><br><span class="line"># A percentage value (between 1% and 100%) means the client eviction threshold</span><br><span class="line"># is based on a percentage of the maxmemory setting. For example to set client</span><br><span class="line"># eviction at 5% of maxmemory:</span><br><span class="line"># maxmemory-clients 5%</span><br><span class="line"></span><br><span class="line"># In the Redis protocol, bulk requests, that are, elements representing single</span><br><span class="line"># strings, are normally limited to 512 mb. However you can change this limit</span><br><span class="line"># here, but must be 1mb or greater</span><br><span class="line">#</span><br><span class="line"># proto-max-bulk-len 512mb</span><br><span class="line"></span><br><span class="line"># Redis calls an internal function to perform many background tasks, like</span><br><span class="line"># closing connections of clients in timeout, purging expired keys that are</span><br><span class="line"># never requested, and so forth.</span><br><span class="line">#</span><br><span class="line"># Not all tasks are performed with the same frequency, but Redis checks for</span><br><span class="line"># tasks to perform according to the specified &quot;hz&quot; value.</span><br><span class="line">#</span><br><span class="line"># By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when</span><br><span class="line"># Redis is idle, but at the same time will make Redis more responsive when</span><br><span class="line"># there are many keys expiring at the same time, and timeouts may be</span><br><span class="line"># handled with more precision.</span><br><span class="line">#</span><br><span class="line"># The range is between 1 and 500, however a value over 100 is usually not</span><br><span class="line"># a good idea. Most users should use the default of 10 and raise this up to</span><br><span class="line"># 100 only in environments where very low latency is required.</span><br><span class="line">hz 10</span><br><span class="line"></span><br><span class="line"># Normally it is useful to have an HZ value which is proportional to the</span><br><span class="line"># number of clients connected. This is useful in order, for instance, to</span><br><span class="line"># avoid too many clients are processed for each background task invocation</span><br><span class="line"># in order to avoid latency spikes.</span><br><span class="line">#</span><br><span class="line"># Since the default HZ value by default is conservatively set to 10, Redis</span><br><span class="line"># offers, and enables by default, the ability to use an adaptive HZ value</span><br><span class="line"># which will temporarily raise when there are many connected clients.</span><br><span class="line">#</span><br><span class="line"># When dynamic HZ is enabled, the actual configured HZ will be used</span><br><span class="line"># as a baseline, but multiples of the configured HZ value will be actually</span><br><span class="line"># used as needed once more clients are connected. In this way an idle</span><br><span class="line"># instance will use very little CPU time while a busy instance will be</span><br><span class="line"># more responsive.</span><br><span class="line">dynamic-hz yes</span><br><span class="line"></span><br><span class="line"># When a child rewrites the AOF file, if the following option is enabled</span><br><span class="line"># the file will be fsync-ed every 4 MB of data generated. This is useful</span><br><span class="line"># in order to commit the file to the disk more incrementally and avoid</span><br><span class="line"># big latency spikes.</span><br><span class="line">aof-rewrite-incremental-fsync yes</span><br><span class="line"></span><br><span class="line"># When redis saves RDB file, if the following option is enabled</span><br><span class="line"># the file will be fsync-ed every 4 MB of data generated. This is useful</span><br><span class="line"># in order to commit the file to the disk more incrementally and avoid</span><br><span class="line"># big latency spikes.</span><br><span class="line">rdb-save-incremental-fsync yes</span><br><span class="line"></span><br><span class="line"># Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good</span><br><span class="line"># idea to start with the default settings and only change them after investigating</span><br><span class="line"># how to improve the performances and how the keys LFU change over time, which</span><br><span class="line"># is possible to inspect via the OBJECT FREQ command.</span><br><span class="line">#</span><br><span class="line"># There are two tunable parameters in the Redis LFU implementation: the</span><br><span class="line"># counter logarithm factor and the counter decay time. It is important to</span><br><span class="line"># understand what the two parameters mean before changing them.</span><br><span class="line">#</span><br><span class="line"># The LFU counter is just 8 bits per key, it&#x27;s maximum value is 255, so Redis</span><br><span class="line"># uses a probabilistic increment with logarithmic behavior. Given the value</span><br><span class="line"># of the old counter, when a key is accessed, the counter is incremented in</span><br><span class="line"># this way:</span><br><span class="line">#</span><br><span class="line"># 1. A random number R between 0 and 1 is extracted.</span><br><span class="line"># 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).</span><br><span class="line"># 3. The counter is incremented only if R &lt; P.</span><br><span class="line">#</span><br><span class="line"># The default lfu-log-factor is 10. This is a table of how the frequency</span><br><span class="line"># counter changes with a different number of accesses with different</span><br><span class="line"># logarithmic factors:</span><br><span class="line">#</span><br><span class="line"># +--------+------------+------------+------------+------------+------------+</span><br><span class="line"># | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |</span><br><span class="line"># +--------+------------+------------+------------+------------+------------+</span><br><span class="line"># | 0      | 104        | 255        | 255        | 255        | 255        |</span><br><span class="line"># +--------+------------+------------+------------+------------+------------+</span><br><span class="line"># | 1      | 18         | 49         | 255        | 255        | 255        |</span><br><span class="line"># +--------+------------+------------+------------+------------+------------+</span><br><span class="line"># | 10     | 10         | 18         | 142        | 255        | 255        |</span><br><span class="line"># +--------+------------+------------+------------+------------+------------+</span><br><span class="line"># | 100    | 8          | 11         | 49         | 143        | 255        |</span><br><span class="line"># +--------+------------+------------+------------+------------+------------+</span><br><span class="line">#</span><br><span class="line"># NOTE: The above table was obtained by running the following commands:</span><br><span class="line">#</span><br><span class="line">#   redis-benchmark -n 1000000 incr foo</span><br><span class="line">#   redis-cli object freq foo</span><br><span class="line">#</span><br><span class="line"># NOTE 2: The counter initial value is 5 in order to give new objects a chance</span><br><span class="line"># to accumulate hits.</span><br><span class="line">#</span><br><span class="line"># The counter decay time is the time, in minutes, that must elapse in order</span><br><span class="line"># for the key counter to be decremented.</span><br><span class="line">#</span><br><span class="line"># The default value for the lfu-decay-time is 1. A special value of 0 means we</span><br><span class="line"># will never decay the counter.</span><br><span class="line">#</span><br><span class="line"># lfu-log-factor 10</span><br><span class="line"># lfu-decay-time 1</span><br><span class="line"></span><br><span class="line">########################### ACTIVE DEFRAGMENTATION #######################</span><br><span class="line">#</span><br><span class="line"># What is active defragmentation?</span><br><span class="line"># -------------------------------</span><br><span class="line">#</span><br><span class="line"># Active (online) defragmentation allows a Redis server to compact the</span><br><span class="line"># spaces left between small allocations and deallocations of data in memory,</span><br><span class="line"># thus allowing to reclaim back memory.</span><br><span class="line">#</span><br><span class="line"># Fragmentation is a natural process that happens with every allocator (but</span><br><span class="line"># less so with Jemalloc, fortunately) and certain workloads. Normally a server</span><br><span class="line"># restart is needed in order to lower the fragmentation, or at least to flush</span><br><span class="line"># away all the data and create it again. However thanks to this feature</span><br><span class="line"># implemented by Oran Agra for Redis 4.0 this process can happen at runtime</span><br><span class="line"># in a &quot;hot&quot; way, while the server is running.</span><br><span class="line">#</span><br><span class="line"># Basically when the fragmentation is over a certain level (see the</span><br><span class="line"># configuration options below) Redis will start to create new copies of the</span><br><span class="line"># values in contiguous memory regions by exploiting certain specific Jemalloc</span><br><span class="line"># features (in order to understand if an allocation is causing fragmentation</span><br><span class="line"># and to allocate it in a better place), and at the same time, will release the</span><br><span class="line"># old copies of the data. This process, repeated incrementally for all the keys</span><br><span class="line"># will cause the fragmentation to drop back to normal values.</span><br><span class="line">#</span><br><span class="line"># Important things to understand:</span><br><span class="line">#</span><br><span class="line"># 1. This feature is disabled by default, and only works if you compiled Redis</span><br><span class="line">#    to use the copy of Jemalloc we ship with the source code of Redis.</span><br><span class="line">#    This is the default with Linux builds.</span><br><span class="line">#</span><br><span class="line"># 2. You never need to enable this feature if you don&#x27;t have fragmentation</span><br><span class="line">#    issues.</span><br><span class="line">#</span><br><span class="line"># 3. Once you experience fragmentation, you can enable this feature when</span><br><span class="line">#    needed with the command &quot;CONFIG SET activedefrag yes&quot;.</span><br><span class="line">#</span><br><span class="line"># The configuration parameters are able to fine tune the behavior of the</span><br><span class="line"># defragmentation process. If you are not sure about what they mean it is</span><br><span class="line"># a good idea to leave the defaults untouched.</span><br><span class="line"></span><br><span class="line"># Active defragmentation is disabled by default</span><br><span class="line"># activedefrag no</span><br><span class="line"></span><br><span class="line"># Minimum amount of fragmentation waste to start active defrag</span><br><span class="line"># active-defrag-ignore-bytes 100mb</span><br><span class="line"></span><br><span class="line"># Minimum percentage of fragmentation to start active defrag</span><br><span class="line"># active-defrag-threshold-lower 10</span><br><span class="line"></span><br><span class="line"># Maximum percentage of fragmentation at which we use maximum effort</span><br><span class="line"># active-defrag-threshold-upper 100</span><br><span class="line"></span><br><span class="line"># Minimal effort for defrag in CPU percentage, to be used when the lower</span><br><span class="line"># threshold is reached</span><br><span class="line"># active-defrag-cycle-min 1</span><br><span class="line"></span><br><span class="line"># Maximal effort for defrag in CPU percentage, to be used when the upper</span><br><span class="line"># threshold is reached</span><br><span class="line"># active-defrag-cycle-max 25</span><br><span class="line"></span><br><span class="line"># Maximum number of set/hash/zset/list fields that will be processed from</span><br><span class="line"># the main dictionary scan</span><br><span class="line"># active-defrag-max-scan-fields 1000</span><br><span class="line"></span><br><span class="line"># Jemalloc background thread for purging will be enabled by default</span><br><span class="line">jemalloc-bg-thread yes</span><br><span class="line"></span><br><span class="line"># It is possible to pin different threads and processes of Redis to specific</span><br><span class="line"># CPUs in your system, in order to maximize the performances of the server.</span><br><span class="line"># This is useful both in order to pin different Redis threads in different</span><br><span class="line"># CPUs, but also in order to make sure that multiple Redis instances running</span><br><span class="line"># in the same host will be pinned to different CPUs.</span><br><span class="line">#</span><br><span class="line"># Normally you can do this using the &quot;taskset&quot; command, however it is also</span><br><span class="line"># possible to this via Redis configuration directly, both in Linux and FreeBSD.</span><br><span class="line">#</span><br><span class="line"># You can pin the server/IO threads, bio threads, aof rewrite child process, and</span><br><span class="line"># the bgsave child process. The syntax to specify the cpu list is the same as</span><br><span class="line"># the taskset command:</span><br><span class="line">#</span><br><span class="line"># Set redis server/io threads to cpu affinity 0,2,4,6:</span><br><span class="line"># server_cpulist 0-7:2</span><br><span class="line">#</span><br><span class="line"># Set bio threads to cpu affinity 1,3:</span><br><span class="line"># bio_cpulist 1,3</span><br><span class="line">#</span><br><span class="line"># Set aof rewrite child process to cpu affinity 8,9,10,11:</span><br><span class="line"># aof_rewrite_cpulist 8-11</span><br><span class="line">#</span><br><span class="line"># Set bgsave child process to cpu affinity 1,10,11</span><br><span class="line"># bgsave_cpulist 1,10-11</span><br><span class="line"></span><br><span class="line"># In some cases redis will emit warnings and even refuse to start if it detects</span><br><span class="line"># that the system is in bad state, it is possible to suppress these warnings</span><br><span class="line"># by setting the following config which takes a space delimited list of warnings</span><br><span class="line"># to suppress</span><br><span class="line">#</span><br><span class="line"># ignore-warnings ARM64-COW-BUG</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>在 <code>/app/redis</code> 目录下修改 <code>redis.conf</code> 文件</p>
<ul>
<li><p>开启 redis 验证 - <mark class="hl-label green">可选</mark> </p>
</li>
<li><p>允许 redis 外地连接 - <mark class="hl-label red">必选</mark> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># bind 127.0.0.1</span><br><span class="line">protected-mode no</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>daemonize no</code> - <mark class="hl-label red">必选</mark> </p>
<p>将 daemonize yes 注释起来或者 daemonize no 设置，因为该配置会和 docker run 中 -d 参数冲突，会导致容器一直启动失败。</p>
</li>
<li><p>开启 redis 数据持久化 - <mark class="hl-label green">可选</mark> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">appendonly yes</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>使用 <code>redis 7.2.1</code> 镜像创建容器（也叫运行镜像）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run \</span><br><span class="line">-p 6379:6379 \</span><br><span class="line">--name myredis \</span><br><span class="line">--privileged=<span class="literal">true</span> \</span><br><span class="line">--restart always \</span><br><span class="line">-v /app/redis/redis_7.2.1.conf:/etc/redis/redis.conf \</span><br><span class="line">-v /app/redis/data:/data \</span><br><span class="line">-d redis:7.2.1 \</span><br><span class="line">redis-server /etc/redis/redis.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试 <code>redis-cli</code> 连接上来</p>
</li>
<li><p>请证明 docker 启动使用了我们自己指定的配置文件</p>
<ul>
<li><p>执行下列命令成功</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 16</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改 <code>redis.conf</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">databases 10</span><br></pre></td></tr></table></figure>
</li>
<li><p>再次执行下列命令失败</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 16</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>测试 <code>redis-cli</code> 连接上来第2次</p>
</li>
</ol>
<h3 id="安装-MySQL"><a href="#安装-MySQL" class="headerlink" title="安装 MySQL"></a>安装 MySQL</h3><h4 id="简单版-1"><a href="#简单版-1" class="headerlink" title="简单版"></a>简单版</h4><ol>
<li><p>运行以下命令安装 MySQL</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run \</span><br><span class="line">-p 3306:3306 \</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=123456 \</span><br><span class="line">-d mysql:5.7</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试 MySQL</p>
<ol>
<li><p>连接 MySQL</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it a1fde4e199d2 /bin/bash</span><br><span class="line">mysql -uroot -p 123456</span><br></pre></td></tr></table></figure>
</li>
<li><p>并创建 <code>数据库 db01</code> 和 <code>表 t1</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">show databases;</span><br><span class="line">create database db01;</span><br><span class="line">use db01;</span><br><span class="line">create table t1(id int,name varchar(20));</span><br><span class="line">insert into t1 values(1,&#x27;z3&#x27;);</span><br><span class="line"># 插入中文会报错</span><br><span class="line">insert into t1 values(2,&#x27;李四&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20230917173605786.png" alt="image-20230917173605786"></p>
</li>
<li><p>查看字符集</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">show variables like &#x27;character%&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20230917173714454.png" alt="image-20230917173714454"></p>
</li>
</ol>
</li>
</ol>
<h4 id="实战版-1"><a href="#实战版-1" class="headerlink" title="实战版"></a>实战版</h4><ol>
<li><p>运行以下命令安装 MySQL</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run \</span><br><span class="line">-p 3306:3306 \</span><br><span class="line">--name mysql \</span><br><span class="line">--privileged=<span class="literal">true</span> \</span><br><span class="line">--restart always \</span><br><span class="line">-v /app/mysql/log:/var/log/mysql \</span><br><span class="line">-v /app/mysql/data:/var/lib/mysql \</span><br><span class="line">-v /app/mysql/conf:/etc/mysql/conf.d \</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=123456 \</span><br><span class="line">-d mysql:5.7</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建配置文件 <code>/app/mysql/my.cnf</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">default_character_set=utf8</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">collation_server=utf8_general_ci</span><br><span class="line">character_set_server=utf8</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="一主一从"><a href="#一主一从" class="headerlink" title="一主一从"></a>一主一从</h4><ol>
<li><p>新建主服务器容器实例 3307</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run \</span><br><span class="line">-p 3307:3306 \</span><br><span class="line">--name mysql-master \</span><br><span class="line">--privileged=<span class="literal">true</span> \</span><br><span class="line">--restart always \</span><br><span class="line">-v /mydata/mysql-master/log:/var/log/mysql \</span><br><span class="line">-v /mydata/mysql-master/data:/var/lib/mysql \</span><br><span class="line">-v /mydata/mysql-master/conf:/etc/mysql/conf.d \</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=123456 \</span><br><span class="line">-d mysql:5.7</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入 <code>/mydata/mysql-master/conf</code> 目录下新建 my.cnf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">## 设置 server_id，同一局域网中需要唯一</span><br><span class="line">server_id=101</span><br><span class="line">## 指定不需要同步的数据库名称</span><br><span class="line">binlog-ignore-db=mysql</span><br><span class="line">## 开启二进制日志功能</span><br><span class="line">log-bin=mall-mysql-bin</span><br><span class="line">## 设置二进制日志使用内存大小（事务）</span><br><span class="line">binlog_cache_size=1M</span><br><span class="line">## 设置使用的二进制日志格式（mixed, statement, row）</span><br><span class="line">binlog_format=mixed</span><br><span class="line">## 二进制日志过期清理时间。默认值为 0，表示不自动清理。</span><br><span class="line">expire_logs_days=7</span><br><span class="line">## 跳过主从复制中遇到的所有错误或指定类型的错误，避免 slave 端复制中断。</span><br><span class="line">## 如：1062 错误是指一些主键重复，1032 错误是因为主从数据库数据不一致</span><br><span class="line">slave_skip_errors=1062</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改完配置后重启 master 实例</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker restart mysql-master</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入 mysql-master 容器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mysql-master /bin/bash</span><br><span class="line">mysql -uroot -p123456</span><br></pre></td></tr></table></figure>
</li>
<li><p>master 容器实例内创建数据同步用户</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE USER &#x27;slave&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;;</span><br><span class="line">GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &#x27;slave&#x27;@&#x27;%&#x27;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建 从服务器实例 3308</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run \</span><br><span class="line">-p 3308:3306 \</span><br><span class="line">--name mysql-slave \</span><br><span class="line">--privileged=<span class="literal">true</span> \</span><br><span class="line">--restart always \</span><br><span class="line">-v /mydata/mysql-slave/log:/var/log/mysql \</span><br><span class="line">-v /mydata/mysql-slave/data:/var/lib/mysql \</span><br><span class="line">-v /mydata/mysql-slave/conf:/etc/mysql/conf.d \</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=123456 \</span><br><span class="line">-d mysql:5.7</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入 <code>/mydata/mysql-slave/conf</code> 目录下新建 my.cnf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">## 设置 server_id，同一局域网中需要唯一</span><br><span class="line">server_id=102</span><br><span class="line">## 指定不需要同步的数据库名称</span><br><span class="line">binlog-ignore-db=mysql</span><br><span class="line">## 开启二进制日志功能，以备 Slave 作为其他数据库实例的 Master 时使用</span><br><span class="line">log-bin=mall-mysql-slave1-bin</span><br><span class="line">## 设置二进制日志使用内存大小（事务）</span><br><span class="line">binlog_cache_size=1M</span><br><span class="line">## 设置使用的二进制日志格式（mixed, statement, row）</span><br><span class="line">binlog_format=mixed</span><br><span class="line">## 二进制日志过期清理时间。默认值为 0，表示不自动清理。</span><br><span class="line">expire_logs_days=7</span><br><span class="line">## 跳过主从复制中遇到的所有错误或指定类型的错误，避免 slave 端复制中断。</span><br><span class="line">## 如：1062 错误是指一些主键重复，1032 错误是因为主从数据库数据不一致</span><br><span class="line">slave_skip_errors=1062</span><br><span class="line">## relay_log 配置中继日志</span><br><span class="line">relay_log=mall-mysql-relay-bin</span><br><span class="line">## log_slave_updates 表示 slave 将复制事件写进自己的二进制日志</span><br><span class="line">log_slave_updates=1</span><br><span class="line">## slave 设置为只读（具有 super 权限的用户除外）</span><br><span class="line">read_only=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改完配置后重启 slave 实例</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker restart mysql-slave</span><br></pre></td></tr></table></figure>
</li>
<li><p>在主数据库中查看主从同步状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SHOW MASTER STATUS \G;</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20231015150250235.png" alt="image-20231015150250235"></p>
</li>
<li><p>进入 mysql-slave 容器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mysql-slave  /bin/bash</span><br><span class="line">mysql -uroot -p123456</span><br></pre></td></tr></table></figure>
</li>
<li><p>在从数据库中配置主从复制</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CHANGE MASTER TO MASTER_HOST=&#x27;宿主机 IP&#x27;, MASTER_USER=&#x27;slave&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_PORT=3307, MASTER_LOG_FILE=&#x27;mall-mysql-bin.000001&#x27;, MASTER_LOG_POS=617, MASTER_CONNECT_RETRY=30;</span><br><span class="line"></span><br><span class="line">CHANGE MASTER TO MASTER_HOST=&#x27;192.168.216.128&#x27;, MASTER_USER=&#x27;slave&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_PORT=3307, MASTER_LOG_FILE=&#x27;mall-mysql-bin.000001&#x27;, MASTER_LOG_POS=617, MASTER_CONNECT_RETRY=30;</span><br><span class="line"></span><br><span class="line">## 说明</span><br><span class="line">master_host: 主数据库的 IP 地址；</span><br><span class="line">master_port: 主数据库的运行端口；</span><br><span class="line">master_user: 在主数据搭建的用于同步数据的用户账号；</span><br><span class="line">master_password: 在主数据搭建的用于同步数据的用户密码；</span><br><span class="line">master_log_file: 指定从数据库要复制数据的日志文件，通过查看主数据的状态，获取 File 参数；</span><br><span class="line">master_log_pos: 指定从数据库从哪个位置开始复制数据，通过查看主数据的状态，获取 Position 参数；</span><br><span class="line">master_connect_retry: 连接失败重试的时间间隔，单位为秒；</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20231015151148359.png" alt="image-20231015151148359"></p>
</li>
<li><p>在从数据库中查看主从同步状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SHOW SLAVE STATUS \G;</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20231015152712622.png" alt="image-20231015152712622"></p>
</li>
<li><p>在从数据库中开启主从同步</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start slave;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看从数据库状态发现已经同步</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SHOW SLAVE STATUS \G;</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20231015153012761.png" alt="image-20231015153012761"></p>
</li>
<li><p>主从复制测试</p>
<ol>
<li><p>在主数据库中创建测试数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create database db01;</span><br><span class="line">show databases;</span><br><span class="line">use db01;</span><br><span class="line">create table t1(id int, name varchar(20));</span><br><span class="line">insert into t1 values(1,&#x27;z3&#x27;);</span><br><span class="line">select * from t1;</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20231015153800140.png" alt="image-20231015153800140"></p>
</li>
<li><p>在从数据库中查看数据是否被同步过来</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">show databases;</span><br><span class="line">use db01;</span><br><span class="line">select * from t1;</span><br></pre></td></tr></table></figure>

<p><img src="/fb9a69c2/image-20231015154105361.png" alt="image-20231015154105361"></p>
</li>
</ol>
</li>
</ol>
<h3 id="安装-Nginx"><a href="#安装-Nginx" class="headerlink" title="安装 Nginx"></a>安装 Nginx</h3><ol>
<li><p>创建目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /docker-app/nginx/conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建临时节点用于挂载容器卷</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:80 --name nginx nginx</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">cp</span> -a nginx:/etc/nginx/nginx.conf /docker-app/nginx/conf</span><br><span class="line">docker <span class="built_in">cp</span> -a nginx:/etc/nginx/conf.d /docker-app/nginx/conf</span><br><span class="line">docker <span class="built_in">cp</span> -a nginx:/var/log/nginx /docker-app/nginx/log</span><br><span class="line">docker <span class="built_in">cp</span> -a nginx:/usr/share/nginx/html /docker-app/nginx/html</span><br><span class="line"></span><br><span class="line">docker stop nginx</span><br><span class="line">docker <span class="built_in">rm</span> nginx</span><br><span class="line">docker <span class="built_in">rm</span> -f nginx</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改 nginx.conf</p>
<ul>
<li><p>隐藏 nginx 版本号</p>
<p>在 <code>nginx.conf</code> 文件的 <code>http</code> 段，添加 <code>server_tokens off;</code>。这样，在 Nginx 的响应头中就不会显示版本号信息。</p>
</li>
<li><p>开启网页压缩</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gzip on;</span><br><span class="line">gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;</span><br><span class="line">gzip_comp_level 6;</span><br><span class="line">gzip_min_length 100;</span><br><span class="line"></span><br><span class="line">brotli on;</span><br><span class="line">brotli_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;</span><br><span class="line">brotli_comp_level 6;</span><br><span class="line">brotli_min_length 100;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>编写 <code>docker-compose.yml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">nginx:</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;80:80&quot;</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./conf/nginx.conf:/etc/nginx/nginx.conf</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./conf/conf.d:/etc/nginx/conf.d</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./log:/var/log/nginx</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./html:/usr/share/nginx/html</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">unless-stopped</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="安装-ELK（ing）"><a href="#安装-ELK（ing）" class="headerlink" title="安装 ELK（ing）"></a>安装 ELK（ing）</h3>]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
</search>
